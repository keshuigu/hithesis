% !Mode:: "TeX:UTF-8"
\begin{conclusions}

本论文围绕人脸识别场景下两类关键的隐私威胁——模板逆向攻击（TIA）与模型反演攻击（MIA）——开展了系统性的研究与方法设计。针对模板匹配型和分类型两类目标模型，本文分别提出并验证了基于扩散生成模型与基于换脸先验的生成微调策略。通过理论推导与实验设计，本文取得以下主要结论：

（1）问题刻画与威胁模型的明确化。论文首先对基于模板匹配的识别系统与基于分类器的识别系统分别建立了清晰的威胁模型与攻击目标：在TIA情形下，攻击者以目标模板为条件，目标是生成在特征空间与模板高度一致的图像；在MIA情形下，攻击者以类别标签为条件，目标是最大化模型在该类别上的输出置信度以重建训练样本的代表性图像。对这两类问题的形式化为后续方法设计与评价指标制定奠定了理论基础。

（2）针对TIA提出基于EDM的条件扩散生成方法，并设计两阶段一致性微调策略。论文证明并实验证明，将EDM（Elucidated Diffusion Models）像素级去噪能力与特征一致性损失相结合，能够在保持生成图像视觉质量的同时显著提升与目标模板的特征相似度。为缓解特征约束对图像质量的负面影响，提出先训练 EDM 核心像素损失、再在冻结主干的条件下微调引入特征一致性损失的两阶段训练策略，从而在图像质量与特征对齐之间取得更好的折中。

（3）针对MIA提出基于换脸（Deepfake）先验的条件生成微调方法。将预训练的换脸生成模型作为人脸先验，并通过标签嵌入与局部微调策略使生成器在保持原有生成能力的同时，学习将输入映射到能被目标分类器判为指定类别的样本分布。该方法利用换脸模型对人脸结构和纹理的建模能力，在类别仅为标签已知的场景下，仍能生成真实性与任务导向性兼具的攻击样本。

（4）方法的有效性与评估指标。基于论文中提出的训练与推理流程，本文建议并采用了多维度的评估指标：特征空间相似性（余弦相似度、L2）、攻击成功率（以系统阈值计）、视觉质量指标（FID、SSIM、PSNR）以及可迁移性与主观可识别性评估。实验与分析表明：在白盒或可查询特征的条件下，基于扩散模型与换脸先验的方法在攻击成功率与视觉质量之间能取得优于单纯像素优化或传统GAN方法的平衡。

（5）局限性与适用边界。本文提出的方法依赖于一定的先验或对目标模型的访问：TIA 方案在没有可靠特征访问或特征输出量化严重受限时性能下降；MIA 方法在严格黑盒场景（仅返回 top-1 类别标签或强限制置信度信息）中成功率受限。此外，扩散模型与高质量换脸模型的训练与采样计算资源成本较高，这在工程部署或大规模攻击场景中构成实际限制。

（6）防御策略与工程建议。基于研究发现，本文提出若干防御方向：对模板进行加密或引入不可逆变换、在模型输出或 API 层引入差分隐私/噪声注入以削弱反演可行性、限制模型查询接口并监控可疑访问模式、在模型训练阶段采用对抗或正则化策略以降低单样本对模型决策的影响。实践中应结合系统需求与安全风险做综合权衡。

（7）未来工作展望。后续研究可沿两条方向推进：一方面，扩展到更严格的黑盒场景与限量输出场景，设计在仅有置信度或 top-k 响应下依然有效的反演方法；另一方面，开展系统性的防御-对抗评估，构建对抗基准以衡量差分隐私、输出截断与访问控制等防御手段在真实应用中的效力与代价。此外，可将方法推广到其他生物特征（如指纹、虹膜）与跨模态隐私泄露场景。

综上所述，本文通过问题刻画、方法设计与实验评估，系统阐明了在不同访问条件下面向人脸识别系统的两类逆向重建攻击的可行性、实现路径与防御方向。研究表明：在白盒或弱受限访问条件下，结合高质量生成先验与特征空间约束的攻击方法能够有效重建具有较高视觉和判别一致性的样本；同时，这也提示了在系统设计中应优先考虑模板保护、输出限制与差分隐私等防御措施以降低潜在风险。

\end{conclusions}
