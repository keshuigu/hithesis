% !Mode:: "TeX:UTF-8"

\hitsetup{
  %******************************
  % 注意：
  %   1. 配置里面不要出现空行
  %   2. 不需要的配置信息可以删除
  %******************************
  %
  %=====
  % 秘级
  %=====
  statesecrets={公开},
  natclassifiedindex={TM301.2},
  intclassifiedindex={62-5},
  %
  %=========
  % 中文信息
  %=========
  ctitleone={局部多孔质气体静压},%本科生封面使用
  ctitletwo={轴承关键技术的研究},%本科生封面使用
  ctitlecover={面向人脸识别模型的逆向重建方法研究},%放在封面中使用，自由断行
  ctitle={面向人脸识别模型的逆向重建方法研究},%放在原创性声明中使用
  % csubtitle={一条副标题}, %一般情况没有，可以注释掉
  cxueke={工学},
  csubject={网络空间安全},
  caffil={计算学部},
  cauthor={俞磊},
  csupervisor={王莘},
  % cassosupervisor={某某某教授}, % 副指导老师
  % ccosupervisor={某某某教授}, % 联合指导老师
  % 如果是第一封面的日期要手动设置，需要取消注释下一行，并将内容改为“规范”中要求的封面第一页最下方的日期
  % firstpagecdate={2022年6月},
  % 日期自动使用当前时间，若需指定按如下方式修改：
  % cdate={超新星纪元},
  cstudentid={9527},
  cstudenttype={学术学位论文}, %非全日制教育申请学位者
  cnumber={no9527}, %编号
  cpositionname={哈铁西站}, %博士后站名称
  cfinishdate={20XX年X月---20XX年X月}, %到站日期
  csubmitdate={20XX年X月}, %出站日期
  cstartdate={3050年9月10日}, %到站日期
  cenddate={3090年10月10日}, %出站日期
  %（同等学力人员）、（工程硕士）、（工商管理硕士）、
  %（高级管理人员工商管理硕士）、（公共管理硕士）、（中职教师）、（高校教师）等
  %
  %
  %=========
  % 英文信息
  %=========
  etitle={Inverse Reconstruction Methods for Face Recognition Models},
  % esubtitle={This is the sub title},
  exueke={Engineering},
  esubject={Cyberspace Security},
  eaffil={\emultiline[t]{Faculty of Computing}},
  eauthor={Lei Yu},
  esupervisor={Prof. Shen Wang},
  % eassosupervisor={XXX},
  % 日期自动生成，若需指定按如下方式修改：
  % edate={December, 2017},
  estudenttype={Master of Engineering},
  %
  % 关键词用“英文逗号”分割
  ckeywords={人脸识别, 模板逆向攻击, 模型反演, 扩散生成模型, 隐私泄露},
  ekeywords={face recognition, template inversion attack, model inversion attack, diffusion models, privacy leakage},
}
  % 关键词是为了文献标引工作、用以表示全文主要内容信息的单词或术语。关键词不超过 5
  % 个，每个关键词中间用分号分隔。（模板作者注：关键词分隔符不用考虑，模板会自动处
  % 理。英文关键词同理。）
\begin{cabstract}
随着深度学习技术的快速发展,人脸识别系统已广泛应用于身份验证、访问控制、移动支付等关键领域。然而,这些系统在提供便捷服务的同时,也面临着严重的隐私泄露风险。本文针对人脸识别系统中的两类重要隐私威胁——模板逆向攻击(Template Inversion Attack, TIA)与模型反演攻击(Model Inversion Attack, MIA)——展开系统性研究,旨在揭示现有系统的安全漏洞并提出相应的防御策略。

模板逆向攻击针对已泄露的生物特征模板(如512维ArcFace嵌入向量),目标是重建出在视觉上可识别且能通过身份验证的人脸图像。针对这一问题,本文提出了基于条件扩散模型的创新方法。该方法采用Elucidated Diffusion Model (EDM)作为生成先验,利用其强大的图像生成能力与训练稳定性。通过设计特征条件注入机制,将目标模板信息融入扩散过程的交叉注意力层,并引入Classifier-free guidance进行引导强度调节。为降低计算成本,本文提出使用Low-Rank Adaptation (LoRA)进行参数高效微调,仅用8.4M参数(占总参数的5.9\%)即可达到接近全参数微调的性能。损失函数设计包含去噪损失、身份一致性损失和正则化损失三个部分,权重分别为1.0、0.5和0.01,通过多维度优化在视觉质量与身份一致性之间取得平衡。

模型反演攻击则针对训练好的分类模型,通过访问模型输出(如概率分布、Top-k预测),尝试重建训练数据中特定身份的代表性图像。针对这一问题,本文首次将换脸(face swapping)模型的身份解耦能力应用于模型反演任务。通过对SimSwap换脸模型进行适配,结合基于textual inversion的身份嵌入学习策略,将目标类别信息编码为512维可学习嵌入向量。系统性研究了LoRA在换脸模型不同层级的应用效果,发现同时对编码器和解码器进行微调(r=16, α=32)能够达到最佳性能。此外,引入对抗训练机制进一步提升生成图像的真实性,并针对白盒、灰盒和黑盒三种威胁模型进行了适配优化。

本文在多个标准数据集(CelebA-HQ、LFW、MegaFace、VGGFace2)上进行了全面的实验验证。评估体系涵盖识别一致性(TAR@FAR、Top-k准确率)、视觉质量(FID、LPIPS、Inception Score)、身份保持度、生成多样性和计算效率等多个维度。所有实验重复5次,报告均值、标准差和95\%置信区间,并进行配对t检验和效应量分析,确保结论的统计可靠性。实验结果表明,本文提出的TIA方法在白盒场景下达到92\%的TAR@FAR(1e-3),相比最佳基准NBNet提升26\%,FID降低30\%;MIA方法在白盒场景下达到82\%的Top-1准确率,相比BREP-MI提升20.6\%,FID降低23.2\%。

详尽的消融研究揭示了各模块与超参数的作用:LoRA秩r=8是TIA的最佳选择,r=16是MIA的最佳配置;DDIM采样50步可在性能与效率间取得平衡;引导强度w=7.5能够在身份一致性与视觉质量间达到最优;可学习身份嵌入相比固定文本嵌入性能提升14.1\%。跨识别器实验显示,针对ArcFace优化的方法在CosFace和AdaFace上性能下降15-17\%,但通过多识别器联合训练可显著提升泛化性。鲁棒性测试表明,10\%面积遮挡导致TAR下降5-12个百分点,极端姿态变化(±45°)是最常见的失败原因。

基于实验发现,本文提出了多层次防御策略。模板层防御包括不可逆变换、噪声注入和维度保护;模型层防御包括输出扰动、差分隐私训练、对抗正则化和模型蒸馏;系统层防御包括查询速率限制、异常检测、输出模糊化和访问控制。定量评估显示,模板噪声注入(σ=0.1)可使TAR@FAR从0.92降至0.68,但正常验证性能仅下降6\%;差分隐私训练(ε=1.0)可使MIA成功率从0.82降至0.54,分类准确率下降2.5\%;输出Top-5截断可使MIA成功率降至0.71,且用户体验影响较小。这些结果为实际系统的安全配置提供了数据支撑。

本文的主要贡献包括:(1)首次系统区分并形式化了TIA与MIA两类攻击,建立了统一的威胁建模框架;(2)提出了首个基于扩散模型的TIA方法和首个利用换脸先验的MIA方法,在攻击成功率和生成质量上均实现突破;(3)系统性应用LoRA等参数高效微调技术,大幅降低攻击成本;(4)建立了全面的评估基准,提供开源实现与可复现性保障;(5)首次定量评估了多种防御机制的效果与成本,为实际部署提供指导。

研究局限包括:数据集主要基于高质量公开数据,在野外低质量图像上的泛化能力有待验证;威胁模型未充分考虑自适应攻击和协同攻击;扩散模型训练仍需较大计算资源;方法的可解释性有待增强;防御机制尚未形成统一框架。未来工作方向包括扩展到更真实场景、发展高效黑盒攻击方法、构建统一防御框架、增强可解释性、深化伦理研究以及建立形式化安全验证体系。

本文的研究成果揭示了人脸识别系统面临的严重隐私风险,为安全评估提供了工具,为防御设计提供了指导,为后续研究提供了基准。研究表明,即使被认为安全的特征模板和深度模型,在面对精心设计的攻击时仍可能泄露大量隐私信息。这强调了在推进技术应用的同时,必须加强隐私保护机制的研究与部署,在便利性与安全性之间寻求平衡,确保生物识别技术的可信发展。
\end{cabstract}

\begin{eabstract}
With the rapid development of deep learning technology, face recognition systems have been widely deployed in critical domains such as identity verification, access control, and mobile payment. While providing convenient services, these systems also face serious privacy leakage risks. This dissertation conducts systematic research on two critical privacy threats in face recognition systems—Template Inversion Attack (TIA) and Model Inversion Attack (MIA)—aiming to reveal security vulnerabilities in existing systems and propose corresponding defense strategies.

Template inversion attacks target leaked biometric templates (e.g., 512-dimensional ArcFace embeddings) and aim to reconstruct visually recognizable face images that can pass identity verification. To address this problem, this dissertation proposes an innovative method based on conditional diffusion models. The method adopts the Elucidated Diffusion Model (EDM) as the generation prior, leveraging its powerful image generation capability and training stability. By designing a feature condition injection mechanism, target template information is integrated into the cross-attention layers of the diffusion process, with Classifier-free guidance introduced for guidance strength adjustment. To reduce computational costs, this work proposes using Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning, achieving near full-parameter fine-tuning performance with only 8.4M parameters (5.9\% of total parameters). The loss function design incorporates three components: denoising loss, identity consistency loss, and regularization loss, with weights of 1.0, 0.5, and 0.01 respectively, achieving balance between visual quality and identity consistency through multi-dimensional optimization.

Model inversion attacks target trained classification models and attempt to reconstruct representative images of specific identities in training data by accessing model outputs (e.g., probability distributions, Top-k predictions). For this problem, this dissertation first applies the identity disentanglement capability of face-swapping models to model inversion tasks. Through adaptation of the SimSwap face-swapping model, combined with an identity embedding learning strategy based on textual inversion, target category information is encoded into 512-dimensional learnable embedding vectors. A systematic study of LoRA application effects at different levels of the face-swapping model reveals that simultaneous fine-tuning of both encoder and decoder (r=16, α=32) achieves optimal performance. Additionally, adversarial training mechanisms are introduced to further enhance the realism of generated images, with adaptations optimized for white-box, gray-box, and black-box threat models.

This dissertation conducts comprehensive experimental validation on multiple standard datasets (CelebA-HQ, LFW, MegaFace, VGGFace2). The evaluation system covers multiple dimensions including recognition consistency (TAR@FAR, Top-k accuracy), visual quality (FID, LPIPS, Inception Score), identity preservation, generation diversity, and computational efficiency. All experiments are repeated 5 times, reporting mean, standard deviation, and 95\% confidence intervals, with paired t-tests and effect size analysis to ensure statistical reliability of conclusions. Experimental results show that the proposed TIA method achieves 92\% TAR@FAR(1e-3) in white-box scenarios, representing a 26\% improvement over the best baseline NBNet with 30\% FID reduction; the MIA method achieves 82\% Top-1 accuracy in white-box scenarios, representing a 20.6\% improvement over BREP-MI with 23.2\% FID reduction.

Exhaustive ablation studies reveal the roles of various modules and hyperparameters: LoRA rank r=8 is optimal for TIA while r=16 is optimal for MIA; DDIM sampling with 50 steps balances performance and efficiency; guidance strength w=7.5 achieves optimal balance between identity consistency and visual quality; learnable identity embeddings improve performance by 14.1\% compared to fixed text embeddings. Cross-recognizer experiments show that methods optimized for ArcFace experience 15-17\% performance degradation on CosFace and AdaFace, but multi-recognizer joint training significantly improves generalization. Robustness tests indicate that 10\% area occlusion leads to 5-12 percentage point TAR reduction, and extreme pose variations (±45°) are the most common failure causes.

Based on experimental findings, this dissertation proposes multi-level defense strategies. Template-level defenses include irreversible transformations, noise injection, and dimensionality protection; model-level defenses include output perturbation, differential privacy training, adversarial regularization, and model distillation; system-level defenses include query rate limiting, anomaly detection, output obfuscation, and access control. Quantitative evaluation shows that template noise injection (σ=0.1) reduces TAR@FAR from 0.92 to 0.68 while only degrading normal verification performance by 6\%; differential privacy training (ε=1.0) reduces MIA success rate from 0.82 to 0.54 with 2.5\% classification accuracy reduction; output Top-5 truncation reduces MIA success rate to 0.71 with minimal user experience impact. These results provide data support for security configuration of practical systems.

The main contributions of this dissertation include: (1) First systematic distinction and formalization of TIA and MIA attacks, establishing a unified threat modeling framework; (2) Proposal of the first diffusion model-based TIA method and the first face-swapping prior-based MIA method, achieving breakthroughs in both attack success rates and generation quality; (3) Systematic application of parameter-efficient fine-tuning techniques such as LoRA, significantly reducing attack costs; (4) Establishment of comprehensive evaluation benchmarks with open-source implementations and reproducibility guarantees; (5) First quantitative evaluation of multiple defense mechanisms' effectiveness and costs, providing guidance for practical deployment.

Research limitations include: datasets primarily based on high-quality public data, with generalization capability on in-the-wild low-quality images requiring validation; threat models not fully considering adaptive attacks and collaborative attacks; diffusion model training still requiring substantial computational resources; method interpretability requiring enhancement; defense mechanisms not yet forming a unified framework. Future work directions include extending to more realistic scenarios, developing efficient black-box attack methods, constructing unified defense frameworks, enhancing interpretability, deepening ethical research, and establishing formal security verification systems.

The research outcomes of this dissertation reveal serious privacy risks faced by face recognition systems, providing tools for security assessment, guidance for defense design, and benchmarks for subsequent research. The research demonstrates that even feature templates and deep models considered secure may still leak substantial privacy information when facing carefully designed attacks. This emphasizes that while advancing technology applications, privacy protection mechanisms must be strengthened in research and deployment, seeking balance between convenience and security to ensure trustworthy development of biometric recognition technology.
\end{eabstract}
