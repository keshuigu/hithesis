% !Mode:: "TeX:UTF-8"

% TODO (写作提示):
% - 注明所用先验换脸模型的具体来源（模型名称/论文/仓库链接）与预训练权重信息。
% - 说明微调时冻结/解冻了哪些网络层（例如冻结主干，只训练标签嵌入层），并给出对应层名或代码路径。
% - 建议补充评估协议与基线：使用哪些基线方法比较，评价指标如何统一计算。

\chapter[面向人脸分类模型的逆向重建方法]{面向人脸分类模型的逆向重建方法}[Reconstruction Method for Face Classification Models]
图像分类模型, 主要应用于图像分类、目标检测等任务。此类模型通常包含与基于模板匹配的模型类似的提取特征的网络层, 与其不同之处在于图像分类模型在提取特征后包含额外的全连接层, 将提取的特征值转化为置信度分布, 输出每个类别的概率分布, 以此来达到分类的目的。图像分类模型的工作流程可以分为三个步骤: (1)输入图像 $x$ 经过特征提取网络 $f(\cdot)$, 得到特征向量 $t = f(x)$; (2)特征向量 $t$ 经过全连接层和激活函数, 输出类别概率分布 $p = g(t)$; (3)根据最大概率进行分类决策 $\hat{y} = \arg\max p$。

针对此类模型, 攻击者的目的是通过对模型参数的分析, 推断出训练数据的分布和特征, 从而恢复用户的隐私信息。与这类模型相关的攻击一般被称为模型反演攻击(Model Inversion Attack, MIA), 其目的是利用"目标分类器"来重构私有训练样本。在本课题中, 我们重点研究白盒情境下的模型反演攻击。在这种情况下, 攻击者被假设可以完全访问目标分类器, 攻击者拟定一个类别, 利用模型参数和训练数据的分布信息, 推断出训练该模型对应该类别训练所使用的图像。具体来说, 假设已知模型 $F$ 及其参数, 攻击者通过优化或生成方法, 寻找一个输入 $x^*$, 使得模型输出满足属于某一特定类别, 从而重建或推断出训练数据的敏感信息。设 $F(x)$ 为模型对输入 $x$ 的分类概率输出, $y_{t}$ 为目标类别。攻击者的目标是找到一个输入 $x^*$，使得模型 $F$ 在目标类别 $y_t$ 上的输出概率最大，即:
$$x^* = \arg\max_{x} \; P(y_t \mid x; F)$$

针对模型反演攻击, 为提升生成图像的质量和多样性, 在此方法的研究中引入基于换脸(Deepfake)技术的先验模型。这类模型利用深度学习方法对人脸图像的结构、纹理和表情等特征进行建模, 通过大量真实人脸数据的训练, 能够学习到人脸在不同身份、姿态和光照条件下的变化规律。换脸模型具备较强的图像生成与特征迁移能力, 可以将目标身份的特征融合到源图像中, 实现高质量的人脸合成。作为先验模型应用于生成任务时, 能够为后续的图像生成过程提供丰富的人脸先验知识, 提升生成图像的真实性和多样性, 并增强模型对目标特征的适应能力。

本课题将换脸模型应用到模型反演攻击中, 只需要在预训练的换脸模型的基础上, 对生成模型进行微调, 使其在保持原有生成能力的同时, 更好地适应目标分类模型的特征分布。微调过程中, 生成模型不仅学习到数据的通用特征, 还能够针对目标任务优化其生成过程, 从而提升生成图像与目标类别之间的匹配度。通过这种方式, 生成模型能够生成既符合真实分布、又与目标特征高度一致的图像, 可以有效提升了模板逆向攻击的成功率和生成样本的视觉质量。

本课题采用基于换脸模型作为先验模型的另一个优势在于其具备的灵活性和可控性。具体来说，假设攻击目标需要生成具有特定人脸姿态、表情或其他属性的图像，换脸模型能够通过调整输入的待替换人脸图像的姿态、表情、光照等因素，灵活地控制最终生成图像的外观特征。例如，在某些实际攻击场景中，攻击者可能希望生成一张带有特定微笑、侧脸或特定视角的人脸图像，以更好地适应目标系统的识别条件或满足特定的攻击需求。基于换脸技术的生成模型能够有效地融合目标身份的特征与输入人脸的姿态和表情，从而实现对生成图像的精细化控制。此外，这种灵活性不仅提升了生成模型在多样化攻击场景下的适应能力，还为攻击者提供了更多的操作空间，使其能够针对不同的目标和应用需求，定制化地生成符合要求的伪造图像。


本课题目前已经完成了训练流程和推理流程研究方案的设计。模型反演攻击的训练流程如图\ref{fig:edm_mia_train}所示:

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{images/train_mia.drawio.pdf}
  \caption{基于换脸生成模型的模型反演攻击训练流程示意图}
  \label{fig:edm_mia_train}
\end{figure}

具体来说, 模型反演攻击中所采用的生成模型骨干结构为选定的预训练换脸模型, 并在其基础上增加标签嵌入层以实现通过分类标签控制的条件生成。在模型的训练阶段, 与\ref{sec:tia_finetune}节中的微调策略类似, 本课题将换脸模型的部分主干层参数冻结, 仅对标签嵌入层及部分生成层进行训练, 以兼顾生成能力与任务适应性。

训练流程可用算法\ref{alg:edm_mia_train}表示: 首先, 给定一张原始图像 $x$, 利用攻击目标的分类模型 $F$ 提取其分类标签 $y = F(x)$。随后, 将标签 $y$ 通过嵌入层映射为条件向量, 与原始图像一同输入到换脸生成模型 $f_{\theta}$, 生成伪造图像 $x' = f_{\theta}(x_0, y)$。生成的图像 $x'$ 再次输入分类模型 $F$, 获得预测标签 $y' = F(x')$。训练目标$\mathcal{L}_{Classification}$是最大化 $x'$ 被分类为目标标签 $y$ 的概率。

\begin{algorithm}[H]
  \caption{模型反演攻击训练流程}
  \label{alg:edm_mia_train}
  \begin{algorithmic}[1]
    \REQUIRE 训练样本集 $\mathcal{D}$, 预训练换脸生成模型 $f_\theta$, 目标分类模型 $F$
    \ENSURE 微调后的换脸生成模型 $f_\theta$
    \FOR{迭代 $=1$ 到 $N$}
    \STATE 随机采样训练样本 $x$ 作为待重建人脸, $x_0$ 作为待替换人脸图像
    \FOR{每个样本 $x$}
    \STATE 计算标签 $y = F(x)$
    \STATE 生成伪造图像 $x' = f_\theta(x_0, y)$
    \STATE 预测标签 $y' = F(x')$
    \STATE 计算分类损失 $\mathcal{L}_{\text{Classification}}(y', y)$
    \ENDFOR
    \STATE 更新 $f_\theta$ 以最小化 $\mathcal{L}_{\text{Classification}}$
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

通过以上训练流程, 生成模型能够学习到如何根据目标标签生成与原始图像结构相似、且被目标分类器判别为指定类别的伪造图像, 从而实现对训练数据隐私的有效还原和攻击。

在模型反演攻击的推理流程中, 攻击者所掌握的信息仅限于目标类别标签, 相较于模板逆向攻击, 能够提供给生成模型的先验信息更为有限。因此, 推理过程中需要通过多次迭代, 不断生成图像并根据模型输出的置信度标签进行反馈优化, 直至生成的图像能够满足目标类别的要求。

整个推理流程如图\ref{fig:edm_mia_infer}所示。推理流程首先以一张初始人脸图像 $x_{\text{input}}$ 作为输入，攻击者可以根据任务需求灵活控制该输入人脸的姿态、表情等属性，以适应不同的攻击场景。在每一次迭代生成过程中，当前生成的图像 $x'$ 会被输入到目标分类模型 $F$ 中，获得其在各类别上的置信度分布 $y' = F(x')$。随后，根据分类模型输出的置信度结果，判断当前生成图像是否已经达到预设的目标类别 $y_t$ 的要求。具体而言，若 $y'_t = F(x') \geq \tau$，其中 $\tau$ 为设定的置信度阈值，则认为生成的图像已经满足目标要求，停止迭代。

如果当前生成的图像尚未达到目标类别的置信度要求，则进行下一轮迭代。根据本轮迭代图像分类损失 $\mathcal{L}_{\text{cls}}(y', y_t)$ 的梯度信息，引导$x'$修改, 形成新的$x_{input}$,作为下一轮迭代的人脸图像输入, 以进一步提升目标类别的置信度。该优化过程不断循环，持续迭代，直到生成的图像在目标类别上的置信度超过设定阈值，或达到最大迭代次数为止。通过这种反馈优化机制，能够有效地引导生成模型逐步生成与目标类别高度匹配的图像，从而提升模型反演攻击的成功率和生成样本的质量。

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{images/infer_mia.drawio.pdf}
  \caption{模型反演攻击推理流程示意图}
  \label{fig:edm_mia_infer}
\end{figure}
\subsection*{实现与复现说明}
为便于同行评估与复现，本文在附录及公开的代码仓库中提供实现细节、预训练权重来源、数据预处理与微调配置等材料。正文仅列出用于说明实验设置的关键超参数与评价协议，详尽的脚本与命令示例置于附录或代码仓库中。

下表列出本研究在微调与评估阶段使用的关键超参数示例，实际数值与配置请参见附录或代码仓库中的配置文件。

\begin{table}[htbp]
  \centering
  \begin{tabular}{lll}
    \hline
    超参数 & 示例值 & 说明 \\
    \hline
    pretrained backbone & 指定换脸模型（注明来源） & 所用先验模型及其预训练权重 \\
    learning rate & $1\times10^{-5}$ & 微调阶段学习率示例 \\
    batch size & 16 & 微调时批量大小示例 \\
    微调步数 & 若干 steps / epochs & 微调训练规模 \\
    随机种子 & 42 & 用于复现性评估 \\
    评价指标 & FID, LPIPS, 特征相似度 & 列出用于比较的指标 \\
    \hline
  \end{tabular}
  \caption{用于描述微调与评估设置的关键超参数示例（MIA）}
  \label{tab:mia_hyperparams}
\end{table}

正文中不列出具体运行命令，以保持论文的学术表达形式；有关训练、微调与推理的具体脚本位置、配置示例及下载链接，已统一放置于论文附录或代码仓库，并在附录中给出引用。

% Local Variables:
% TeX-master: "../main"
% TeX-engine: xetex
% End:
