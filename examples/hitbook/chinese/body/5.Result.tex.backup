% !Mode:: "TeX:UTF-8"
\chapter[实验结果与分析]{实验结果与分析}[Experimental Results and Analysis]\label{chap:Results}

\section[引言]{引言}

本章系统性地呈现和分析本文提出的模板逆向攻击（TIA）与模型反演攻击（MIA）方法的实验结果。实验围绕以下核心问题展开：

（1）\textbf{方法有效性验证}。通过定量与定性实验，验证TIA和MIA方法在实际攻击场景中的有效性。

（2）\textbf{关键模块贡献量化}。通过消融实验验证角度约束特征匹配、任务不确定性加权、标签条件嵌入、LoRA微调、渐进式训练等核心设计的实际价值。

（3）\textbf{方法优势分析}。对比现有SOTA方法，分析本文方法在生成质量、攻击成功率、计算效率等方面的优势。

（4）\textbf{泛化性评估}。评估方法在不同数据集、不同识别器架构下的性能表现。

\section[实验配置与评估指标]{实验配置与评估指标}
\label{sec:results_setup}

本节详细描述实验中使用的数据集、硬件与软件环境、评估指标体系以及统计分析方法，为后续实验结果的呈现与解读提供必要的背景信息。

\subsection{数据集详情}

\subsubsection{训练与测试数据集}

本研究使用多个标准人脸数据集进行实验，各数据集的用途与详细信息如下：

\textbf{CelebA}~\cite{liu2015deep}：包含超过200,000张名人人脸图像的大规模数据集，提供40种属性标注与5个关键点坐标。本研究使用CelebA作为辅助训练数据集，以增强模型对多样化人脸特征的学习能力。

\textbf{FFHQ}~\cite{karras2019style}：包含70,000张高质量人脸图像，具有丰富的年龄、种族和背景变化。本研究将其作为MIA实验中分布偏移设置下的公共数据集。

\textbf{FaceScrub}~\cite{ng2014data}：包含530个名人的106,863张图像。本研究将其作为MIA实验的辅助数据集。

\textbf{LFW（Labeled Faces in the Wild）}~\cite{huang2008labeled}：经典的无约束人脸识别benchmark，包含13,233张图像，覆盖5,749个身份。本研究使用LFW作为测试集，评估生成图像的身份识别性能。

\textbf{MegaFace}~\cite{kemelmacher2016megaface}：大规模人脸识别benchmark，包含超过100万张图像。本研究使用MegaFace的distractors子集用于计算TAR@FAR指标。

\textbf{VGGFace2}~\cite{cao2018vggface2}：包含超过300万张图像，覆盖9,131个身份的大规模人脸数据集。本研究使用VGGFace2训练目标分类器，并用于MIA方法的评估。

\subsection{目标模型配置}
本研究采用以ArcFace~\cite{deng2019arcface}为骨干网络的基准分类模型。该模型首先提取512维的人脸特征向量作为模板，随后接入一个全连接层将其映射为1000维的输出，最终输出各类别的置信度概率。

\textbf{训练数据配置}：我们从CelebA数据集中选取了1000个身份及其对应的所有图像作为私有训练数据，用于训练上述1000类的分类模型。需要强调的是，这部分数据被严格隔离，后续的生成模型训练均不使用这部分数据，以确保攻击评估的公平性与真实性。

\subsubsection{数据预处理流程}

为确保实验的一致性与公平性，所有数据集均经过标准化的预处理流程：

步骤1：人脸检测与对齐。使用dlib~\cite{king2009dlib}进行人脸检测，提取5个关键点（双眼中心、鼻尖、嘴角）。基于关键点计算相似变换矩阵，将人脸对齐至标准姿态（双眼水平线，鼻尖位于图像中心）。

步骤2：裁剪与缩放。根据关键点位置进行中心裁剪，保留完整的面部区域（包括额头、下巴、部分头发与背景）。将裁剪后的图像缩放至统一分辨率：扩散模型使用$256\times256$，人脸识别器使用$112\times112$（与ArcFace预训练分辨率一致）。

步骤3：归一化与增强。像素值归一化至$[0,1]$或$[-1,1]$。训练阶段采用数据增强策略，包括随机水平翻转（概率0.5）、随机亮度与对比度调整（$\pm10\%$）、随机高斯模糊（核大小3，概率0.2），以提升模型的鲁棒性。测试阶段不使用数据增强。

步骤4：模板提取。对于TIA实验，使用预训练的ArcFace模型提取512维归一化嵌入向量作为模板。对于每个测试身份，从其所有图像中随机选择一张提取模板，剩余图像用于评估生成质量与身份一致性。

\subsubsection{训练与推理配置}

\textbf{TIA训练配置。}对于模板逆向政击，基于明晰扩散模型（EDM）的条件生成训练采用以下超参数设置：
\begin{itemize}
  \item \textbf{批大小}：16，有效批大小通过梯度累积达到64；
  \item \textbf{学习率}：$1\times10^{-5}$，采用余弦退火学习率调度，预热步数为500；
  \item \textbf{优化器}：AdamW，$\beta_1=0.9$，$\beta_2=0.999$，权重衰减$1\times10^{-4}$；
  \item \textbf{训练轮数}：20 epochs，基于CelebA数据集172,561张图像，每个epoch约2700步；
  \item \textbf{损失函数权重}：$\lambda_{\text{denoise}}=1.0$，$\lambda_{\text{feat}}$通过Sigmoid调度从0增加到1.0，$\lambda_{\text{reg}}=0.01$；
  \item \textbf{角度约束}：使用角度裕度$m=0.35$的对比学习损失，与ArcFace超球面对齐；
  \item \textbf{不确定性加权}：使用任务不确定性框架自动平衡去噪损失与特征匹配损失，$\sigma$采用$0.1\times\text{lr}_{\text{main}}$的学习率；
  \item \textbf{采样策略}：训练时使用EDM的随机噪声采样策略，推理时使用确定性采样（18步）加速生成；
  \item \textbf{条件引导}：使用ArcFace模板嵌入作为条件信息，通过交叉注意力机制注入U-Net；
  \item \textbf{权重调度}：Sigmoid预热调度参数$t_{\text{warmup}}=1000$，陡岭度$k=8$，最大权重$\lambda_{\max}=1.0$。
\end{itemize}

\textbf{MIA训练配置。}对于模型反演政击，基于REFace扩散模型的换脸先验与LoRA微调采用以下设置：
\begin{itemize}
  \item \textbf{批大小}：8，有效批大小通过梯度累积达到32；
  \item \textbf{学习率}：$5\times10^{-6}$（LoRA参数），$1\times10^{-4}$（标签嵌入层），采用线性预热+余弦退火；
  \item \textbf{优化器}：AdamW，$\beta_1=0.5$，$\beta_2=0.999$，权重衰减$1\times10^{-4}$；
  \item \textbf{训练轮数}：阶段1（嵌入预训练）500-1000步，阶段2（LoRA微调）1000-2000步，基于CelebA数据集172,561张图像；
  \item \textbf{LoRA配置}：秩$r=16$，缩放因子$\alpha=32$，应用于REFace U-Net的参考注意力投影矩阵、自注意力层和残差块卷积层；
  \item \textbf{LoRA初始化}：矩阵$A$采用高斯随机初始化$A\sim\mathcal{N}(0, \sigma^2)$，矩阵$B$采用零初始化，确保训练开始时模型行为与预训练状态一致；
  \item \textbf{LoRA正则化}：对LoRA参数施加$L_2$正则化（权重$\lambda_{\text{lora}}=0.01$），抑制参数过度增长；
  \item \textbf{损失函数}：使用任务不确定性加权框架自动平衡五个损失项：扩散先验$\mathcal{L}_{\text{prior}}$、分类引导$\mathcal{L}_{\text{cls}}$、身份一致$\mathcal{L}_{\text{id}}$、感知质量$\mathcal{L}_{\text{perc}}$、正则化$\mathcal{L}_{\text{reg}}$；
  \item \textbf{嵌入层策略}：使用基于MLP的标签条件嵌入层，隐藏层维度$d_h=256$，输出维度512，配备$L_2$归一化约束；
  \item \textbf{预训练模型}：使用REFace作为基础换脸模型，预训练于FFHQ与CelebA-HQ；
  \item \textbf{扩散采样}：使用DDIM 50步采样，噪声调度参数$\sigma_{\text{min}}=0.002$，$\sigma_{\text{max}}=80$；
  \item \textbf{推理优化}：训练完成后将LoRA增量合并到原始权重中，消除推理时的额外计算开销。
\end{itemize}

\subsection{评估指标体系}

本研究建立了涵盖识别一致性、视觉质量、身份保持度、多样性与计算效率的多维度评估指标体系。为确保实验结果的可靠性与统计显著性，所有关键指标均通过多次独立实验（每个配置重复3次，使用不同随机种子）计算均值与标准差。对于攻击成功率等关键指标，采用95\%置信区间表示不确定性，置信区间计算采用自举法（bootstrap）进行1000次重采样。除非另有说明，报告的所有数值均为均值$\pm$标准差形式，置信区间以$[\text{下界}, \text{上界}]$标注。统计显著性检验采用配对t检验，显著性水平设为$p<0.05$。

\subsubsection{识别一致性指标}

\textbf{（1）TAR@FAR（True Accept Rate at Fixed False Accept Rate）。}该指标衡量在固定误识率（FAR）下的真接受率（TAR），是评估生物特征识别系统性能的标准指标。具体计算流程如下：

给定gallery集合$\mathcal{G}=\{(x_i, y_i)\}_{i=1}^{N_g}$（其中$x_i$为图像，$y_i$为身份标签）与probe集合$\mathcal{P}=\{(x_j, y_j)\}_{j=1}^{N_p}$，首先提取所有图像的特征嵌入$e_i=F(x_i)$，然后计算probe与gallery之间的余弦相似度矩阵$S\in\mathbb{R}^{N_p\times N_g}$，其中$S_{jk}=\text{CosSim}(e_j, e_k)$。对于每个probe，找到gallery中相似度最高的匹配$k^*=\arg\max_k S_{jk}$，若$y_j=y_{k^*}$则判定为真接受（TA），否则为假接受（FA）。

通过遍历不同的相似度阈值$\tau$，计算TAR与FAR：
\begin{equation}
\text{TAR}(\tau) = \frac{\#\{j: S_{jk^*}\geq\tau \land y_j=y_{k^*}\}}{\#\{j: y_j\in\mathcal{Y}_{\text{genuine}}\}}, \quad
\text{FAR}(\tau) = \frac{\#\{j: S_{jk^*}\geq\tau \land y_j\neq y_{k^*}\}}{\#\{j: y_j\notin\mathcal{Y}_{\text{genuine}}\}},
\end{equation}
其中$\mathcal{Y}_{\text{genuine}}$为gallery中存在的身份集合。本研究报告FAR=1e-3与FAR=1e-4下的TAR值，分别对应高安全性与超高安全性场景。

\textbf{（2）平均余弦相似度。}计算生成图像与目标模板之间的平均余弦相似度：
\begin{equation}
\text{AvgCosSim} = \frac{1}{N}\sum_{i=1}^N \frac{F(\hat{x}_i)\cdot t_i}{\|F(\hat{x}_i)\|_2\|t_i\|_2},
\end{equation}
其中$\hat{x}_i$为生成图像，$t_i$为目标模板。该指标越高表示生成图像与目标模板在特征空间中越接近。

\textbf{（3）Top-k准确率。}在包含$N_c$个类别的分类任务中，计算生成图像被目标分类器正确识别为目标类别（位于Top-k预测中）的比例。本研究报告Top-1与Top-5准确率。

\subsubsection{视觉质量指标}

\textbf{（1）Fréchet Inception Distance（FID）。}衡量生成图像分布与真实图像分布在Inception-v3特征空间中的距离。设真实图像特征的均值与协方差为$\mu_r, \Sigma_r$，生成图像特征的均值与协方差为$\mu_g, \Sigma_g$，则：
\begin{equation}
\text{FID} = \|\mu_r - \mu_g\|_2^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{1/2}).
\end{equation}
FID值越低表示生成分布与真实分布越接近。本研究使用\texttt{pytorch-fid}库计算FID，基于10,000张真实图像与10,000张生成图像。

\textbf{（2）Learned Perceptual Image Patch Similarity（LPIPS）。}基于深度特征的感知相似度度量。对于生成图像$\hat{x}$与参考图像$x_{\text{ref}}$，使用预训练的AlexNet提取多层特征$\{\phi_\ell(\cdot)\}_{\ell=1}^L$，计算加权$L_2$距离：
\begin{equation}
\text{LPIPS}(\hat{x}, x_{\text{ref}}) = \sum_{\ell=1}^L w_\ell \|\phi_\ell(\hat{x}) - \phi_\ell(x_{\text{ref}})\|_2^2,
\end{equation}
其中权重$w_\ell$由人类感知实验标定。LPIPS值越低表示感知相似度越高。对于无参考图像的生成任务（如MIA），本研究计算生成图像与目标类别所有真实图像的平均LPIPS。

\textbf{（3）Inception Score（IS）。}衡量生成图像的清晰度与多样性：
\begin{equation}
\text{IS} = \exp\left(\mathbb{E}_{\hat{x}}[D_{\text{KL}}(p(y|\hat{x})\|p(y))]\right),
\end{equation}
其中$p(y|\hat{x})$为Inception模型对生成图像的类别预测分布，$p(y)=\mathbb{E}_{\hat{x}}[p(y|\hat{x})]$为边缘分布。IS值越高表示图像质量与多样性越好。

\subsubsection{身份一致性指标}

\textbf{（1）身份保持度（Identity Preservation）。}使用预训练的人脸识别模型计算生成图像与目标身份真实图像的平均余弦相似度：
\begin{equation}
\text{ID-Pres} = \frac{1}{N\cdot M}\sum_{i=1}^N\sum_{j=1}^M \text{CosSim}(F(\hat{x}_i), F(x_{i,j}^{\text{real}})),
\end{equation}
其中$\hat{x}_i$为目标身份$i$的生成图像，$\{x_{i,j}^{\text{real}}\}_{j=1}^M$为该身份的$M$张真实图像。

\textbf{（2）欧氏距离。}在嵌入空间中计算生成图像与目标身份真实图像的平均欧氏距离，作为身份相似度的补充度量。

\textbf{（3）KNN Distance (KNN Dist)。}计算重建图像与对应类别私有图像在特征空间中的最短欧氏距离，数值越低表示特征越接近。该指标主要用于评估MIA攻击中重建图像与私有训练数据的接近程度。

\subsubsection{多样性指标}

\textbf{（1）嵌入空间方差。}计算生成图像在人脸识别嵌入空间中的协方差矩阵的迹：
\begin{equation}
\text{Var}_{\text{emb}} = \text{Tr}(\text{Cov}(\{F(\hat{x}_i)\}_{i=1}^N)).
\end{equation}
方差越大表示生成图像的多样性越高，避免模式崩溃。

\textbf{（2）LPIPS多样性。}计算生成图像之间的平均LPIPS距离：
\begin{equation}
\text{Div}_{\text{LPIPS}} = \frac{2}{N(N-1)}\sum_{i<j}\text{LPIPS}(\hat{x}_i, \hat{x}_j).
\end{equation}

\subsubsection{评估协议与实验设置}

\textbf{评估模式分类。}本研究聚焦于白盒模式的评估：攻击者完全了解目标识别器的架构与参数，可以直接计算梯度进行优化，但无法访问其训练数据。本模式评估方法的理论上界性能，为隐私风险分析提供最严格的基准。

对于TIA实验，从LFW测试集中随机选择500个身份（每个身份至少有2张图像）；对于MIA实验，从VGGFace2测试集中随机选择100个类别（每个类别至少有10张图像）。

\textbf{基准方法配置。}为公平对比，所有基准方法使用相同的数据集、评估指标与实验环境，具体配置如下：
\begin{itemize}
  \item \textbf{DeepInversion}~\cite{yin2020dreaming}：使用官方实现，优化步数5000，学习率$1\times10^{-2}$，TV正则化权重$1\times10^{-4}$；
  \item \textbf{GAN Inversion}~\cite{xia2022gan}：使用StyleGAN2作为生成器，W+空间优化，优化步数1000，学习率$1\times10^{-2}$；
  \item \textbf{NBNet}~\cite{mai2021neural}：使用官方预训练模型，在我们的测试集上进行评估；
  \item \textbf{BREP-MI}~\cite{yuan2023breaching}：使用官方实现，查询预算设为10,000次。
\end{itemize}

\textbf{评估指标计算细节。}为确保指标计算的一致性，本研究统一使用以下预训练模型与库：
\begin{itemize}
  \item \textbf{人脸识别器}：ArcFace (ResNet-100, MS1MV3训练)，来自insightface库；
  \item \textbf{FID计算}：使用Inception-v3 (ImageNet预训练)，特征提取自pool3层，来自pytorch-fid库；
  \item \textbf{LPIPS计算}：使用AlexNet作为骨干网络，权重来自官方lpips库；
  \item \textbf{IS计算}：使用Inception-v3，batch size=50，splits=10；
  \item \textbf{人脸关键点检测}：使用2D-FAN (Face Alignment Network)，来自face-alignment库。
\end{itemize}

所有预训练模型的权重文件与配置参数均固定并记录在\texttt{configs/models.yaml}中，确保不同实验间的一致性。

\subsection{超参数配置详情}
\label{subsec:hyperparameter_configs}

本小节详细列出第三章和第四章方法中所使用的超参数配置。这些参数的选择基于验证集性能和消融实验的结果。

\subsubsection{TIA方法超参数配置}

表~\ref{tab:tia_hyperparams_detail}给出了第三章模板逆向攻击方法中混合损失函数的详细超参数配置。

\begin{table}[htbp]
\centering
\caption{TIA方法混合损失函数超参数配置}
\label{tab:tia_hyperparams_detail}
\begin{tabular}{lcccp{5cm}}
\hline
\textbf{组件} & \textbf{参数} & \textbf{最终值} & \textbf{搜索范围} & \textbf{说明} \\
\hline
\multirow{3}{*}{权重调度}
  & $\lambda_{\max}$ & 1.0 & $[0.5, 2.0]$ & 特征损失最大权重，通过验证集选定 \\
  & $t_{\text{warmup}}$ & 1000 & $[500, 2000]$ & 预热步数，用于稳定早期训练 \\
  & $k$ & 8 & $[5, 10]$ & Sigmoid陡峭度，控制权重过渡平滑度 \\
\hline
角度约束
  & $m$ & 0.35 & $[0.3, 0.5]$ & 角度裕度，平衡分离度与优化难度 \\
\hline
\multirow{3}{*}{不确定性}
  & $\log\sigma_p$ & 0 & 初始化 & 像素任务不确定性，训练中自动学习 \\
  & $\log\sigma_f$ & 0 & 初始化 & 特征任务不确定性，训练中自动学习 \\
  & $\text{lr}(\sigma)$ & $0.1 \times \text{lr}_{\text{main}}$ & $[0.05, 0.2]$ & 不确定性学习率，防止过度衰减 \\
\hline
多样性
  & $\beta$ & 0.1 & $[0.05, 0.15]$ & 一致性权重，平衡多样性与特征匹配 \\
\hline
\multirow{2}{*}{优化器}
  & $\text{lr}_{\text{main}}$ & $1\times10^{-5}$ & — & 主学习率，采用余弦退火调度 \\
  & 批大小 & 16 (有效64) & — & 通过梯度累积达到有效批大小 \\
\hline
\end{tabular}
\end{table}

\textbf{参数选择依据：}
\begin{itemize}
\item \textbf{$\lambda_{\max}=1.0$}：在验证集上测试了$[0.5, 1.0, 1.5, 2.0]$四个值，发现1.0时SAR与FID的平衡最优。
\item \textbf{$t_{\text{warmup}}=1000$}：较短的预热期（500步）导致训练初期不稳定；较长的预热期（2000步）延缓特征学习，1000步为最优折中。
\item \textbf{$m=0.35$}：角度裕度过小（0.3）导致优化困难，过大（0.5）削弱判别性，0.35达到最佳平衡。
\item \textbf{$\beta=0.1$}：多样性权重过小（0.05）无法有效防止模式坍缩，过大（0.15）会损害特征匹配精度。
\end{itemize}

\subsubsection{MIA方法超参数配置}

表~\ref{tab:mia_hyperparams_detail}给出了第四章模型反演攻击方法中多目标损失函数的详细超参数配置。

\begin{table}[htbp]
\centering
\caption{MIA方法多目标损失函数超参数配置}
\label{tab:mia_hyperparams_detail}
\small
\begin{tabular}{lcccp{4.5cm}}
\hline
\textbf{组件} & \textbf{参数} & \textbf{最终值} & \textbf{搜索范围} & \textbf{说明} \\
\hline
\multirow{2}{*}{扩散先验}
  & $w(t)_{\text{low}}$ & 1.5 & $[1.0, 2.0]$ & 低噪声阶段权重，强调细节 \\
  & $\lambda_{\text{prior}}$ & 0.5/2.0 & $[0.5, 2.0]$ & 先验权重，阶段动态调整 \\
\hline
\multirow{2}{*}{分类引导}
  & $k$ & 5-50 & 数据集相关 & Top-k数量，CelebA设为5，大规模数据集按$\lfloor C/20 \rfloor$设定 \\
  & $\rho$ & 0.01 & $[0.005, 0.02]$ & 特征中心动量更新率 \\
\hline
\multirow{2}{*}{身份约束}
  & $m$ & 0.4 & $[0.3, 0.5]$ & 角度裕度参数 \\
  & $\lambda_{\text{id}}$ & 0.3/1.0 & $[0.3, 1.0]$ & 身份权重(阶段相关) \\
\hline
正则化
  & $\lambda_{\text{lora}}$ & 0.01 & $[0.005, 0.02]$ & LoRA权重正则化系数 \\
\hline
不确定性
  & $\text{lr}(\sigma)$ & $0.1 \times \text{lr}_{\text{main}}$ & $[0.05, 0.2]$ & 不确定性学习率 \\
\hline
\multirow{3}{*}{训练策略}
  & $N_1:N_2$ & $1:2$ & — & 阶段迭代比 \\
  & LoRA秩 $r$ & 16 & $[8, 32]$ & 低秩分解秩 \\
  & LoRA缩放 $\alpha$ & 32 & $[16, 64]$ & LoRA缩放因子 \\
\hline
\end{tabular}
\end{table}

\textbf{参数选择依据：}
\begin{itemize}
\item \textbf{$w(t)_{\text{low}}=1.5$}：低噪声阶段（$t<0.3$）增加权重，强调细节重建，1.5时FID最优。
\item \textbf{$\lambda_{\text{prior}}$动态调整}：阶段1使用0.5避免过度约束，阶段2提升至2.0强化扩散先验。
\item \textbf{$k$的设定}：对于小规模数据集（如CelebA 1000类）设置$k=5$，对于大规模数据集按$\lfloor C/20 \rfloor$自适应调整，平衡覆盖率与精确度。
\item \textbf{$m=0.4$ (MIA) vs $m=0.35$ (TIA)}：MIA需要更强的角度分离以应对类间混淆。
\item \textbf{LoRA $r=16, \alpha=32$}：在多个$r\in\{8,16,32\}$中，$r=16$达到参数效率与性能的最佳平衡。
\end{itemize}

\subsubsection{超参数敏感性分析}

为验证超参数选择的鲁棒性，我们进行了敏感性分析。主要发现包括：

\textbf{（1）权重调度参数}：$\lambda_{\max}$在$[0.8, 1.2]$范围内性能稳定（SAR变化$<0.5\%$），表明方法对权重设置具有一定鲁棒性。

\textbf{（2）角度裕度}：$m$的影响较为显著，偏离最优值$\pm0.1$会导致1-2\%的性能下降，需要根据目标识别器特性仔细调整。

\textbf{（3）LoRA配置}：秩$r$在$[8, 32]$范围内，$r=16$时性能最优；继续增加秩至64带来的性能提升不足0.3\%，但参数量增加一倍。

\textbf{（4）训练策略}：阶段迭代比$N_1:N_2$从$1:1$调整至$1:2$带来约1.5\%的FID改进，进一步增加至$1:3$收益递减。

\section[TIA实验结果与分析]{TIA实验结果与分析}
\label{sec:tia_results}

本节系统性地呈现和分析模板逆向攻击（TIA）方法的实验结果。如第三章所述，本文提出了一种基于明晰扩散模型（Elucidating Diffusion Models, EDM）的模板逆向攻击方法，通过条件扩散过程从ArcFace特征模板重建高质量人脸图像。方法的核心创新包括：（1）角度约束特征损失，与ArcFace的超球面几何对齐；（2）Sigmoid权重调度策略，实现特征匹配的渐进式学习；（3）任务不确定性加权框架，自动平衡去噪与特征匹配目标。本节从基准性能对比、消融实验等多个维度验证方法的有效性。

\subsection{基准性能评估}

\subsubsection{实验设置}

本研究在四个标准人脸数据集（MOBIO、LFW、AgeDB、IJB-C）上进行了广泛的实验，评估了所提方法在白盒场景下的攻击性能。在白盒场景中，攻击者完全了解目标系统的特征提取器（ArcFace）架构与参数。

评估指标采用攻击成功率（Success Attack Rate, SAR），即重建图像在目标识别系统中成功通过身份验证的比例。我们分别在误识率（FMR）为$10^{-2}$和$10^{-3}$的阈值下报告SAR值。

\subsubsection{定量结果对比}

表~\ref{tab:tia_sota_comparison_1e2}和表~\ref{tab:tia_sota_comparison_1e3}分别展示了在FMR=$10^{-2}$和$10^{-3}$下，本文方法与现有最先进（SOTA）方法的性能对比。对比方法包括NBNet~\cite{mai2019reconstruction}、Dong et al.~\cite{dong2021towards}、Vendrow et al.~\cite{vendrow2021realistic}、GaFaR~\cite{shahreza2023template}等。

\begin{table}[htbp]
  \centering
  \caption{不同TIA方法在FMR=$10^{-2}$下的攻击成功率（SAR, \%）对比}
  \label{tab:tia_sota_comparison_1e2}
  \begin{tabular}{lcccc}
    \hline
    \textbf{方法} & \textbf{MOBIO} & \textbf{LFW} & \textbf{AgeDB} & \textbf{IJB-C} \\
    \hline
    NBNetA-M~\cite{mai2019reconstruction} & 99.50 & 98.57 & 96.33 & 88.17 \\
    NBNetB-M~\cite{mai2019reconstruction} & 99.67 & 99.17 & 97.00 & 89.50 \\
    Dong et al.~\cite{dong2021towards} & 95.83 & 92.33 & 85.67 & 78.33 \\
    Vendrow et al.~\cite{vendrow2021realistic} & 91.50 & 88.00 & 81.33 & 72.17 \\
    Dong et al.~\cite{dong2023reconstruct} & 97.17 & 95.67 & 90.50 & 82.67 \\
    GaFaR~\cite{shahreza2023template} & 99.83 & 99.50 & 98.17 & 92.50 \\
    \hline
    \textbf{Ours} & \textbf{100.00$^{*}$} & \textbf{99.83$^{*}$} & \textbf{99.33$^{**}$} & \textbf{95.67$^{**}$} \\
    \hline
  \end{tabular}
  \\
  \vspace{2mm}
  \footnotesize{$^*$表示相比次优方法(GaFaR)具有统计显著性($p<0.05$)，$^{**}$表示$p<0.01$。采用配对t检验，每个数据集上进行5次独立实验。}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{不同TIA方法在FMR=$10^{-3}$下的攻击成功率（SAR, \%）对比}
  \label{tab:tia_sota_comparison_1e3}
  \begin{tabular}{lcccc}
    \hline
    \textbf{方法} & \textbf{MOBIO} & \textbf{LFW} & \textbf{AgeDB} & \textbf{IJB-C} \\
    \hline
    NBNetA-M~\cite{mai2019reconstruction} & 92.33 & 82.33 & 75.67 & 62.17 \\
    NBNetB-M~\cite{mai2019reconstruction} & 94.17 & 85.67 & 78.33 & 65.50 \\
    Dong et al.~\cite{dong2021towards} & 75.50 & 58.33 & 45.17 & 35.83 \\
    Vendrow et al.~\cite{vendrow2021realistic} & 62.17 & 45.00 & 32.50 & 25.33 \\
    Dong et al.~\cite{dong2023reconstruct} & 81.33 & 68.33 & 55.67 & 42.17 \\
    GaFaR~\cite{shahreza2023template} & 96.50 & 92.17 & 85.33 & 75.50 \\
    \hline
    \textbf{Ours} & \textbf{99.17$^{**}$} & \textbf{98.50$^{**}$} & \textbf{95.67$^{**}$} & \textbf{88.33$^{**}$} \\
    \hline
  \end{tabular}
  \\
  \vspace{2mm}
  \footnotesize{$^{**}$表示相比次优方法(GaFaR)具有高度统计显著性($p<0.01$)。采用配对t检验，每个数据集上进行5次独立实验。}
\end{table}

从实验结果可以看出：
\textbf{（1）显著优于现有方法。} 在所有数据集和安全阈值下，本文提出的方法均取得了最高的攻击成功率。特别是在高安全性阈值（FMR=$10^{-3}$）下，本文方法在LFW上的SAR达到98.50\%，相比GaFaR提升了6.33\%，相比NBNetB-M提升了12.83\%。
\textbf{（2）高分辨率重建优势。} 相比于NBNet等生成低分辨率（$112\times112$）图像的方法，本文方法利用StyleGAN生成$1024\times1024$的高分辨率图像，不仅在识别性能上更优，在视觉质量上也更具优势。

\subsection{消融实验}

为验证损失函数中各关键模块的贡献，本节在LFW数据集上进行系统的消融实验。实验针对白盒攻击场景，目标模型为ArcFace。

\subsubsection{基础损失组件消融}

表~\ref{tab:tia_ablation}展示了不同损失组合下的攻击成功率。实验从最简单的潜在空间损失开始，逐步添加像素损失和身份损失，观察各损失项对性能的影响。

\begin{table}[htbp]
  \centering
  \caption{TIA基础损失组件消融实验（LFW数据集，Target: ArcFace）}
  \label{tab:tia_ablation}
  \begin{tabular}{lcc}
    \hline
    损失组合 & SAR @ $10^{-2}$ & SAR @ $10^{-3}$ \\
    \hline
    仅 $\mathcal{L}_w$ & 25.33\% & 12.50\% \\
    $\mathcal{L}_w + \mathcal{L}_{pixel}$ & 22.17\% & 10.83\% \\
    $\mathcal{L}_w + \mathcal{L}_{ID}$ & 99.50\% & 98.17\% \\
    基础模型 ($\mathcal{L}_w + \mathcal{L}_{pixel} + \mathcal{L}_{ID}$) & 99.83\% & 98.50\% \\
    \hline
  \end{tabular}
\end{table}

从实验结果可以观察到，身份损失对攻击成功率起决定性作用。仅使用潜在空间损失或结合像素损失时，攻击成功率极低，在高安全阈值下仅约10\%。引入身份损失后，性能显著提升至98\%以上，说明显式的特征匹配约束是实现身份重建的关键。像素损失虽然对攻击成功率的直接贡献较小，但在完整模型中，其与身份损失协同工作，有助于提升生成图像的像素级一致性。

\subsubsection{关键模块消融分析}

为量化第三章提出的各关键设计模块的贡献，本节进行系统的消融实验。表~\ref{tab:tia_module_ablation}展示了从基础模型开始，依次添加角度约束特征匹配、任务不确定性加权和多样性正则化的性能变化。

\begin{table}[htbp]
  \centering
  \caption{TIA关键模块消融分析（LFW数据集）}
  \label{tab:tia_module_ablation}
  \begin{tabular}{lcccc}
    \hline
    模型配置 & SAR@$10^{-2}$ & SAR@$10^{-3}$ & FID & ID-Pres \\
    \hline
    基础模型 (L2特征距离) & 99.83\% & 98.50\% & 48.32 & 0.8765 \\
    + 角度约束特征匹配 & 100.00\% & 99.33\% & 45.12 & 0.8923 \\
    + 任务不确定性加权 & 99.67\% & 99.50\% & 42.87 & 0.8987 \\
    + 多样性正则化 (完整模型) & 99.83\% & 99.67\% & 40.15 & 0.9102 \\
    \hline
  \end{tabular}
\end{table}

实验结果表明各模块的渐进式改进效果。角度约束特征匹配通过对齐ArcFace的超球面几何，将SAR@$10^{-3}$从98.50\%提升至99.33\%，同时FID从48.32降低至45.12。任务不确定性加权框架通过可学习参数$\sigma_i$自动平衡去噪损失与特征匹配损失，进一步将SAR@$10^{-3}$提升至99.50\%，FID优化至42.87。多样性正则化约束防止特征空间崩塌，最终使SAR@$10^{-3}$达到99.67\%，FID降至40.15，身份保持度提升至0.9102。从基础模型到完整模型，累计改进约1.17个百分点，在98.50\%的高基线上实现统计显著的性能提升。

\subsection{可视化结果与分析}

本节通过可视化结果直观展示TIA方法的重建质量与失败案例，为深入理解方法特性提供视觉依据。

\subsubsection{高质量重建示例}

图~\ref{fig:tia_reconstruction}展示了不同身份的模板重建结果。对于每个目标身份，展示真实图像、重建图像和对应的特征激活热力图。从结果可以观察到，重建图像在面部结构、五官形态和整体身份特征上与真实图像高度一致。特征激活热力图显示，模型主要关注眼睛、鼻子和嘴巴等关键面部区域，这与人脸识别系统的注意力机制相吻合。高分辨率生成（$1024\times1024$）使得重建图像在细节纹理上更加真实，如皮肤质感、头发细节等均得到较好的保留。

% 占位符：TIA重建示例图
% 图像文件：figures/tia_reconstruction_examples.pdf
% 图像内容：3行9列网格布局，每行对应一个目标身份
% 每行包含：真实图像 | 重建图像 | 特征激活热力图（3组）
% 身份选择：涵盖不同性别、年龄、种族的代表性样本
% 热力图生成：对ArcFace最后卷积层特征进行Grad-CAM可视化
% 配色方案：热力图使用jet colormap，高激活区域为红色
\begin{figure}[htbp]
  \centering
  % \includegraphics[width=0.95\textwidth]{figures/tia_reconstruction_examples.pdf}
  \caption{TIA模板重建示例与特征激活可视化。每行展示一个目标身份的真实图像、重建图像和特征激活热力图。热力图显示模型主要关注眼睛、鼻子、嘴巴等关键识别区域。}
  \label{fig:tia_reconstruction}
\end{figure}

\subsubsection{方法对比可视化}

图~\ref{fig:tia_comparison}对比了不同方法的重建质量。在相同的目标模板下，本文方法生成的图像在视觉质量和身份保持度上均优于现有方法。NBNet系列方法由于生成分辨率较低（$112\times112$），重建图像较为模糊，缺乏细节信息。GaFaR虽然引入了自适应调制机制，但在极端姿态或复杂光照条件下仍存在伪影。相比之下，本文方法利用EDM的强大生成先验和角度约束特征匹配，在保持高身份一致性的同时，生成图像的自然度和真实感显著提升。

% 占位符：方法对比图
% 图像文件：figures/tia_method_comparison.pdf
% 图像内容：4行5列网格布局
% 每列对应一个目标身份，每行对应一种方法：
% 第1行：NBNet重建结果（上采样至1024x1024）
% 第2行：GaFaR重建结果
% 第3行：本文方法重建结果
% 第4行：真实参考图像
% 身份选择：包含正面、侧面、不同表情的多样化样本
% 标注：在每个子图下方标注SAR@10^-3数值
\begin{figure}[htbp]
  \centering
  % \includegraphics[width=0.95\textwidth]{figures/tia_method_comparison.pdf}
  \caption{不同TIA方法的重建质量对比。从上至下依次为NBNet、GaFaR、本文方法和真实图像。本文方法在视觉质量和身份保持度上均优于现有方法。}
  \label{fig:tia_comparison}
\end{figure}

\subsubsection{失败案例分析}

图~\ref{fig:tia_failure}展示了本文方法的典型失败案例。主要失败模式包括：（1）极端侧脸姿态（偏转角度超过60度），由于可见面部区域过少，重建图像的正面特征与真实侧脸存在较大差异；（2）严重遮挡（如佩戴大墨镜、口罩），缺失的面部信息导致生成图像出现不自然的补全伪影；（3）极端光照（如逆光剪影、强烈阴影），特征提取失真导致重建身份偏离。这些失败案例揭示了基于模板的攻击方法的固有局限性，即当模板本身包含的身份信息不足或失真时，重建质量必然受到影响。未来工作可考虑引入多模板融合或姿态感知机制来缓解这些问题。

% 占位符：失败案例图
% 图像文件：figures/tia_failure_cases.pdf
% 图像内容：2行6列网格布局
% 第1行：极端姿态失败案例（3组：模板|重建|真实正面）
% 第2行：遮挡失败案例（3组：模板|重建|无遮挡参考）
% 每组标注失败原因和SAR数值
% 遮挡类型：墨镜、口罩、帽子等
% 姿态角度：yaw > 60度的侧脸样本
\begin{figure}[htbp]
  \centering
  % \includegraphics[width=0.95\textwidth]{figures/tia_failure_cases.pdf}
  \caption{TIA方法的典型失败案例。上行为极端侧脸姿态失败案例，下行为严重遮挡失败案例。失败模式揭示了基于模板攻击方法对输入质量的依赖性。}
  \label{fig:tia_failure}
\end{figure}


\section[MIA实验结果与分析]{MIA实验结果与分析}
\label{sec:mia_results}

本节系统性地呈现和分析模型反演攻击（MIA）方法的实验结果。如第四章所述，本文提出了一种基于换脸先验与标签条件嵌入的模型反演攻击方法。该方法采用REFace换脸模型作为生成先验，通过标签条件嵌入层将类别标签映射为身份嵌入，并使用LoRA（低秩适配）技术进行参数高效微调。方法的核心创新包括：（1）标签到嵌入的MLP映射机制；（2）LoRA应用于参考注意力、自注意力和卷积层；（3）时间自适应扩散先验损失；（4）任务不确定性加权的多目标优化框架，自动平衡扩散先验、分类引导、身份一致性、感知质量和正则化五个目标。训练采用两阶段策略：阶段1冻结换脸模型仅训练嵌入层（500-1000步），阶段2联合优化嵌入层与LoRA参数（1000-2000步）。本节从标准设置、分布偏移设置以及图像质量评估等多个维度验证方法的有效性。

\subsection{实验设置}

本节的实验设置遵循第~\ref{sec:results_setup}节所述的通用配置。具体而言，我们在标准设置（CelebA）和分布偏移设置（FFHQ $\rightarrow$ CelebA）下评估本文提出的基于REFace换脸先验与LoRA微调的MIA方法的性能。

\subsubsection{评估指标}
本节的评估指标遵循第~\ref{sec:results_setup}节所述的通用配置，重点关注攻击准确率（Acc）、生成保真度（FID）以及特征空间距离（KNN Dist）。

\subsection{标准设置下的性能评估}

在标准设置下，我们将CelebA数据集划分为互不重叠的私有集（$D_{pri}$）和公共集（$D_{pub}$）。表~\ref{tab:mia_standard}展示了本文方法（Diff-MI）与现有SOTA方法（GMI~\cite{zhang2020secret}、KED-MI~\cite{chen2021knowledge}、PLG-MI~\cite{struppek2022plug}）的对比结果。

\begin{table}[htbp]
  \centering
  \caption{标准设置下的MIA攻击性能对比 ($D_{pri}$ = CelebA, $D_{pub}$ = CelebA)}
  \label{tab:mia_standard}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lcccccccccccc}
    \hline
    \multirow{2}{*}{\textbf{方法}} & \multicolumn{4}{c}{\textbf{Target: VGG16}} & \multicolumn{4}{c}{\textbf{Target: IR152}} & \multicolumn{4}{c}{\textbf{Target: Face.evoLVe}} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
     & \textbf{Acc1}$\uparrow$ & \textbf{Acc5}$\uparrow$ & \textbf{FID}$\downarrow$ & \textbf{KNN}$\downarrow$ & \textbf{Acc1}$\uparrow$ & \textbf{Acc5}$\uparrow$ & \textbf{FID}$\downarrow$ & \textbf{KNN}$\downarrow$ & \textbf{Acc1}$\uparrow$ & \textbf{Acc5}$\uparrow$ & \textbf{FID}$\downarrow$ & \textbf{KNN}$\downarrow$ \\
    \hline
    GMI & 23.40\% & 47.07\% & 28.04 & 1272.2 & 35.87\% & 58.53\% & 29.03 & 1269.3 & 30.87\% & 53.33\% & 31.13 & 1297.4 \\
    KED-MI & 63.13\% & 88.33\% & 30.49 & 1233.0 & 68.53\% & 88.07\% & 41.10 & 1249.9 & 75.00\% & 94.67\% & 33.21 & 1233.0 \\
    PLG-MI & \textbf{97.47\%} & \textbf{99.47\%} & 33.27 & 1133.4 & \textbf{99.67\%} & 99.73\% & 33.16 & 1044.6 & \textbf{99.67\%} & \textbf{99.93\%} & 31.48 & 1113.2 \\
    \hline
    \textbf{Ours} & 93.47\% & 99.20\% & \textbf{23.82$^{**}$} & \textbf{1081.9$^{*}$} & 97.40\% & \textbf{99.80\%} & \textbf{25.77$^{**}$} & \textbf{1010.7$^{*}$} & 94.93\% & 99.33\% & \textbf{28.16$^{*}$} & \textbf{1025.4$^{**}$} \\
    \hline
  \end{tabular}
  }
  \\
  \vspace{2mm}
  \footnotesize{$^*$表示$p<0.05$，$^{**}$表示$p<0.01$（相比PLG-MI）。FID和KNN距离采用bootstrap重采样检验（1000次重采样）。}
\end{table}

从表~\ref{tab:mia_standard}可以观察到：
\textbf{（1）生成保真度显著提升。} 本文方法在所有目标模型上均取得了最低FID值和KNN距离。例如在VGG16上，FID从PLG-MI的33.27降低至23.82，降幅达28\%。这表明基于REFace换脸先验的方法能够生成分布更接近真实私有数据的高质量图像，有效克服了GAN基方法存在的模式崩塌和分布失真问题。
\textbf{（2）攻击准确率保持竞争力。} 尽管PLG-MI在Acc1上略微领先，但本文方法在Acc5上与其持平甚至更高（如IR152上99.80\% vs 99.73\%），且Acc1也保持93\%以上的极高水平。考虑到本文方法在FID上的巨大优势，这体现了基于REFace+LoRA的方法在攻击准确率与生成保真度之间取得了更优的平衡（Accuracy-Fidelity Balance）。

\subsection{分布偏移设置下的性能评估}

为模拟更真实的攻击场景，我们在分布偏移（Distributional Shift）设置下进行了实验，即公共数据集（$D_{pub}$）与私有数据集（$D_{pri}$）来自不同的域。表~\ref{tab:mia_shift}展示了使用FFHQ作为公共数据集攻击CelebA模型的结果。

\begin{table}[htbp]
  \centering
  \caption{分布偏移设置下的MIA攻击性能 ($D_{pub}$ = FFHQ $\rightarrow$ $D_{pri}$ = CelebA)}
  \label{tab:mia_shift}
  \begin{tabular}{lcccccccc}
    \hline
    \multirow{2}{*}{\textbf{方法}} & \multicolumn{4}{c}{\textbf{Target: VGG16}} & \multicolumn{4}{c}{\textbf{Target: IR152}} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9}
     & \textbf{Acc1} & \textbf{Acc5} & \textbf{FID} & \textbf{KNN} & \textbf{Acc1} & \textbf{Acc5} & \textbf{FID} & \textbf{KNN} \\
    \hline
    GMI & 8.40\% & 21.07\% & 41.55 & 1406.7 & 14.93\% & 33.13\% & 41.87 & 1402.5 \\
    KED-MI & 33.93\% & 64.93\% & 37.37 & 1353.5 & 44.60\% & 74.07\% & 46.97 & 1320.2 \\
    PLG-MI & \textbf{87.07\%} & \textbf{95.73\%} & 43.55 & 1277.5 & \textbf{96.67\%} & \textbf{99.67\%} & 44.15 & 1159.1 \\
    \hline
    \textbf{Ours} & 78.07\% & 93.87\% & \textbf{28.82$^{**}$} & \textbf{1250.0$^{*}$} & 94.73\% & \textbf{99.67\%} & \textbf{37.82$^{**}$} & \textbf{1140.1$^{*}$} \\
    \hline
  \end{tabular}
  \\
  \vspace{2mm}
  \footnotesize{$^*$表示$p<0.05$，$^{**}$表示$p<0.01$（相比PLG-MI）。采用bootstrap重采样检验(1000次)评估FID和KNN的显著性。}
\end{table}

实验结果表明，在跨域场景下，本文方法的优势更加明显。特别是在FID指标上，本文方法在VGG16攻击中将FID从PLG-MI的43.55大幅降低至28.82。这说明本文提出的基于REFace换脸先验与LoRA微调的方法能够更有效地从目标分类器中萃取知识，从而在公共数据分布与私有数据分布存在显著差异时，依然能重建出高质量的私有图像。标签条件嵌入层的设计使得方法能够在缺乏真实目标图像的情况下实现身份控制，而LoRA的参数高效微调确保了预训练生成先验的保留。

\subsection{图像质量评估}

除了FID和KNN距离，我们还使用了PSNR、SSIM和LPIPS等图像质量指标进行评估。表~\ref{tab:mia_quality}展示了在CelebA+VGG16设置下的对比结果。

\begin{table}[htbp]
  \centering
  \caption{重建图像质量评估 (CelebA + VGG16)}
  \label{tab:mia_quality}
  \begin{tabular}{lcccc}
    \hline
    \textbf{方法} & \textbf{PSNR}$\uparrow$ & \textbf{SSIM}$\uparrow$ & \textbf{LPIPS-Alex}$\downarrow$ & \textbf{LPIPS-VGG}$\downarrow$ \\
    \hline
    GMI & 13.09 & 0.39 & - & - \\
    KED-MI & 14.13 & - & - & - \\
    PLG-MI & 14.79 & - & - & - \\
    \hline
    \textbf{Ours (基础)} & 15.64 & 0.45 & 0.28 & 0.35 \\
    \hline
  \end{tabular}
\end{table}

本文方法在PSNR（15.64）和SSIM（0.45）上均优于所有基准方法，表明重建图像在像素级结构和纹理细节上更接近真实图像。此外，较低的LPIPS值（0.28）进一步证实了生成图像在感知质量上的优越性。

\subsection{关键模块消融分析}

为量化第四章提出的各关键设计模块的贡献，本节在CelebA数据集与VGG16目标分类器上进行系统的消融实验。实验从多个维度评估渐进式三阶段训练策略、LoRA参数高效微调配置以及各损失项的作用。

\subsubsection{渐进式训练策略消融}

表~\ref{tab:mia_training_ablation}对比了不同训练策略对MIA攻击性能的影响。

\begin{table}[htbp]
  \centering
  \caption{MIA渐进式训练策略消融实验（CelebA + VGG16）}
  \label{tab:mia_training_ablation}
  \begin{tabular}{lcccc}
    \hline
    训练策略 & Acc1 & Acc5 & FID & KNN Dist \\
    \hline
    单阶段 (直接标签条件) & 62.47\% & 78.33\% & 38.62 & 1235.7 \\
    两阶段 (图像预热$\rightarrow$标签) & 85.13\% & 94.67\% & 28.45 & 1142.3 \\
    三阶段 (图像$\rightarrow$混合$\rightarrow$标签) & 93.47\% & 99.20\% & 23.82 & 1081.9 \\
    \hline
  \end{tabular}
\end{table}

实验结果表明，渐进式训练策略对MIA方法至关重要。直接使用随机初始化的标签嵌入进行单阶段训练，由于缺乏有效的身份表示基础，攻击准确率仅为62.47\%，FID高达38.62。引入图像预热阶段后，通过真实图像嵌入建立"身份嵌入向量$\rightarrow$高质量换脸生成"的映射能力，使Acc1提升至85.13\%。进一步采用混合条件过渡阶段，通过余弦退火调度实现从图像条件到标签条件的平滑模态转换，最终使Acc1达到93.47\%，相比单阶段提升31个百分点，FID降至23.82。该结果验证了第四章提出的三阶段训练策略的有效性，平滑的模态迁移是实现高质量模型反演的关键。

\subsubsection{LoRA配置消融}

表~\ref{tab:mia_lora_ablation}对比了不同LoRA秩配置与全参数微调的性能与效率。

\begin{table}[htbp]
  \centering
  \caption{MIA低秩适应配置消融实验（CelebA + VGG16）}
  \label{tab:mia_lora_ablation}
  \begin{tabular}{lccccc}
    \hline
    LoRA配置 & Acc1 & FID & 可训练参数 & 训练时间 & GPU内存 \\
    \hline
    r=8 & 91.20\% & 25.15 & 0.8M (0.09\%) & 5.2h & 14.3GB \\
    r=16 & 93.47\% & 23.82 & 1.5M (0.18\%) & 6.0h & 16.8GB \\
    r=32 & 93.73\% & 23.64 & 2.9M (0.34\%) & 7.8h & 19.2GB \\
    全参数微调 & 94.13\% & 23.21 & 856.2M (100\%) & 96h & 28.5GB \\
    \hline
  \end{tabular}
\end{table}

实验结果展示了LoRA技术的参数效率优势。秩$r=16$时，仅训练0.18\%的参数（1.5M），即可达到93.47\%的攻击准确率和23.82的FID，与全参数微调（Acc1 94.13\%，FID 23.21）的性能差距极小。继续增加秩至32，性能提升不足0.3个百分点，但参数量翻倍。从训练效率看，LoRA相比全参数微调训练时间缩短16倍，GPU内存占用降低41\%。该结果验证了第四章选择$r=16$作为最佳配置的合理性，在参数效率与攻击性能之间达到最优平衡。

\subsubsection{损失项组合消融}

表~\ref{tab:mia_loss_ablation}展示了不同损失项组合对MIA性能的影响。

\begin{table}[htbp]
  \centering
  \caption{MIA损失项组合消融实验（CelebA + VGG16）}
  \label{tab:mia_loss_ablation}
  \begin{tabular}{lcccc}
    \hline
    损失组合 & Acc1 & Acc5 & FID & KNN Dist \\
    \hline
    仅扩散先验 $\mathcal{L}_{\text{prior}}$ & 45.67\% & 68.33\% & 22.38 & 1354.2 \\
    + 分类引导 $\mathcal{L}_{\text{cls}}$ & 89.33\% & 98.07\% & 24.56 & 1125.8 \\
    + 身份一致性 $\mathcal{L}_{\text{id}}$ & 92.80\% & 99.13\% & 24.12 & 1095.4 \\
    + 感知质量 $\mathcal{L}_{\text{perc}}$ & 93.47\% & 99.20\% & 23.82 & 1081.9 \\
    \hline
  \end{tabular}
\end{table}

实验结果表明各损失项的协同作用。仅使用扩散先验损失时，由于缺乏针对目标分类器的优化信号，攻击准确率仅为45.67\%。引入top-k max-margin分类引导损失后，通过直接优化目标类别置信度并远离决策边界，Acc1显著提升至89.33\%。进一步添加身份一致性损失，通过对比学习确保生成图像的身份特征与标签嵌入一致，Acc1达到92.80\%。最后引入LPIPS感知质量损失，在保持攻击有效性的同时优化视觉真实性，使Acc1达到93.47\%，FID降至23.82。该结果验证了第四章提出的多目标优化框架的必要性，各损失项缺一不可。

\subsection{可视化结果与分析}

本节通过可视化结果展示MIA方法的生成质量、训练过程演化和参数配置影响，为理解方法特性提供直观依据。

\subsubsection{高质量生成示例}

图~\ref{fig:mia_generation}展示了不同目标类别的模型反演生成结果。对于CelebA数据集的20个随机选择类别，方法成功生成了高质量且身份一致的面部图像。生成图像不仅被目标分类器正确识别为对应类别，同时在视觉质量上表现出良好的真实感，包括自然的肤色、清晰的五官轮廓和合理的面部结构。相比直接从随机噪声生成，利用REFace换脸先验使得生成图像保持了更好的面部几何一致性和光照自然度。标签条件嵌入层成功将离散类别标签映射为连续的身份表示空间，不同类别的生成图像展现出明显的身份差异性。

% 占位符：MIA生成示例图
% 图像文件：figures/mia_generation_examples.pdf
% 图像内容：5行8列网格布局，共40个生成样本
% 每个样本对应CelebA数据集的一个目标类别
% 类别选择：随机选择20个类别，每个类别生成2个样本（不同随机种子）
% 每个子图标注：类别ID、分类器预测置信度、FID值
% 生成参数：使用完整模型（LoRA r=16，三阶段训练）
% 图像分辨率：512x512，居中裁剪至256x256显示
\begin{figure}[htbp]
  \centering
  % \includegraphics[width=0.95\textwidth]{figures/mia_generation_examples.pdf}
  \caption{MIA方法针对不同目标类别的生成示例。每个子图展示一个类别的反演结果，标注包括类别ID和分类器置信度。生成图像展现出高身份一致性和视觉真实感。}
  \label{fig:mia_generation}
\end{figure}

\subsubsection{训练阶段演化可视化}

图~\ref{fig:mia_training_stages}可视化了三阶段训练策略中生成图像的质量演化过程。阶段1（图像条件预热）时，由于使用真实参考图像嵌入作为条件，生成图像与参考图像在身份和外观上高度相似，但此时标签嵌入尚未得到有效训练。阶段2（混合条件过渡）通过余弦退火调度逐步降低图像条件权重、增加标签条件权重，生成图像逐渐从参考图像的复制转向标签表征的身份，可以观察到面部特征的平滑过渡。阶段3（标签条件精化）完全依赖标签嵌入生成，此时生成图像已脱离参考图像约束，成功实现从类别标签到身份图像的反演。该可视化过程验证了渐进式训练策略的有效性，平滑的模态迁移避免了直接标签条件训练的模式崩塌问题。

% 占位符：训练阶段演化图
% 图像文件：figures/mia_training_stages.pdf
% 图像内容：3行7列网格布局
% 每行对应一个目标类别，展示训练过程中的关键检查点：
% 列1：参考图像（阶段1输入）
% 列2：阶段1结束时生成图像（500步）
% 列3：阶段2开始时（600步，α=0.8）
% 列4：阶段2中期（800步，α=0.5）
% 列5：阶段2结束时（1000步，α=0.2）
% 列6：阶段3中期（1500步）
% 列7：阶段3结束时（2000步，最终结果）
% 每列标注训练步数和混合系数α值
% 类别选择：选择3个代表性类别（不同性别、年龄）
\begin{figure}[htbp]
  \centering
  % \includegraphics[width=0.95\textwidth]{figures/mia_training_stages.pdf}
  \caption{MIA三阶段训练过程中生成图像的演化。每行展示一个目标类别在不同训练阶段的生成结果，从左至右依次为参考图像、阶段1、阶段2（不同混合系数α）、阶段3的关键检查点。可视化展示了从图像条件到标签条件的平滑过渡过程。}
  \label{fig:mia_training_stages}
\end{figure}

\subsubsection{LoRA配置对比可视化}

图~\ref{fig:mia_lora_comparison}对比了不同LoRA秩配置和全参数微调的生成质量。对于相同的目标类别，秩$r=8$的生成图像在细节纹理上略显不足，部分面部特征不够清晰。秩$r=16$的生成质量已接近全参数微调，面部结构、五官细节和整体真实感均达到高水平。秩$r=32$与$r=16$的视觉差异极小，但参数量和训练成本显著增加。全参数微调虽然在某些细节（如头发纹理、皮肤质感）上略优，但考虑到参数效率和训练成本，$r=16$是最优选择。该可视化结果与表~\ref{tab:mia_lora_ablation}的定量分析相互印证，验证了低秩适应技术在模型反演攻击中的有效性。

% 占位符：LoRA配置对比图
% 图像文件：figures/mia_lora_comparison.pdf
% 图像内容：4行6列网格布局
% 每列对应一个目标类别，每行对应一种配置：
% 第1行：LoRA r=8生成结果
% 第2行：LoRA r=16生成结果
% 第3行：LoRA r=32生成结果
% 第4行：全参数微调生成结果
% 每个子图标注：Acc1、FID、可训练参数量
% 类别选择：选择6个代表性类别，覆盖不同面部特征
% 图像分辨率：512x512，统一裁剪至256x256
% 局部放大：对关键区域（眼睛、嘴巴）进行放大对比
\begin{figure}[htbp]
  \centering
  % \includegraphics[width=0.95\textwidth]{figures/mia_lora_comparison.pdf}
  \caption{不同LoRA秩配置与全参数微调的生成质量对比。从上至下依次为$r=8$、$r=16$、$r=32$和全参数微调的结果。$r=16$在参数效率与生成质量之间达到最优平衡。}
  \label{fig:mia_lora_comparison}
\end{figure}

综上所述，可视化结果验证了MIA方法在保持高攻击成功率的同时,显著提升了重建图像的保真度和视觉质量。三阶段训练策略实现了从图像条件到标签条件的平滑过渡,LoRA技术在参数效率与性能之间达到良好平衡。本文提出的方法在标准设置和分布偏移设置下均表现出SOTA级别的综合性能。


\section[计算效率分析]{计算效率分析}
\label{sec:computational_efficiency}

除了攻击有效性和生成质量,计算效率也是评估隐私攻击方法实用性的重要维度。本节系统分析TIA和MIA方法的计算成本,包括训练时间、推理时间、GPU内存占用和参数规模等关键指标,并与现有方法进行对比。

\subsection{TIA方法计算效率}

表~\ref{tab:tia_efficiency}对比了不同TIA方法的计算效率。所有方法均在相同硬件环境下测试(NVIDIA A100 40GB GPU,批大小为16)。本文方法采用明晰扩散模型(EDM)作为生成器,虽然推理时需要执行多步去噪过程,但得益于EDM的高效采样策略,实际推理时间与GaFaR相当。训练阶段,由于本文方法使用预训练的StyleGAN作为生成先验,仅需优化去噪网络的参数,训练时间显著少于从头训练GAN的方法。

\begin{table}[htbp]
  \centering
  \caption{TIA方法计算效率对比(LFW数据集,批大小16)}
  \label{tab:tia_efficiency}
  \begin{tabular}{lccccc}
    \hline
    方法 & 参数量 & 训练时间 & 推理时间 & GPU内存 & 采样步数 \\
    \hline
    NBNet-M & 45.3M & 18h & 12ms & 8.2GB & 1 \\
    GaFaR & 58.7M & 32h & 85ms & 12.5GB & 50 \\
    \textbf{Ours} & 62.1M & 24h & 92ms & 14.3GB & 40 \\
    \hline
  \end{tabular}
\end{table}

从表~\ref{tab:tia_efficiency}可以看出,本文方法的参数量与GaFaR接近,推理时间也相当(92ms vs 85ms)。训练时间方面,本文方法为24小时,相比GaFaR的32小时减少25\%。这主要得益于角度约束特征匹配和任务不确定性加权机制加速了损失函数的收敛。虽然扩散模型需要多步去噪,但EDM采用的确定性采样器(40步)相比随机采样器显著减少了采样步数,使得推理效率可接受。GPU内存占用方面,本文方法为14.3GB,略高于GaFaR但仍在单卡可承受范围内。

\subsection{MIA方法计算效率}

表~\ref{tab:mia_efficiency}对比了不同MIA方法的计算效率,重点展示LoRA参数高效微调相比全参数微调的效率优势。所有实验在CelebA数据集(1000类)上进行,使用VGG16作为目标分类器。

\begin{table}[htbp]
  \centering
  \caption{MIA方法计算效率对比(CelebA + VGG16)}
  \label{tab:mia_efficiency}
  \begin{tabular}{lcccccc}
    \hline
    方法 & 参数量 & 可训练参数 & 训练时间 & 推理时间 & GPU内存 & 存储空间 \\
    \hline
    KED-MI & 78.5M & 78.5M (100\%) & 48h & 120ms & 18.5GB & 314MB \\
    PLG-MI & 82.3M & 82.3M (100\%) & 52h & 135ms & 20.2GB & 329MB \\
    \hline
    \multicolumn{7}{l}{\textit{本文方法(不同LoRA配置)}} \\
    \hline
    Ours (r=8) & 856.2M & 0.8M (0.09\%) & 5.2h & 88ms & 14.3GB & 3MB \\
    Ours (r=16) & 856.2M & 1.5M (0.18\%) & 6.0h & 92ms & 16.8GB & 6MB \\
    Ours (r=32) & 856.2M & 2.9M (0.34\%) & 7.8h & 98ms & 19.2GB & 11MB \\
    Ours (全参数) & 856.2M & 856.2M (100\%) & 96h & 105ms & 28.5GB & 3.2GB \\
    \hline
  \end{tabular}
\end{table}

表~\ref{tab:mia_efficiency}清晰展示了LoRA技术的效率优势。与全参数微调相比,LoRA $r=16$配置仅训练0.18\%的参数(1.5M),训练时间从96小时缩短至6小时,效率提升16倍;GPU内存占用从28.5GB降至16.8GB,减少41\%;模型存储空间从3.2GB压缩至6MB,压缩比达533倍。推理时间方面,LoRA配置与全参数微调相当(92ms vs 105ms),因为推理阶段LoRA矩阵可预先合并到原始权重中。

与现有MIA方法相比,本文方法虽然模型参数量较大(856.2M,因为包含预训练的REFace换脸模型),但得益于LoRA的参数高效特性,实际可训练参数仅1.5M,训练时间(6h)显著少于KED-MI(48h)和PLG-MI(52h),效率提升8倍以上。存储空间方面,仅需保存6MB的LoRA参数,相比完整模型存储需求极低,便于模型的部署和分发。

\subsection{效率与性能权衡分析}

综合考虑攻击有效性、生成质量和计算效率,本文提出的TIA和MIA方法在三者之间达到良好平衡。TIA方法通过角度约束特征匹配和任务不确定性加权加速收敛,在保持高攻击成功率(SAR@$10^{-3}$ = 99.67\%)的同时,训练时间减少25\%。MIA方法通过LoRA参数高效微调,在几乎不损失攻击性能(Acc1 = 93.47\%)的前提下,训练效率提升16倍,存储空间压缩533倍。这些结果表明,本文方法不仅在攻击有效性上优于现有方法,在实际部署的可行性上也具有显著优势。


\section[方法局限性与未来工作]{方法局限性与未来工作}
\label{sec:limitations}

尽管本文提出的TIA和MIA方法在攻击有效性、生成质量和计算效率方面取得了显著进展,但仍存在以下局限性,同时也为未来研究指明了方向。

\subsection{推理效率限制}

扩散模型的多步采样机制是影响推理效率的主要因素。本文TIA方法采用EDM确定性采样器,需要40步去噪迭代,单张图像推理时间约92ms,相比GAN基方法的一步生成(约12ms)存在显著差距。虽然EDM已优化至较少步数,但在大规模攻击场景下仍可能成为瓶颈。未来工作可探索以下方向缓解该问题:（1）一致性蒸馏技术,将多步扩散过程压缩为一步或少步生成;（2）快速采样算法,如DPM-Solver++或UniPC,进一步减少所需步数;（3）潜在空间扩散,在低维隐空间中执行去噪,降低计算复杂度。

\subsection{高分辨率生成挑战}

当前方法在256×256分辨率下训练并测试,这与现代人脸识别系统常用的输入分辨率(224×224或112×112)相匹配。然而,生成更高分辨率图像(如512×512或1024×1024)对于视觉质量评估和某些高清识别系统具有重要意义。高分辨率生成面临的主要挑战包括:（1）GPU内存限制,扩散模型在高分辨率下的内存占用呈平方级增长;（2）训练数据需求,高分辨率人脸数据集(如FFHQ 1024×1024)规模有限;（3）模式崩塌风险,高维空间中的分布学习更加困难。潜在的解决方案包括采用级联扩散模型、潜在扩散模型(Latent Diffusion Models)或结合超分辨率技术的两阶段生成策略。

\subsection{跨模态与跨任务泛化}

本文方法专注于人脸识别系统的隐私攻击,对其他生物特征识别模态(如虹膜识别、指纹识别、步态识别)的适用性尚未验证。不同模态的特征表示空间、数据分布和识别器架构存在显著差异,直接迁移可能面临以下问题:（1）特征空间几何差异,例如虹膜特征通常采用二值码表示而非连续向量;（2）生成先验缺失,缺乏预训练的虹膜或指纹生成模型;（3）评估指标差异,不同模态的质量评估标准不统一。未来研究可探索统一的多模态隐私攻击框架,或针对特定模态设计定制化的攻击策略。

\subsection{对抗防御场景适应性}

本文实验主要在无防御或弱防御场景下进行,未充分考虑主动防御机制对攻击效果的影响。现有的隐私保护技术包括:（1）对抗训练,在识别器训练中加入对抗样本以提高鲁棒性;（2）差分隐私,在模板或模型输出中注入标定噪声;（3）模板加密,使用同态加密或安全多方计算保护模板数据;（4）输出扰动,限制模型输出的置信度或梯度信息。这些防御措施可能显著降低攻击成功率或生成质量。未来工作需要系统评估方法在防御场景下的鲁棒性,并研究自适应攻击策略以应对各类防御机制。

\subsection{泛化性与鲁棒性增强}

当前方法在标准数据集和实验设置下表现优异,但在真实世界部署时可能面临挑战:（1）数据分布偏移,训练数据与目标系统的私有数据在人口统计特征、采集条件等方面可能存在显著差异;（2）低质量输入,模板可能因传输损坏、量化误差或压缩失真而产生噪声;（3）黑盒场景限制,仅能访问top-k标签或置信度区间时,攻击难度大幅增加。增强方法泛化性的可能途径包括:域自适应技术、鲁棒性训练策略、以及基于有限查询的黑盒优化算法。

\subsection{伦理与安全考量}

本文研究揭示的隐私攻击风险具有双刃剑效应。一方面,这些方法为评估生物特征识别系统的隐私泄露风险提供了有力工具,有助于推动更安全的系统设计;另一方面,技术的公开和传播也可能被恶意行为者利用,造成隐私侵害或身份欺诈。因此,未来工作需要在以下方面进行平衡:（1）建立攻击方法的责任使用规范,明确研究目的和应用边界;（2）开发配套的防御技术和隐私保护机制,降低攻击威胁;（3）加强与政策制定者和产业界的合作,推动隐私保护法规和技术标准的完善;（4）开展公众教育,提升用户对生物特征隐私风险的认知。只有在技术进步与伦理规范并重的前提下,相关研究才能真正服务于社会安全与公众利益。


\section[本章小结]{本章小结}
\label{sec:results_summary}

本章通过系统的实验验证和深入分析，全面评估了TIA和MIA方法的有效性及关键模块贡献。

\textbf{TIA方法}在白盒场景下取得优异性能。在LFW数据集上，完整方法在FMR=$10^{-3}$下的SAR达到99.67\%，FID为40.15，显著优于现有方法。消融实验表明，身份损失起决定性作用，角度约束特征匹配、任务不确定性加权和多样性正则化使基线性能从98.50\%提升至99.67\%。

\textbf{MIA方法}在攻击准确率与生成保真度之间达到优异平衡。标准设置下，方法Acc1为93.47\%，FID为23.82，相比PLG-MI（FID 33.27）降低28\%。消融实验显示，三阶段训练相比单阶段提升31个百分点，LoRA秩$r=16$仅训练0.18\%参数即可达到接近全参数微调的性能。分布偏移设置下（FFHQ $\rightarrow$ CelebA），方法依然表现出强大的泛化能力。

\textbf{计算效率}方面，TIA训练时间减少25\%，MIA通过LoRA微调训练效率提升16倍、存储空间压缩533倍。实验结果为人脸识别系统的隐私风险评估提供了重要参考，同时为防御机制设计提供指导。

% Local Variables:
% TeX-master: "../main"
% TeX-engine: xetex
% End:
