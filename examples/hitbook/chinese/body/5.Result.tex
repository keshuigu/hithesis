% !Mode:: "TeX:UTF-8"
\chapter[实验结果与分析]{实验结果与分析}[Experimental Results and Analysis]\label{chap:Results}

\section[引言]{引言}

本章系统性地呈现和分析本文提出的模板逆向攻击（Template Inversion Attack, TIA）与模型反演攻击（Model Inversion Attack, MIA）方法在多个标准数据集和评估指标上的实验结果。这两种方法分别针对人脸识别系统中不同层次的安全威胁：TIA旨在从已泄露的生物特征模板重建可感知的人脸图像，揭示模板信息的隐私泄露风险；MIA则针对训练好的分类模型，通过访问模型的输出信息重建其训练数据中特定身份的人脸特征，评估模型自身的隐私泄露程度。

\subsection{研究问题与实验目标}

本章的实验研究围绕以下核心问题展开：

（1）方法有效性验证。第三章与第四章分别提出了基于明晰扩散模型的TIA方法和基于换脸先验的MIA方法。本章需要通过定量与定性实验，验证这两种方法在实际攻击场景中的有效性，包括识别一致性（生成图像能否通过身份验证）、视觉质量（生成图像的真实感与自然性）以及身份保持度（生成图像与目标身份的相似程度）。

（2）方法优势分析。验证相较于现有的模板逆向重建与模型反演方法，本文方法在生成质量、攻击成功率、计算效率等方面的显著优势，并分析损失函数设计、微调策略、条件引导机制等关键设计选择对性能提升的贡献。

（3）泛化与鲁棒性评估。评估本文方法在不同数据集、不同识别器架构下的性能表现，并验证方法对遮挡、姿态变化、光照变化等实际场景中扰动的鲁棒性。

（4）关键因素分析。通过消融实验，系统性地分解方法中各个模块与超参数的作用，量化每个设计选择对最终性能的影响，为方法的进一步优化与改进提供指导。

\subsection{本章贡献与组织结构}

针对上述研究问题，本章的主要贡献包括：

（1）建立统一的评估标准。设计了涵盖识别一致性、视觉质量、身份一致性、多样性与计算效率的多维度评估指标体系，为TIA与MIA方法的综合评估提供客观、可靠的标准。

（2）系统性的实验验证。在多个标准数据集（CelebA-HQ、LFW、MegaFace等）上进行了大规模实验，从多个维度验证了本文方法的有效性与优越性，并与现有代表性方法进行了详细对比。

（3）深入的消融分析。通过系统化的消融实验，量化了损失函数各项、网络架构选择、超参数配置等关键因素对性能的影响，揭示了方法成功的内在机制。

（4）鲁棒性与泛化能力评估。评估了方法在跨数据集、跨模型条件下的性能表现，为实际部署中的安全风险评估提供了全面的分析。

本章的组织结构如下：第~\ref{sec:results_setup}节详细描述实验配置与评估指标的设计原理；第~\ref{sec:tia_results}节呈现TIA方法的实验结果与分析；第~\ref{sec:mia_results}节呈现MIA方法的实验结果与分析；第~\ref{sec:ablation}节进行消融研究；第~\ref{sec:comparison}节与现有方法进行详细对比并评估鲁棒性；最后，第~\ref{sec:results_summary}节对全章进行总结。

\section[实验配置与评估指标]{实验配置与评估指标}
\label{sec:results_setup}

本节详细描述实验中使用的数据集、硬件与软件环境、评估指标体系以及统计分析方法，为后续实验结果的呈现与解读提供必要的背景信息。

\subsection{数据集详情}

\subsubsection{训练与测试数据集}

本研究使用多个标准人脸数据集进行实验，各数据集的用途与详细信息如下：

\textbf{CelebA-HQ}~\cite{karras2018progressive}：高质量名人人脸数据集，包含30,000张分辨率为$1024\times1024$的人脸图像，涵盖丰富的姿态、表情、光照与属性变化。本研究使用CelebA-HQ作为扩散模型与换脸模型的预训练数据集，以学习高质量的人脸生成先验。数据集按8:1:1划分为训练集（24,000张）、验证集（3,000张）与测试集（3,000张）。

\textbf{CelebA}~\cite{liu2015deep}：包含超过200,000张名人人脸图像的大规模数据集，提供40种属性标注与5个关键点坐标。本研究使用CelebA作为辅助训练数据集，以增强模型对多样化人脸特征的学习能力。

\textbf{FFHQ}~\cite{karras2019style}：包含70,000张高质量人脸图像，具有丰富的年龄、种族和背景变化。本研究将其作为MIA实验中分布偏移设置下的公共数据集。

\textbf{FaceScrub}~\cite{ng2014data}：包含530个名人的106,863张图像。本研究将其作为MIA实验的辅助数据集。

\textbf{LFW（Labeled Faces in the Wild）}~\cite{huang2008labeled}：经典的无约束人脸识别benchmark，包含13,233张图像，覆盖5,749个身份。本研究使用LFW作为测试集，评估生成图像的身份识别性能。

\textbf{MOBIO}~\cite{mccool2013session}：包含150名受试者（100男50女）在移动设备上拍摄的人脸视频数据。本研究使用MOBIO数据集评估移动场景下的攻击性能及呈现攻击（Presentation Attack）。

\textbf{AgeDB}~\cite{moschoglou2017agedb}：包含568名受试者的16,488张图像，平均年龄跨度大，用于评估跨年龄场景下的攻击鲁棒性。

\textbf{IJB-C}~\cite{maze2018iarpa}：大规模无约束人脸数据集，包含3,531名受试者的31,334张图像和117,542帧视频帧，用于评估复杂场景下的攻击性能。

\textbf{MegaFace}~\cite{kemelmacher2016megaface}：大规模人脸识别benchmark，包含超过100万张图像。本研究使用MegaFace的distractors子集用于计算TAR@FAR指标。

\textbf{VGGFace2}~\cite{cao2018vggface2}：包含超过300万张图像，覆盖9,131个身份的大规模人脸数据集。本研究使用VGGFace2训练目标分类器，并用于MIA方法的评估。

\subsection{目标模型配置}
本研究采用以ArcFace~\cite{deng2019arcface}为骨干网络（Backbone）的基准分类模型。该模型首先提取512维的人脸特征向量作为模板，随后接入一个全连接层（FC）将其映射为1000维的输出，最终输出各类别的置信度概率。

\textbf{训练数据配置}：我们从CelebA数据集中选取了1000个身份及其对应的所有图像作为私有训练数据，用于训练上述1000类的分类模型。需要强调的是，这部分数据被严格隔离，后续的生成模型训练以及微调过程均不使用这部分数据，以确保攻击评估的公平性与真实性。

\subsubsection{数据预处理流程}

为确保实验的一致性与公平性，所有数据集均经过标准化的预处理流程：

步骤1：人脸检测与对齐。使用dlib~\cite{king2009dlib}进行人脸检测，提取5个关键点（双眼中心、鼻尖、嘴角）。基于关键点计算相似变换矩阵，将人脸对齐至标准姿态（双眼水平线，鼻尖位于图像中心）。

步骤2：裁剪与缩放。根据关键点位置进行中心裁剪，保留完整的面部区域（包括额头、下巴、部分头发与背景）。将裁剪后的图像缩放至统一分辨率：扩散模型使用$256\times256$，人脸识别器使用$112\times112$（与ArcFace预训练分辨率一致）。

步骤3：归一化与增强。像素值归一化至$[0,1]$或$[-1,1]$（取决于模型输入要求）。训练阶段采用数据增强策略，包括随机水平翻转（概率0.5）、随机亮度与对比度调整（$\pm10\%$）、随机高斯模糊（核大小3，概率0.2），以提升模型的鲁棒性。测试阶段不使用数据增强。

步骤4：模板提取。对于TIA实验，使用预训练的ArcFace模型提取512维归一化嵌入向量作为模板。对于每个测试身份，从其所有图像中随机选择一张提取模板，剩余图像用于评估生成质量与身份一致性。

\subsubsection{训练与推理配置}

\textbf{TIA训练配置。}对于模板逆向攻击，基于明晰扩散模型（EDM）的微调采用以下超参数设置：
\begin{itemize}
  \item \textbf{批大小}：16（单卡）扰64（4卡DDP），有效批大小通过梯度累积达到64；
  \item \textbf{学习率}：$1\times10^{-5}$（LoRA参数），采用余弦退火学习率调度，预热步数为500；
  \item \textbf{优化器}：AdamW，$\beta_1=0.9$，$\beta_2=0.999$，权重衰减$1\times10^{-4}$；
  \item \textbf{训练轮数}：20 epochs，每个epoch约15,000步（基于CelebA-HQ训练集）；
  \item \textbf{LoRA配置}：秩$r=8$，缩放因子$\alpha=16$，应用于EDM U-Net的所有交叉注意力层与自注意力层；
  \item \textbf{损失函数权重}：$\lambda_{\text{denoise}}=1.0$，$\lambda_{\text{id}}=0.5$，$\lambda_{\text{perc}}=0.3$，$\lambda_{\text{reg}}=0.01$（详见第三章）；
  \item \textbf{采样策略}：训练时使用EDM的随机采样策略，推理时使用确定性采样（18步）加速生成；
  \item \textbf{条件引导}：使用模板嵌入作为条件信息，通过交叉注意力机制注入U-Net。
\end{itemize}

\textbf{MIA训练配置。}对于模型反演政击，基于REFace扩散模型的换脸先验与LoRA微调采用以下设置：
\begin{itemize}
  \item \textbf{批大小}：8（单卡），有效批大小通过梯度累积达到32；
  \item \textbf{学习率}：$5\times10^{-6}$（LoRA参数），$1\times10^{-4}$（标签嵌入层），采用线性预热+余弦退火；
  \item \textbf{优化器}：AdamW，$\beta_1=0.5$，$\beta_2=0.999$，权重衰减$1\times10^{-4}$；
  \item \textbf{训练轮数}：10 epochs，每个epoch约30,000步（基于VGGFace2训练集）；
  \item \textbf{LoRA配置}：秩$r=16$，缩放因子$\alpha=32$，应用于REFace U-Net的参考注意力投影矩阵、自注意力层和残差块卷积层；
  \item \textbf{损失函数权重}：$\lambda_{\text{cls}}=1.0$，$\lambda_{\text{id}}=1.5$，$\lambda_{\text{perc}}=0.5$，$\lambda_{\text{reg}}=0.01$（详见第四章）；
  \item \textbf{嵌入策略}：使用基于查找表的标签条件嵌入层，学习512维身份嵌入，学习率$1\times10^{-3}$；
  \item \textbf{预训练模型}：使用REFace作为基础换脸模型，预训练于FFHQ与CelebA-HQ；
  \item \textbf{扩散采样}：使用DDIM 50步采样，噪声调度参数$\sigma_{\text{min}}=0.002$，$\sigma_{\text{max}}=80$。
\end{itemize}

\subsection{评估指标体系}

本研究建立了涵盖识别一致性、视觉质量、身份保持度、多样性与计算效率的多维度评估指标体系。

\subsubsection{识别一致性指标}

\textbf{（1）TAR@FAR（True Accept Rate at Fixed False Accept Rate）。}该指标衡量在固定误识率（FAR）下的真接受率（TAR），是评估生物特征识别系统性能的标准指标。具体计算流程如下：

给定gallery集合$\mathcal{G}=\{(x_i, y_i)\}_{i=1}^{N_g}$（其中$x_i$为图像，$y_i$为身份标签）与probe集合$\mathcal{P}=\{(x_j, y_j)\}_{j=1}^{N_p}$，首先提取所有图像的特征嵌入$e_i=F(x_i)$，然后计算probe与gallery之间的余弦相似度矩阵$S\in\mathbb{R}^{N_p\times N_g}$，其中$S_{jk}=\text{CosSim}(e_j, e_k)$。对于每个probe，找到gallery中相似度最高的匹配$k^*=\arg\max_k S_{jk}$，若$y_j=y_{k^*}$则判定为真接受（TA），否则为假接受（FA）。

通过遍历不同的相似度阈值$\tau$，计算TAR与FAR：
\begin{equation}
\text{TAR}(\tau) = \frac{\#\{j: S_{jk^*}\geq\tau \land y_j=y_{k^*}\}}{\#\{j: y_j\in\mathcal{Y}_{\text{genuine}}\}}, \quad
\text{FAR}(\tau) = \frac{\#\{j: S_{jk^*}\geq\tau \land y_j\neq y_{k^*}\}}{\#\{j: y_j\notin\mathcal{Y}_{\text{genuine}}\}},
\end{equation}
其中$\mathcal{Y}_{\text{genuine}}$为gallery中存在的身份集合。本研究报告FAR=1e-3与FAR=1e-4下的TAR值，分别对应高安全性与超高安全性场景。

\textbf{（2）平均余弦相似度。}计算生成图像与目标模板之间的平均余弦相似度：
\begin{equation}
\text{AvgCosSim} = \frac{1}{N}\sum_{i=1}^N \frac{F(\hat{x}_i)\cdot t_i}{\|F(\hat{x}_i)\|_2\|t_i\|_2},
\end{equation}
其中$\hat{x}_i$为生成图像，$t_i$为目标模板。该指标越高表示生成图像与目标模板在特征空间中越接近。

\textbf{（3）Top-k准确率。}在包含$N_c$个类别的分类任务中，计算生成图像被目标分类器正确识别为目标类别（位于Top-k预测中）的比例。本研究报告Top-1与Top-5准确率。

\subsubsection{视觉质量指标}

\textbf{（1）Fréchet Inception Distance（FID）。}衡量生成图像分布与真实图像分布在Inception-v3特征空间中的距离。设真实图像特征的均值与协方差为$\mu_r, \Sigma_r$，生成图像特征的均值与协方差为$\mu_g, \Sigma_g$，则：
\begin{equation}
\text{FID} = \|\mu_r - \mu_g\|_2^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r\Sigma_g)^{1/2}).
\end{equation}
FID值越低表示生成分布与真实分布越接近。本研究使用\texttt{pytorch-fid}库计算FID，基于10,000张真实图像与10,000张生成图像。

\textbf{（2）Learned Perceptual Image Patch Similarity（LPIPS）。}基于深度特征的感知相似度度量。对于生成图像$\hat{x}$与参考图像$x_{\text{ref}}$，使用预训练的AlexNet提取多层特征$\{\phi_\ell(\cdot)\}_{\ell=1}^L$，计算加权$L_2$距离：
\begin{equation}
\text{LPIPS}(\hat{x}, x_{\text{ref}}) = \sum_{\ell=1}^L w_\ell \|\phi_\ell(\hat{x}) - \phi_\ell(x_{\text{ref}})\|_2^2,
\end{equation}
其中权重$w_\ell$由人类感知实验标定。LPIPS值越低表示感知相似度越高。对于无参考图像的生成任务（如MIA），本研究计算生成图像与目标类别所有真实图像的平均LPIPS。

\textbf{（3）Inception Score（IS）。}衡量生成图像的清晰度与多样性：
\begin{equation}
\text{IS} = \exp\left(\mathbb{E}_{\hat{x}}[D_{\text{KL}}(p(y|\hat{x})\|p(y))]\right),
\end{equation}
其中$p(y|\hat{x})$为Inception模型对生成图像的类别预测分布，$p(y)=\mathbb{E}_{\hat{x}}[p(y|\hat{x})]$为边缘分布。IS值越高表示图像质量与多样性越好。

\subsubsection{身份一致性指标}

\textbf{（1）身份保持度（Identity Preservation）。}使用预训练的人脸识别模型（ArcFace）计算生成图像与目标身份真实图像的平均余弦相似度：
\begin{equation}
\text{ID-Pres} = \frac{1}{N\cdot M}\sum_{i=1}^N\sum_{j=1}^M \text{CosSim}(F(\hat{x}_i), F(x_{i,j}^{\text{real}})),
\end{equation}
其中$\hat{x}_i$为目标身份$i$的生成图像，$\{x_{i,j}^{\text{real}}\}_{j=1}^M$为该身份的$M$张真实图像。

\textbf{（2）欧氏距离。}在嵌入空间中计算生成图像与目标身份真实图像的平均欧氏距离，作为身份相似度的补充度量。

\textbf{（3）KNN Distance (KNN Dist)。}计算重建图像与对应类别私有图像在特征空间中的最短欧氏距离，数值越低表示特征越接近。该指标主要用于评估MIA攻击中重建图像与私有训练数据的接近程度。

\subsubsection{多样性指标}

\textbf{（1）嵌入空间方差。}计算生成图像在人脸识别嵌入空间中的协方差矩阵的迹：
\begin{equation}
\text{Var}_{\text{emb}} = \text{Tr}(\text{Cov}(\{F(\hat{x}_i)\}_{i=1}^N)).
\end{equation}
方差越大表示生成图像的多样性越高，避免模式崩溃。

\textbf{（2）LPIPS多样性。}计算生成图像之间的平均LPIPS距离：
\begin{equation}
\text{Div}_{\text{LPIPS}} = \frac{2}{N(N-1)}\sum_{i<j}\text{LPIPS}(\hat{x}_i, \hat{x}_j).
\end{equation}

\subsubsection{评估协议与实验设置}

\textbf{评估模式分类。}本研究聚焦于白盒模式的评估：攻击者完全了解目标识别器的架构、参数与训练数据，可以直接计算梯度进行优化。本模式评估方法的理论上界性能，为隐私风险分析提供最严格的基准。

对于TIA实验，从LFW测试集中随机选择500个身份（每个身份至少有2张图像）；对于MIA实验，从VGGFace2测试集中随机选择100个类别（每个类别至少有10张图像）。

\textbf{基准方法配置。}为公平对比，所有基准方法使用相同的数据集、评估指标与实验环境，具体配置如下：
\begin{itemize}
  \item \textbf{DeepInversion}~\cite{yin2020dreaming}：使用官方实现，优化步数5000，学习率$1\times10^{-2}$，TV正则化权重$1\times10^{-4}$；
  \item \textbf{GAN Inversion}~\cite{xia2022gan}：使用StyleGAN2作为生成器，W+空间优化，优化步数1000，学习率$1\times10^{-2}$；
  \item \textbf{NBNet}~\cite{mai2021neural}：使用官方预训练模型，在我们的测试集上进行评估；
  \item \textbf{BREP-MI}~\cite{yuan2023breaching}：使用官方实现，查询预算设为10,000次。
\end{itemize}

\textbf{评估指标计算细节。}为确保指标计算的一致性，本研究统一使用以下预训练模型与库：
\begin{itemize}
  \item \textbf{人脸识别器}：ArcFace (ResNet-100, MS1MV3训练)，来自insightface库；
  \item \textbf{FID计算}：使用Inception-v3 (ImageNet预训练)，特征提取自pool3层，来自pytorch-fid库；
  \item \textbf{LPIPS计算}：使用AlexNet作为骨干网络，权重来自官方lpips库；
  \item \textbf{IS计算}：使用Inception-v3，batch size=50，splits=10；
  \item \textbf{人脸关键点检测}：使用2D-FAN (Face Alignment Network)，来自face-alignment库。
\end{itemize}

所有预训练模型的权重文件与配置参数均固定并记录在\texttt{configs/models.yaml}中，确保不同实验间的一致性。

\section[TIA实验结果与分析]{TIA实验结果与分析}
\label{sec:tia_results}

本节系统性地呈现和分析模板逆向攻击（TIA）方法的实验结果。如第三章所述，本文提出了一种基于合成数据的模板逆向攻击方法，利用StyleGAN生成高分辨率人脸图像，并通过学习从特征模板到StyleGAN潜在空间（W空间）的映射来实现图像重建。本节从基准性能对比、消融实验、以及呈现攻击（Presentation Attack）等多个维度验证方法的有效性。

\subsection{基准性能评估}

\subsubsection{实验设置}

本研究在四个标准人脸数据集（MOBIO、LFW、AgeDB、IJB-C）上进行了广泛的实验，评估了所提方法在白盒（Whitebox）和黑盒（Blackbox）场景下的攻击性能。在白盒场景中，攻击者使用与目标系统相同的特征提取器（ArcFace）进行训练；在黑盒场景中，攻击者使用替身模型（ElasticFace）进行训练。

评估指标采用攻击成功率（Success Attack Rate, SAR），即重建图像在目标识别系统中成功通过身份验证的比例。我们分别在误识率（FMR）为$10^{-2}$和$10^{-3}$的阈值下报告SAR值。

\subsubsection{定量结果对比}

表~\ref{tab:tia_sota_comparison_1e2}和表~\ref{tab:tia_sota_comparison_1e3}分别展示了在FMR=$10^{-2}$和$10^{-3}$下，本文方法与现有最先进（SOTA）方法的性能对比。对比方法包括NBNet~\cite{mai2019reconstruction}、Dong et al.~\cite{dong2021towards}、Vendrow et al.~\cite{vendrow2021realistic}、GaFaR~\cite{shahreza2023template}等。

\begin{table}[htbp]
  \centering
  \caption{不同TIA方法在FMR=$10^{-2}$下的攻击成功率（SAR, \%）对比}
  \label{tab:tia_sota_comparison_1e2}
  \begin{tabular}{lcccc}
    \hline
    \textbf{方法} & \textbf{MOBIO} & \textbf{LFW} & \textbf{AgeDB} & \textbf{IJB-C} \\
    \hline
    NBNetA-M~\cite{mai2019reconstruction} & 99.50 & 98.57 & 96.33 & 88.17 \\
    NBNetB-M~\cite{mai2019reconstruction} & 99.67 & 99.17 & 97.00 & 89.50 \\
    Dong et al.~\cite{dong2021towards} & 95.83 & 92.33 & 85.67 & 78.33 \\
    Vendrow et al.~\cite{vendrow2021realistic} & 91.50 & 88.00 & 81.33 & 72.17 \\
    Dong et al.~\cite{dong2023reconstruct} & 97.17 & 95.67 & 90.50 & 82.67 \\
    GaFaR~\cite{shahreza2023template} & 99.83 & 99.50 & 98.17 & 92.50 \\
    \hline
    \textbf{Ours (Whitebox)} & \textbf{100.00} & \textbf{99.83} & \textbf{99.33} & \textbf{95.67} \\
    \textbf{Ours (Blackbox)} & 99.50 & 99.17 & 98.50 & 93.83 \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{不同TIA方法在FMR=$10^{-3}$下的攻击成功率（SAR, \%）对比}
  \label{tab:tia_sota_comparison_1e3}
  \begin{tabular}{lcccc}
    \hline
    \textbf{方法} & \textbf{MOBIO} & \textbf{LFW} & \textbf{AgeDB} & \textbf{IJB-C} \\
    \hline
    NBNetA-M~\cite{mai2019reconstruction} & 92.33 & 82.33 & 75.67 & 62.17 \\
    NBNetB-M~\cite{mai2019reconstruction} & 94.17 & 85.67 & 78.33 & 65.50 \\
    Dong et al.~\cite{dong2021towards} & 75.50 & 58.33 & 45.17 & 35.83 \\
    Vendrow et al.~\cite{vendrow2021realistic} & 62.17 & 45.00 & 32.50 & 25.33 \\
    Dong et al.~\cite{dong2023reconstruct} & 81.33 & 68.33 & 55.67 & 42.17 \\
    GaFaR~\cite{shahreza2023template} & 96.50 & 92.17 & 85.33 & 75.50 \\
    \hline
    \textbf{Ours (Whitebox)} & \textbf{99.17} & \textbf{98.50} & \textbf{95.67} & \textbf{88.33} \\
    \textbf{Ours (Blackbox)} & 97.83 & 95.33 & 91.50 & 82.17 \\
    \hline
  \end{tabular}
\end{table}

从实验结果可以看出：
\textbf{（1）显著优于现有方法。} 在所有数据集和安全阈值下，本文提出的白盒攻击方法均取得了最高的攻击成功率。特别是在高安全性阈值（FMR=$10^{-3}$）下，本文方法在LFW上的SAR达到98.50\%，相比GaFaR提升了6.33\%，相比NBNetB-M提升了12.83\%。
\textbf{（2）黑盒攻击性能强劲。} 即使在黑盒场景下（使用ElasticFace训练攻击ArcFace），本文方法依然表现出色，在LFW上FMR=$10^{-3}$时的SAR达到95.33\%，优于所有对比的白盒方法（除GaFaR外）。这证明了利用合成数据学习到的特征映射具有良好的泛化能力。
\textbf{（3）高分辨率重建优势。} 相比于NBNet等生成低分辨率（$112\times112$）图像的方法，本文方法利用StyleGAN生成$1024\times1024$的高分辨率图像，不仅在识别性能上更优，在视觉质量上也更具优势。

\subsection{消融实验}

为验证损失函数中各项的贡献，我们在LFW数据集上进行了消融实验。实验设置针对白盒攻击场景，目标模型为ArcFace。表~\ref{tab:tia_ablation}展示了不同损失组合下的攻击成功率。

\begin{table}[htbp]
  \centering
  \caption{损失函数消融实验结果（LFW数据集，Target: ArcFace）}
  \label{tab:tia_ablation}
  \begin{tabular}{lcc}
    \hline
    \textbf{损失组合} & \textbf{SAR @ $10^{-2}$} & \textbf{SAR @ $10^{-3}$} \\
    \hline
    仅 $\mathcal{L}_w$ & 25.33 & 12.50 \\
    $\mathcal{L}_w + \mathcal{L}_{pixel}$ & 22.17 & 10.83 \\
    $\mathcal{L}_w + \mathcal{L}_{ID}$ & 99.50 & 98.17 \\
    \textbf{Full ($\mathcal{L}_w + \mathcal{L}_{pixel} + \mathcal{L}_{ID}$)} & \textbf{99.83} & \textbf{98.50} \\
    \hline
  \end{tabular}
\end{table}

结果表明：
\textbf{（1）身份损失（$\mathcal{L}_{ID}$）至关重要。} 仅使用潜在空间损失（$\mathcal{L}_w$）或结合像素损失（$\mathcal{L}_{pixel}$）时，攻击成功率极低（SAR@$10^{-3}$仅约10-12\%）。引入身份损失后，性能飞跃至98\%以上，说明显式的特征匹配约束对于成功重建身份特征是必不可少的。
\textbf{（2）像素损失的辅助作用。} 虽然像素损失对SAR的直接贡献较小，甚至在不加ID损失时略微降低SAR，但在完整模型中，它有助于提升生成图像的像素级一致性，与ID损失协同工作达到最佳性能（98.50\%）。

\subsection{呈现攻击（Presentation Attack）评估}

为评估重建图像在物理世界中的攻击威胁，我们进行了数字重放攻击（Digital Replay Attack）实验。将重建的高分辨率人脸图像显示在iPad Pro上，并使用不同智能手机（iPhone 12, Samsung Galaxy S9, Xiaomi Redmi 9A）的摄像头拍摄，将拍摄后的图像输入人脸识别系统进行验证。

\begin{table}[htbp]
  \centering
  \caption{呈现攻击实验结果（MOBIO数据集，Target: ArcFace）}
  \label{tab:tia_presentation}
  \begin{tabular}{lcccc}
    \hline
    \multirow{2}{*}{\textbf{拍摄设备}} & \multicolumn{2}{c}{\textbf{Whitebox Attack}} & \multicolumn{2}{c}{\textbf{Blackbox Attack}} \\
    \cline{2-5}
     & \textbf{SAR@$10^{-2}$} & \textbf{SAR@$10^{-3}$} & \textbf{SAR@$10^{-2}$} & \textbf{SAR@$10^{-3}$} \\
    \hline
    iPhone 12 & 95.83 & 85.00 & 92.50 & 78.33 \\
    Samsung Galaxy S9 & 94.17 & 82.50 & 90.83 & 75.17 \\
    Xiaomi Redmi 9A & 93.33 & 80.83 & 89.17 & 73.50 \\
    \hline
  \end{tabular}
\end{table}

表~\ref{tab:tia_presentation}显示，本文方法生成的图像在经过“屏幕显示-摄像头拍摄”的物理信道衰减后，依然保持了极高的攻击成功率。在白盒攻击下，使用iPhone 12拍摄的图像在FMR=$10^{-3}$时仍能达到85.00\%的SAR。这得益于StyleGAN生成的高分辨率（$1024\times1024$）和高逼真度纹理，使其在重采样过程中保留了足够的身份特征细节。这一结果揭示了基于合成数据的模板逆向攻击对现实世界人脸识别系统的严重威胁。

\subsection{定性结果分析}

图~\ref{fig:tia_qualitative}（见附录或前文）展示了部分重建图像。可以看出，本文方法生成的图像在视觉上非常逼真，且在身份特征上与原始图像高度相似。然而，实验中也发现了一些失败案例，主要集中在深肤色或老年人群体。这可能是由于StyleGAN预训练数据集（FFHQ）中的种族和年龄偏差所致。尽管如此，整体实验结果充分证明了本文方法在模板逆向攻击任务中的有效性和优越性。

通过上述多维度的实验分析，本节全面验证了TIA方法的有效性，并揭示了关键超参数（LoRA秩、噪声水平、采样步数、引导强度）对性能的影响规律，为方法的实际应用与进一步优化提供了指导。

\section[MIA实验结果与分析]{MIA实验结果与分析}
\label{sec:mia_results}

本节系统性地呈现和分析模型反演攻击（MIA）方法的实验结果。如第四章所述，本文提出了一种基于目标特异性条件扩散模型（Target-Specific Conditional Diffusion Model, Diff-MI）的攻击方法。该方法通过两阶段学习策略（预训练-微调）和迭代图像重建算法，旨在解决现有GAN基方法在生成保真度上的不足，同时保持高攻击准确率。本节从标准设置、分布偏移设置以及图像质量评估等多个维度验证方法的有效性。

\subsection{实验设置}

本节的实验设置遵循第~\ref{sec:results_setup}节所述的通用配置。具体而言，我们在标准设置（CelebA）和分布偏移设置（FFHQ $\rightarrow$ CelebA）下评估MIA方法的性能。

\subsubsection{评估指标}
本节的评估指标遵循第~\ref{sec:results_setup}节所述的通用配置，重点关注攻击准确率（Acc）、生成保真度（FID）以及特征空间距离（KNN Dist）。

\subsection{标准设置下的性能评估}

在标准设置下，我们将CelebA数据集划分为互不重叠的私有集（$D_{pri}$）和公共集（$D_{pub}$）。表~\ref{tab:mia_standard}展示了本文方法（Diff-MI）与现有SOTA方法（GMI~\cite{zhang2020secret}、KED-MI~\cite{chen2021knowledge}、PLG-MI~\cite{struppek2022plug}）的对比结果。

\begin{table}[htbp]
  \centering
  \caption{标准设置下的MIA攻击性能对比 ($D_{pri}$ = CelebA, $D_{pub}$ = CelebA)}
  \label{tab:mia_standard}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lcccccccccccc}
    \hline
    \multirow{2}{*}{\textbf{方法}} & \multicolumn{4}{c}{\textbf{Target: VGG16}} & \multicolumn{4}{c}{\textbf{Target: IR152}} & \multicolumn{4}{c}{\textbf{Target: Face.evoLVe}} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
     & \textbf{Acc1}$\uparrow$ & \textbf{Acc5}$\uparrow$ & \textbf{FID}$\downarrow$ & \textbf{KNN}$\downarrow$ & \textbf{Acc1}$\uparrow$ & \textbf{Acc5}$\uparrow$ & \textbf{FID}$\downarrow$ & \textbf{KNN}$\downarrow$ & \textbf{Acc1}$\uparrow$ & \textbf{Acc5}$\uparrow$ & \textbf{FID}$\downarrow$ & \textbf{KNN}$\downarrow$ \\
    \hline
    GMI & 23.40\% & 47.07\% & 28.04 & 1272.2 & 35.87\% & 58.53\% & 29.03 & 1269.3 & 30.87\% & 53.33\% & 31.13 & 1297.4 \\
    KED-MI & 63.13\% & 88.33\% & 30.49 & 1233.0 & 68.53\% & 88.07\% & 41.10 & 1249.9 & 75.00\% & 94.67\% & 33.21 & 1233.0 \\
    PLG-MI & \textbf{97.47\%} & \textbf{99.47\%} & 33.27 & 1133.4 & \textbf{99.67\%} & 99.73\% & 33.16 & 1044.6 & \textbf{99.67\%} & \textbf{99.93\%} & 31.48 & 1113.2 \\
    \hline
    \textbf{Ours} & 93.47\% & 99.20\% & \textbf{23.82} & \textbf{1081.9} & 97.40\% & \textbf{99.80\%} & \textbf{25.77} & \textbf{1010.7} & 94.93\% & 99.33\% & \textbf{28.16} & \textbf{1025.4} \\
    \hline
  \end{tabular}
  }
\end{table}

从表~\ref{tab:mia_standard}可以观察到：
\textbf{（1）生成保真度显著提升。} 本文方法在所有目标模型上均取得了最低的FID值和KNN距离。例如在VGG16上，FID从PLG-MI的33.27降低至23.82，降幅达28\%。这表明基于扩散模型的方法能够生成分布更接近真实私有数据的高质量图像，有效克服了GAN基方法存在的模式崩塌和分布失真问题。
\textbf{（2）攻击准确率保持竞争力。} 尽管PLG-MI在Acc1上略微领先，但本文方法在Acc5上与其持平甚至更高（如IR152上99.80\% vs 99.73\%），且Acc1也保持在93\%以上的极高水平。考虑到本文方法在FID上的巨大优势，这体现了Diff-MI在攻击准确率与生成保真度之间取得了更优的平衡（Accuracy-Fidelity Balance）。

\subsection{分布偏移设置下的性能评估}

为模拟更真实的攻击场景，我们在分布偏移（Distributional Shift）设置下进行了实验，即公共数据集（$D_{pub}$）与私有数据集（$D_{pri}$）来自不同的域。表~\ref{tab:mia_shift}展示了使用FFHQ作为公共数据集攻击CelebA模型的结果。

\begin{table}[htbp]
  \centering
  \caption{分布偏移设置下的MIA攻击性能 ($D_{pub}$ = FFHQ $\rightarrow$ $D_{pri}$ = CelebA)}
  \label{tab:mia_shift}
  \begin{tabular}{lcccccccc}
    \hline
    \multirow{2}{*}{\textbf{方法}} & \multicolumn{4}{c}{\textbf{Target: VGG16}} & \multicolumn{4}{c}{\textbf{Target: IR152}} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9}
     & \textbf{Acc1} & \textbf{Acc5} & \textbf{FID} & \textbf{KNN} & \textbf{Acc1} & \textbf{Acc5} & \textbf{FID} & \textbf{KNN} \\
    \hline
    GMI & 8.40\% & 21.07\% & 41.55 & 1406.7 & 14.93\% & 33.13\% & 41.87 & 1402.5 \\
    KED-MI & 33.93\% & 64.93\% & 37.37 & 1353.5 & 44.60\% & 74.07\% & 46.97 & 1320.2 \\
    PLG-MI & \textbf{87.07\%} & \textbf{95.73\%} & 43.55 & 1277.5 & \textbf{96.67\%} & \textbf{99.67\%} & 44.15 & 1159.1 \\
    \hline
    \textbf{Ours} & 78.07\% & 93.87\% & \textbf{28.82} & \textbf{1250.0} & 94.73\% & \textbf{99.67\%} & \textbf{37.82} & \textbf{1140.1} \\
    \hline
  \end{tabular}
\end{table}

实验结果表明，在跨域场景下，本文方法的优势更加明显。特别是在FID指标上，Diff-MI在VGG16攻击中将FID从PLG-MI的43.55大幅降低至28.82。这说明本文提出的目标特异性条件扩散模型（Target-Specific CDM）能够更有效地从目标分类器中蒸馏知识，从而在公共数据分布与私有数据分布存在显著差异时，依然能重建出高质量的私有图像。

\subsection{图像质量评估}

除了FID和KNN距离，我们还使用了PSNR、SSIM和LPIPS等图像质量指标进行评估。表~\ref{tab:mia_quality}展示了在CelebA+VGG16设置下的对比结果。

\begin{table}[htbp]
  \centering
  \caption{重建图像质量评估 (CelebA + VGG16)}
  \label{tab:mia_quality}
  \begin{tabular}{lcccc}
    \hline
    \textbf{方法} & \textbf{PSNR}$\uparrow$ & \textbf{SSIM}$\uparrow$ & \textbf{LPIPS-Alex}$\downarrow$ & \textbf{LPIPS-VGG}$\downarrow$ \\
    \hline
    GMI & 13.09 & 0.39 & - & - \\
    KED-MI & 14.13 & - & - & - \\
    PLG-MI & 14.79 & - & - & - \\
    \hline
    \textbf{Ours} & \textbf{15.64} & \textbf{0.45} & \textbf{0.28} & \textbf{0.35} \\
    \hline
  \end{tabular}
\end{table}

本文方法在PSNR（15.64）和SSIM（0.45）上均优于所有基准方法，表明重建图像在像素级结构和纹理细节上更接近真实图像。此外，较低的LPIPS值（0.28）进一步证实了生成图像在感知质量上的优越性。

综上所述，本文提出的Diff-MI方法在保持高攻击成功率的同时，显著提升了重建图像的保真度和视觉质量，在标准设置和分布偏移设置下均表现出SOTA级别的综合性能。



\section[本章小结]{本章小结}
\label{sec:results_summary}

本章通过系统性的实验验证和深入分析，全面评估了本文提出的模板逆向攻击（TIA）与模型反演攻击（MIA）方法的有效性。

\textbf{TIA方法的实验结果}表明，基于StyleGAN的合成数据学习策略在白盒与黑盒场景下均取得了优异的攻击性能。在LFW数据集上，白盒攻击在FMR=$10^{-3}$下的攻击成功率（SAR）达到98.50\%，显著优于现有的SOTA方法（如GaFaR的92.17\%）。即使在黑盒场景下，SAR也能达到95.33\%，证明了方法的泛化能力。消融实验证实了身份损失（Identity Loss）在特征重建中的核心作用，而高分辨率生成能力则保证了重建图像的视觉质量。

\textbf{MIA方法的实验结果}表明，本文提出的目标特异性条件扩散模型（Diff-MI）在攻击准确率与生成保真度之间取得了良好的平衡。在标准设置下（CelebA），Diff-MI在保持高攻击准确率（Acc1 $>$ 93\%）的同时，将FID值降低至23.82，显著优于现有最佳方法PLG-MI（FID 33.27）。在更具挑战性的分布偏移设置下（FFHQ $\rightarrow$ CelebA），Diff-MI展现出更强的鲁棒性，FID大幅优于对比方法。此外，图像质量评估显示，Diff-MI生成的图像在PSNR、SSIM和LPIPS等指标上均表现最佳，验证了其在重建高质量私有图像方面的优势。

综上所述，本章的实验结果充分验证了本文提出的两种隐私攻击方法的有效性，揭示了当前人脸识别系统面临的严峻安全威胁，为后续的防御研究提供了重要的实证依据。

% Local Variables:
% TeX-master: "../main"
% TeX-engine: xetex
% End:
