% !Mode:: "TeX:UTF-8"
\chapter{理论基础}[Theoretical Foundation]
\section{引言}
\section{人脸识别模型}

深度学习技术的蓬勃发展使得人脸识别系统在近十年内取得了突破性进展，已广泛应用于身份验证、访问控制、安全监控等关键领域。现代人脸识别系统通常基于深度神经网络提取高维特征表示（即嵌入向量或模板），并通过度量学习优化使得同一身份的样本在嵌入空间中聚集、不同身份样本相互分离。本节系统性地阐述深度人脸识别系统的理论基础、技术架构与评估方法，为第3章提出的嵌入一致性约束、模板逆向重建方法及后续实验评价提供必要的理论支撑与工程依据。

\subsection{系统架构与表示学习}

典型的深度人脸识别系统可抽象为一个端到端的映射流程，由以下三个核心模块构成：

\textbf{（1）预处理模块。}该模块负责将原始输入图像转换为适合网络输入的标准化表示。具体包括：
\begin{itemize}
  \item \textit{人脸检测与对齐}：采用级联卷积网络（如 MTCNN）或基于锚框的检测器（如 RetinaFace）定位人脸区域，并通过人脸关键点（通常为5点或68点标记）进行仿射变换，将人脸对齐到标准姿态；
  \item \textit{裁剪与尺度归一化}：将对齐后的人脸裁剪为固定分辨率（常用 $112\times112$ 或 $224\times224$ 像素），以适配后续网络的输入要求；
  \item \textit{像素值归一化}：对输入像素进行归一化处理，例如缩放到 $[-1,1]$ 区间或基于训练集统计量进行零均值、单位方差标准化，以改善网络训练的收敛性与鲁棒性。
\end{itemize}

\textbf{（2）特征提取网络。}该模块是识别系统的核心，通常采用深度卷积神经网络（CNN）或 Transformer 架构作为骨干网络。给定预处理后的输入图像 $x\in\mathcal{X}$，特征提取网络 $F(\cdot;\theta)$ 输出一个固定维度的实值嵌入向量：
\begin{equation}\label{eq:embedding}
      f = F(x;\theta)\in\mathbb{R}^d,
\end{equation}
其中 $\theta$ 表示网络的可学习参数，$d$ 为嵌入维度（通常取 128、256 或 512）。该嵌入向量旨在编码输入图像的身份相关信息，同时抑制与身份无关的变化因素（如光照、表情、姿态等）。

为提高匹配的稳定性与可比性，实践中通常对嵌入向量进行 $L_2$ 归一化：
\begin{equation}\label{eq:norm_embedding}
      \tilde{f} = \frac{f}{\|f\|_2},
\end{equation}
使得所有嵌入向量位于单位超球面上。此时，余弦相似度与欧氏距离存在简单的等价关系，便于后续的相似度计算与阈值决策。

\textbf{（3）匹配与决策模块。}在推理阶段，系统通过计算查询图像嵌入 $\tilde{f}_1$ 与参考模板嵌入 $\tilde{f}_2$ 之间的相似度来判断是否属于同一身份。常用的相似度度量包括：
\begin{itemize}
  \item \textit{余弦相似度}：
  \begin{equation}\label{eq:cosine_sim}
      \mathrm{sim}_{\text{cos}}(\tilde{f}_1,\tilde{f}_2) = \tilde{f}_1 \cdot \tilde{f}_2 = \frac{f_1 \cdot f_2}{\|f_1\|_2\|f_2\|_2};
  \end{equation}
  \item \textit{欧氏距离}：
  \begin{equation}\label{eq:euclidean_dist}
      d_{\text{euc}}(\tilde{f}_1,\tilde{f}_2) = \|\tilde{f}_1 - \tilde{f}_2\|_2.
  \end{equation}
\end{itemize}
对于归一化嵌入，余弦相似度与欧氏距离具有单调关系：$d_{\text{euc}}^2 = 2(1-\mathrm{sim}_{\text{cos}})$。在验证任务中，通过设定阈值 $\tau$ 判定匹配结果；在识别任务中，则在候选集中选择相似度最高或距离最小的样本作为识别结果。

\subsection{训练范式：度量学习与判别性损失}

深度人脸识别模型的训练目标是学习一个嵌入空间，使得同一身份的样本在该空间中距离较近（类内紧致性），不同身份的样本相互远离（类间可分性）。这一目标通常通过度量学习（metric learning）实现，主流方法可归纳为以下几类：

\textbf{（1）对比损失（Contrastive Loss）。}对比损失通过构造正样本对（同一身份）与负样本对（不同身份），直接优化样本对之间的距离：
\begin{equation}\label{eq:contrastive_loss}
      L_{\text{con}} = y \cdot d^2 + (1-y) \cdot \max(0, m - d)^2,
\end{equation}
其中 $d=\|f_1-f_2\|_2$ 为嵌入距离，$y\in\{0,1\}$ 为标签（1表示同一身份，0表示不同身份），$m$ 为边界超参数。该损失鼓励正样本对距离小于边界，负样本对距离大于边界。

\textbf{（2）三元组损失（Triplet Loss）。}三元组损失通过构造锚点（anchor）、正样本（positive）与负样本（negative）的三元组，优化相对距离：
\begin{equation}\label{eq:triplet_loss}
      L_{\text{tri}} = \max\left(0, \|f_a - f_p\|_2 - \|f_a - f_n\|_2 + \alpha\right),
\end{equation}
其中 $f_a, f_p, f_n$ 分别为锚点、正样本与负样本的嵌入，$\alpha$ 为边界参数。该损失要求正样本对距离比负样本对距离至少小 $\alpha$。三元组损失的有效性依赖于困难样本挖掘策略（如 hard negative mining）。

\textbf{（3）基于角度边距的分类损失。}近年来，基于角度边距的损失函数（如 ArcFace、CosFace、SphereFace 等）成为主流方法。这类方法将人脸识别视为分类问题，在 softmax 分类器中引入角度或余弦边距，使得类内嵌入更加紧致、类间分离更加明显。

以 ArcFace 为例，其损失函数定义为：
\begin{equation}\label{eq:arcface_loss}
      L_{\text{arc}} = -\frac{1}{N}\sum_{i=1}^N \log \frac{e^{s\cos(\theta_{y_i}+m)}}{e^{s\cos(\theta_{y_i}+m)} + \sum_{j\neq y_i} e^{s\cos\theta_j}},
\end{equation}
其中 $\theta_j$ 为嵌入向量与第 $j$ 类权重向量之间的夹角，$y_i$ 为真实类别标签，$s$ 为尺度因子，$m$ 为角度边距。通过在目标类的角度上增加 $m$，ArcFace 显式地增大了决策边界，从而提升了模型的判别能力与泛化性能。

实践中，角度边距损失通常与归一化权重向量与嵌入向量结合使用，使得特征学习在超球面上进行，具有更强的几何可解释性。

\subsection{网络架构与工程实现}

\textbf{（1）骨干网络选择。}人脸识别系统的性能很大程度上取决于骨干网络的表达能力。常用的骨干架构包括：
\begin{itemize}
  \item \textit{残差网络（ResNet）}：ResNet 系列（如 ResNet-50、ResNet-100、ResNet-152）通过引入残差连接缓解了深度网络的梯度消失问题，是人脸识别中最广泛使用的骨干网络之一；
  \item \textit{轻量级网络}：在移动端或资源受限场景下，常采用 MobileNet、EfficientNet 等轻量级架构，以在保持较高精度的同时降低计算与存储开销；
  \item \textit{Transformer 架构}：近年来，基于自注意力机制的 Vision Transformer（ViT）及其变体（如 Swin Transformer）在人脸识别中也展现出了优异性能，尤其在大规模数据集上的泛化能力更强。
\end{itemize}

\textbf{（2）嵌入维度与存储策略。}嵌入维度 $d$ 的选择需要在表达能力与计算效率之间权衡。较高的维度（如 512）能提供更丰富的表示，但也增加了存储与计算成本；较低的维度（如 128）则更适合于轻量级应用。实际部署中，模板通常以归一化的浮点向量形式存储，或经过量化（如 8位整数）以进一步压缩存储空间。需要注意的是，存储格式、归一化策略以及是否附带元数据（如采集时间、设备信息等）均会影响模板的安全性与可逆性。

\textbf{（3）数据增强与训练技巧。}为提高模型的鲁棒性与泛化能力，训练过程中通常采用丰富的数据增强策略，包括随机裁剪、水平翻转、色彩抖动、随机遮挡等。此外，混合精度训练、学习率预热与余弦退火、梯度裁剪等工程技巧也是提升训练效率与模型性能的重要手段。

\subsection{评估协议与性能指标}

人脸识别系统的评估通常分为两类任务：验证（verification）与识别/检索（identification/retrieval）。

\textbf{（1）验证任务。}验证任务旨在判断两幅输入图像是否属于同一身份。评估时，通常构造正样本对与负样本对，计算嵌入相似度，并根据不同阈值计算以下指标：
\begin{itemize}
  \item \textit{真接受率（True Accept Rate, TAR）}：正样本对中相似度高于阈值的比例；
  \item \textit{假接受率（False Accept Rate, FAR）}：负样本对中相似度高于阈值的比例；
  \item \textit{TAR@FAR}：在指定假接受率（如 FAR=0.01\% 或 FAR=0.1\%）下的真接受率，是衡量系统安全性与便捷性的关键指标；
  \item \textit{ROC 曲线与 AUC}：接收者操作特征（ROC）曲线展示了不同阈值下 TAR 与 FAR 的权衡关系，曲线下面积（AUC）越大表示系统性能越优。
\end{itemize}

\textbf{（2）识别/检索任务。}识别任务在给定的候选集（gallery）中对查询样本（probe）进行检索，选出最匹配的身份。常用评估指标包括：
\begin{itemize}
  \item \textit{Top-k 准确率（Rank-k Accuracy）}：查询样本的真实身份出现在前 $k$ 个检索结果中的比例，尤其关注 Top-1（即首位命中率）；
  \item \textit{平均精度均值（Mean Average Precision, mAP）}：综合考虑精度与召回率的指标，常用于大规模检索场景。
\end{itemize}

\textbf{（3）常用基准数据集。}人脸识别领域的标准评估数据集包括 LFW（Labeled Faces in the Wild）、CFP（Celebrities in Frontal-Profile）、AgeDB、MegaFace、IJB-B/C 等。这些数据集涵盖了不同年龄、姿态、光照、遮挡等变化因素，为模型性能的全面评估提供了基准。

\subsection{模板安全性与隐私风险}

人脸识别系统中的嵌入向量（模板）既用于实时比对，也可能长期存储于数据库中，因此属于高度敏感的生物特征信息。模板一旦泄露，攻击者可能通过逆向重建、模型反演等手段恢复原始人脸图像，造成严重的隐私侵犯。影响模板安全性的因素包括：
\begin{itemize}
  \item \textit{存储格式}：浮点表示保留了更多信息，量化或哈希表示则在一定程度上降低了可逆性；
  \item \textit{归一化策略}：归一化会改变嵌入的数值分布，影响逆向攻击的难度；
  \item \textit{元数据泄露}：如果模板附带采集设备、时间戳等元数据，攻击者可能获得额外的上下文信息，进一步提高攻击成功率。
\end{itemize}

为应对上述风险，研究者提出了多种防护方案，包括可撤销生物识别（revocable biometrics）、加密模板存储（encrypted template）、模板保护方案（template protection schemes）以及基于同态加密或安全多方计算的隐私保护匹配协议。然而，这些方案在安全性、效率与兼容性之间仍存在权衡，尚未形成广泛部署的统一标准。

本章所述的嵌入表示、训练范式与评估协议将直接用于第3章所提出的嵌入一致性损失、模板逆向攻击判据（如式~\eqref{eq:attack_goal}）以及后续的实验评价。在实验设计中，本研究将同时报告识别一致性指标（如 TAR@FAR、Top-k 准确率）与感知质量指标（如 LPIPS、FID、IS 等），以全面评估模板重建攻击的有效性与防御方案的鲁棒性。

\subsection{小结}

本节系统性地阐述了深度人脸识别模型的理论基础与工程实践，涵盖了系统架构（预处理、特征提取、匹配决策）、训练范式（对比损失、三元组损失、角度边距损失）、网络架构选择（ResNet、MobileNet、Transformer）、评估协议（验证任务、识别任务、基准数据集）以及模板安全性问题。这些内容为后续章节的方法设计、实验评估与安全性分析奠定了坚实的理论基础，确保了研究工作在概念定义、度量标准与实现细节上的严谨性与一致性。


\section{扩散模型基础}

扩散概率模型（Diffusion Probabilistic Models）作为一类新兴的深度生成模型，近年来在图像生成、语音合成、分子设计等多个领域展现出了卓越的性能。与生成对抗网络（GAN）和变分自编码器（VAE）相比，扩散模型通过构建一个可逆的马尔可夫链，将数据分布逐步转化为简单的高斯分布，再通过学习逆向过程实现数据生成，具有训练稳定、生成质量高、模式覆盖全面等优势。本节系统性地阐述扩散模型的理论基础、数学推导与条件生成机制，为第3章中基于扩散模型的模板逆向重建方法提供必要的理论支撑。

\subsection{扩散模型的数学框架}

扩散模型的核心思想是将复杂的数据分布通过前向扩散过程逐步转化为简单的先验分布（通常为标准高斯分布），再通过学习参数化的反向去噪过程来生成新样本。从数学角度看，扩散模型可以从两个互补的视角来理解：基于离散时间的变分推断框架（DDPM）和基于连续时间的随机微分方程框架（Score-based SDE）。这两种表述在理论上是等价的，但在实现和应用上各有侧重。

\textbf{符号约定。}为便于后续表述，本文采用如下记号体系：
\begin{itemize}
  \item $x_0\in\mathbb{R}^D$ 表示原始数据样本；
  \item $x_t$ 表示扩散过程在时刻 $t\in[0,T]$ 的噪声化状态；
  \item $\epsilon_\theta(x_t,t)$ 或 $s_\theta(x_t,t)$ 表示参数化的噪声预测网络或得分函数估计器；
  \item $\beta_t\in(0,1)$ 为噪声调度参数，$\alpha_t=1-\beta_t$，$\overline{\alpha}_t=\prod_{i=1}^t\alpha_i$ 为累积信噪比相关量；
  \item $\sigma_t$ 表示反向过程中的随机性强度。
\end{itemize}

\subsection{前向扩散过程}

前向扩散过程将原始数据 $x_0$ 通过逐步添加高斯噪声的方式，在 $T$ 个时间步内转化为近似标准高斯分布的随机噪声 $x_T$。具体地，前向过程定义为一个马尔可夫链：
\begin{equation}\label{eq:forward_diffusion}
      q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}\,x_{t-1}, \beta_t I),
\end{equation}
其中 $\{\beta_t\}_{t=1}^T$ 为预先设定的噪声调度序列，满足 $0<\beta_1<\beta_2<\cdots<\beta_T<1$。通常采用线性调度、余弦调度或其他启发式策略，以确保扩散过程的平滑性与最终噪声水平的充分性。

整个前向过程可表示为联合分布：
\begin{equation}\label{eq:forward_joint}
      q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1}).
\end{equation}

前向扩散过程的一个重要性质是可以通过重参数化技巧直接从 $x_0$ 采样任意时刻 $t$ 的噪声状态 $x_t$，而无需逐步迭代。定义 $\alpha_t=1-\beta_t$ 和 $\overline{\alpha}_t=\prod_{i=1}^t\alpha_i$，通过递归展开式~\eqref{eq:forward_diffusion} 并利用高斯分布的卷积性质，可得：
\begin{equation}\label{eq:forward_closed_form}
      q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\overline{\alpha}_t}\,x_0, (1-\overline{\alpha}_t)I).
\end{equation}

等价地，采用重参数化表示：
\begin{equation}\label{eq:reparameterization}
      x_t = \sqrt{\overline{\alpha}_t}\,x_0 + \sqrt{1-\overline{\alpha}_t}\,\epsilon, \quad \epsilon\sim\mathcal{N}(0,I).
\end{equation}

当扩散步数 $T$ 足够大且噪声调度合理时，$\overline{\alpha}_T\approx 0$，此时 $x_T$ 近似服从标准高斯分布 $\mathcal{N}(0,I)$，从而实现了将任意数据分布映射到简单先验分布的目标。

\subsection{反向去噪过程}

反向去噪过程旨在从随机噪声 $x_T\sim\mathcal{N}(0,I)$ 出发，通过逐步去噪恢复原始数据 $x_0$。理想情况下，若已知真实的反向条件分布 $q(x_{t-1}|x_t)$，则可以精确地逆转扩散过程。然而，该分布依赖于整个数据集的全局信息，无法直接计算。

扩散模型的核心思想是用参数化的神经网络 $p_\theta$ 来近似反向过程，将其建模为一个参数化的马尔可夫链：
\begin{equation}\label{eq:reverse_process}
      p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \Sigma_\theta(x_t,t)),
\end{equation}
其中 $\mu_\theta$ 和 $\Sigma_\theta$ 为待学习的均值和协方差函数。实践中，通常将协方差固定为 $\Sigma_\theta(x_t,t)=\sigma_t^2 I$（$\sigma_t$ 为预定义或可学习的标量），仅学习均值函数。

完整的生成过程定义为：
\begin{equation}\label{eq:reverse_joint}
      p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t), \quad p(x_T)=\mathcal{N}(0,I).
\end{equation}

\subsection{训练目标：变分下界}

扩散模型的训练基于变分推断中的证据下界（Evidence Lower Bound, ELBO）。将扩散模型视为一个包含 $T$ 个隐变量的层次化隐变量模型（类似于层次化VAE），可以推导出对数似然的变分下界：
\begin{equation}\label{eq:elbo}
      \log p_\theta(x_0) \geq \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right] = -L_{\text{VLB}},
\end{equation}
其中训练目标为最小化负的变分下界 $L_{\text{VLB}}$。通过展开并利用马尔可夫性质和条件独立性，可以将 $L_{\text{VLB}}$ 分解为：
\begin{align}\label{eq:loss_decomposition}
      L_{\text{VLB}} = &\; D_{\text{KL}}(q(x_T|x_0)\|p(x_T)) \nonumber\\
      &+ \sum_{t=2}^T \mathbb{E}_{q(x_t|x_0)}\left[D_{\text{KL}}(q(x_{t-1}|x_t,x_0)\|p_\theta(x_{t-1}|x_t))\right] \nonumber\\
      &- \mathbb{E}_{q(x_1|x_0)}[\log p_\theta(x_0|x_1)],
\end{align}
其中第一项为终止时刻的KL散度（由于前向过程固定，该项为常数可忽略），第二项为中间时刻的KL散度之和，第三项为重构项。

关键的观察是，条件后验分布 $q(x_{t-1}|x_t,x_0)$ 在给定 $x_0$ 的情况下是可处理的高斯分布。利用贝叶斯规则：
\begin{equation}\label{eq:posterior}
      q(x_{t-1}|x_t,x_0) = \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)} \propto \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t,x_0), \tilde{\beta}_t I),
\end{equation}
其中通过配方可得：
\begin{align}
      \tilde{\mu}_t(x_t,x_0) &= \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}x_t + \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t}x_0, \label{eq:posterior_mean}\\
      \tilde{\beta}_t &= \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t}\beta_t. \label{eq:posterior_var}
\end{align}

为使 $p_\theta(x_{t-1}|x_t)$ 与 $q(x_{t-1}|x_t,x_0)$ 的KL散度最小化，自然的做法是让神经网络预测的均值 $\mu_\theta(x_t,t)$ 接近真实后验均值 $\tilde{\mu}_t(x_t,x_0)$。然而，$\tilde{\mu}_t$ 依赖于未知的 $x_0$，需要通过式~\eqref{eq:reparameterization} 将其重参数化为关于噪声 $\epsilon$ 的函数。

\subsection{噪声预测与得分匹配的等价性}

将式~\eqref{eq:reparameterization} 改写为：
\begin{equation}\label{eq:x0_from_xt}
      x_0 = \frac{x_t - \sqrt{1-\overline{\alpha}_t}\,\epsilon}{\sqrt{\overline{\alpha}_t}}.
\end{equation}
代入式~\eqref{eq:posterior_mean}，可以得到后验均值关于噪声 $\epsilon$ 的表达式。实践中发现，直接让网络预测噪声 $\epsilon$ 而非均值 $\mu_\theta$ 或原始数据 $x_0$，能够获得更稳定的训练与更好的生成质量。

具体地，定义噪声预测网络 $\epsilon_\theta(x_t,t)$，训练目标简化为：
\begin{equation}\label{eq:noise_loss}
      L_{\text{simple}} = \mathbb{E}_{t,x_0,\epsilon}\left[\|\epsilon - \epsilon_\theta(x_t,t)\|^2\right],
\end{equation}
其中 $t$ 从 $\{1,\ldots,T\}$ 中均匀采样，$x_0$ 从数据分布采样，$\epsilon\sim\mathcal{N}(0,I)$，$x_t$ 由式~\eqref{eq:reparameterization} 生成。该简化目标等价于在所有时间步上均匀加权的变分下界（忽略了与 $t$ 相关的权重因子）。

从得分匹配（score matching）的角度看，噪声预测目标与学习数据分布的得分函数 $\nabla_{x_t}\log p(x_t)$ 是等价的。根据Tweedie's formula，对于高斯扰动的后验均值估计，有：
\begin{equation}\label{eq:tweedie}
      \mathbb{E}[x_0|x_t] = \frac{x_t + (1-\overline{\alpha}_t)\nabla_{x_t}\log p(x_t)}{\sqrt{\overline{\alpha}_t}}.
\end{equation}
结合式~\eqref{eq:x0_from_xt} 和噪声预测 $\epsilon_\theta$，可以建立得分函数与噪声预测器的关系：
\begin{equation}\label{eq:score_noise_relation}
      \nabla_{x_t}\log p(x_t) = -\frac{1}{\sqrt{1-\overline{\alpha}_t}}\epsilon_\theta(x_t,t).
\end{equation}

这一等价性说明，训练扩散模型的噪声预测目标本质上是在学习不同噪声水平下的得分函数，而反向采样过程则相当于沿着得分函数的方向进行朗之万动力学（Langevin dynamics）采样。

\subsection{采样过程与随机微分方程视角}

训练完成后，生成新样本的过程从 $x_T\sim\mathcal{N}(0,I)$ 开始，逐步迭代反向过程直至 $x_0$。标准的DDPM采样公式为：
\begin{equation}\label{eq:ddpm_sampling}
      x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\overline{\alpha}_t}}\epsilon_\theta(x_t,t)\right) + \sigma_t z,
\end{equation}
其中 $z\sim\mathcal{N}(0,I)$，$\sigma_t$ 通常取 $\sqrt{\beta_t}$ 或 $\sqrt{\tilde{\beta}_t}$。

从连续时间的视角看，前向扩散过程可以建模为随机微分方程（SDE）：
\begin{equation}\label{eq:forward_sde}
      dx = f(x,t)\,dt + g(t)\,dW_t,
\end{equation}
其中 $f(x,t)$ 为漂移项，$g(t)$ 为扩散系数，$W_t$ 为标准维纳过程。对应的反向SDE为：
\begin{equation}\label{eq:reverse_sde}
      dx = \left[f(x,t) - g(t)^2\nabla_x\log p_t(x)\right]dt + g(t)\,d\bar{W}_t,
\end{equation}
其中 $\bar{W}_t$ 为反向维纳过程。通过学习得分函数 $\nabla_x\log p_t(x)$（即通过 $\epsilon_\theta$ 隐式表示），可以数值求解反向SDE来生成样本。

连续时间视角不仅提供了理论上的统一框架，还启发了多种高效采样器的设计，例如DDIM（去除随机性的确定性采样）、DPM-Solver（基于高阶数值解法）以及EDM（Elucidating Diffusion Models，统一调度与采样策略）等，这些方法通过减少采样步数显著提升了生成效率。

\subsection{条件生成与引导机制}

在实际应用中，往往需要根据特定条件（如类别标签、文本描述、人脸嵌入等）来控制生成内容。扩散模型的条件生成通常通过在采样过程中引入条件信息 $y$ 来实现，核心思想是学习条件分布 $p(x|y)$ 的得分函数 $\nabla_x\log p(x_t|y)$。

\textbf{（1）分类器引导（Classifier Guidance）。}若已有一个在噪声数据上训练的分类器 $p_\phi(y|x_t)$，则根据贝叶斯规则：
\begin{equation}\label{eq:classifier_guidance}
      \nabla_x\log p(x_t|y) = \nabla_x\log p(x_t) + \nabla_x\log p_\phi(y|x_t).
\end{equation}
在采样时，将无条件得分与分类器梯度相加，并通过引导强度 $\omega$ 调节条件信息的权重：
\begin{equation}\label{eq:guided_score}
      \tilde{\nabla}_x\log p(x_t|y) = \nabla_x\log p(x_t) + \omega\nabla_x\log p_\phi(y|x_t).
\end{equation}

\textbf{（2）无分类器引导（Classifier-Free Guidance）。}为避免训练额外的分类器，无分类器引导在训练时随机屏蔽条件信息（例如以一定概率将条件 $y$ 替换为空标记 $\emptyset$），使模型同时学习条件分布 $p(x|y)$ 和无条件分布 $p(x)$。在采样时，通过线性外推组合两者的得分：
\begin{equation}\label{eq:cfg}
      \tilde{\epsilon}_\theta(x_t,y,t) = \epsilon_\theta(x_t,\emptyset,t) + \omega\left[\epsilon_\theta(x_t,y,t) - \epsilon_\theta(x_t,\emptyset,t)\right],
\end{equation}
其中 $\omega>1$ 时可以增强条件的影响，但过大的 $\omega$ 可能导致生成样本偏离真实分布或产生伪影。

\textbf{（3）基于能量的引导。}对于本研究中的模板逆向重建任务，条件信息为目标人脸嵌入 $f_{\text{target}}$。可以将嵌入相似度定义为能量函数：
\begin{equation}\label{eq:energy_guidance}
      E(x;f_{\text{target}}) = -\lambda\cdot\text{sim}(F(x), f_{\text{target}}),
\end{equation}
其中 $F(\cdot)$ 为预训练的人脸识别网络，$\text{sim}(\cdot,\cdot)$ 为余弦相似度，$\lambda$ 为引导强度。在采样过程中，通过反向传播计算 $\nabla_x E(x;f_{\text{target}})$，并将其叠加到无条件得分上：
\begin{equation}\label{eq:energy_score}
      \tilde{\nabla}_x\log p(x_t|f_{\text{target}}) = \nabla_x\log p(x_t) - \nabla_x E(x_t;f_{\text{target}}).
\end{equation}

这种基于能量的引导方式允许在不重新训练生成模型的情况下，灵活地引入不同的约束条件（如身份一致性、感知质量、属性控制等），为第3章中提出的嵌入一致性约束与模板逆向重建方法提供了直接的技术路径。

\subsection{潜空间扩散与高效采样}

尽管扩散模型在像素空间取得了成功，但直接在高分辨率图像上进行扩散建模计算代价高昂。潜空间扩散模型（Latent Diffusion Models, LDM）通过在预训练的自编码器（如VAE）的潜在表示空间中进行扩散，显著降低了计算与存储开销，同时保持了生成质量。

具体地，首先训练一个编码器 $\mathcal{E}$ 和解码器 $\mathcal{D}$，使得 $z=\mathcal{E}(x)$ 为原始图像 $x$ 的低维表示，$\hat{x}=\mathcal{D}(z)\approx x$。扩散过程在潜变量 $z$ 上进行，训练目标和采样过程与像素空间扩散模型类似，最终通过解码器 $\mathcal{D}$ 将生成的潜变量映射回图像空间。

此外，为进一步提升采样效率，研究者提出了多种加速策略，包括：
\begin{itemize}
  \item \textit{确定性采样（DDIM）}：通过去除采样过程中的随机性，实现更少步数的高质量生成；
  \item \textit{高阶数值求解器（DPM-Solver、EDM）}：利用常微分方程（ODE）求解器的高阶方法减少离散化误差；
  \item \textit{知识蒸馏与一致性模型}：通过将多步扩散过程蒸馏为单步或少步模型，实现接近实时的生成速度。
\end{itemize}

本研究在实现模板重建时将综合考虑生成质量与计算效率，选用潜空间扩散模型作为基础架构，并结合高效采样器与基于能量的引导机制，以在有限的计算资源下实现高保真的人脸重建。

\subsection{小结}

本节系统性地阐述了扩散概率模型的理论基础与核心机制，涵盖了以下要点：（1）前向扩散过程通过马尔可夫链将数据分布逐步转化为高斯噪声，其闭式表达（式~\eqref{eq:forward_closed_form}）支持高效的训练数据生成；（2）反向去噪过程通过参数化神经网络学习条件分布，并基于变分下界（ELBO）进行训练；（3）噪声预测目标与得分匹配在理论上的等价性（式~\eqref{eq:score_noise_relation}），为扩散模型提供了坚实的理论基础；（4）条件生成机制（分类器引导、无分类器引导、基于能量的引导）为本研究中基于人脸嵌入的模板重建提供了直接的技术路径；（5）潜空间扩散与高效采样策略显著降低了计算开销，使得高分辨率人脸生成成为可能。

这些理论与方法将在第3章中被具体应用于设计嵌入一致性约束的条件扩散模型，以实现从人脸识别模板到原始图像的高保真重建。通过结合本节所述的得分函数引导机制与人脸识别领域的特定约束（如身份一致性、感知质量、属性保持等），本研究将探索扩散模型在生物特征隐私保护与攻击评估中的潜力与局限性。

\subsection[换脸先验模型]{换脸先验模型}

换脸（face swapping / face replacement）技术旨在将源人物的面部属性（身份特征、表情或口型）无缝地迁移到目标人物图像或视频上。尽管换脸最初多用于娱乐与影视制作，近年来该技术在深度学习推动下取得显著进展，同时也引发了严重的隐私与滥用风险问题。本小节综述主流的方法学路线、实现要点、常用评估指标以及与模板逆向重建/模型反演研究的关系。

1) 方法学路线。当前换脸方法大致可分为三类：
- 基于自编码器/生成器的端到端映射：这类方法通常采用编码器—解码器结构（或 U-Net 变体）学习从源图像到目标图像的像素级映射，训练时通过重构与身份保持的复合损失确保源身份在目标图像中得以保留；当用于视频时需加上时序一致性约束以避免抖动或闪烁。
- 基于几何/模型的融合（如 3DMM / 仿真渲染）：通过人脸关键点、3D 面部模型估计姿态与光照，先将源面部几何与纹理投影到目标基础几何上，再进行色彩与边界融合；该类方法在保持几何一致性与物理可解释性方面具有优势，便于处理大角度或光照差异。
- 基于条件生成与迁移学习的少样本/单样本方法：为提升泛化性，出现了少样本或单样本换脸方法（使用多任务训练、特征分离或风格融合），以及通过对抗训练提高合成的真实感。这些方法通常将身份表示与表情/姿态表示解耦，并在解码阶段重组以获得目标图像。

2) 身份与表情分解。换脸任务的核心挑战在于如何把“身份”与“可变因素”（表情、姿态、光照）有效分离。常用技术包括特征域投影与互信息最小化、利用专门的身份判别器保证身份一致性、以及在训练损失中引入嵌入相似度约束（如与预训练人脸识别网络的嵌入对齐）来显式维护身份特征。这一点与本研究中的“嵌入一致性”目标高度相关：将识别器嵌入作为度量或训练目标可以在换脸系统中提高身份保真度，但同时也提示了当模板或嵌入被泄露时的滥用风险。

3) 时序与视频一致性。视频换脸需关注帧间一致性与口型/表情同步问题，常用策略包括时序卷积、运动场估计、基于光流的混合以及在损失函数中加入感知级和光流一致性项；对于实时或近实时应用，还需优化推理延迟与内存占用。

4) 无缝融合与后处理。为了获得视觉上无缝的复合效果，换脸系统通常采用面部边界的软掩模、颜色一致化（color transfer）、多尺度融合与 Poisson blending 等技术来减少边界伪影和色彩不一致；这些工程化步骤对最终识别一致性也有影响，因过度平滑可能损失身份细节，而粗糙拼接又可能导致被识别器拒绝匹配。

5) 评估指标与伦理考量。换脸质量评估既包含感知真实度（FID、SSIM、LPIPS 等），也要求衡量身份保真（通过人脸识别模型的验证成功率或嵌入相似度）。重要的是，换脸研究与应用必须考虑伦理与法律约束，包括数据使用许可、明确标注合成内容、以及防止滥用的技术与政策手段（例如水印化、可检测性或合成内容签名）。

6) 与模板逆向重建的相关性。换脸技术与模板逆向重建在方法论上存在交集：二者都依赖高质量的生成模型、身份-表情分解与嵌入级约束。换脸系统展示了在人脸领域重构高保真外观的能力，这既为逆向重建提供了可借鉴的生成与融合技巧，也提醒我们应在设计防御与评估时考虑换脸方法可能带来的更强攻击主体（例如利用换脸生成的高保真图像进行识别系统的旁路攻击）。

7) 工程化建议。本研究在使用换脸或人脸合成作为基线/对照时，将采用统一的对齐与预处理流程、明确记录是否使用时序信息、并在报告中同时给出感知质量与识别一致性指标。此外，对于需要个性化适配的场景（例如少量样本下的跨域换脸），推荐采用参数高效的微调策略（如 LoRA）以在不完全重训练生成器的前提下获得较好的身份迁移效果。

上述内容为换脸技术的要点梳理，为第3章中利用生成模型进行模板重建、以及第5章中的实验对照提供参考。
\section[低秩适配技术]{低秩适配技术}
% === 被替换/新增内容的记录（保留原占位以便回溯） ===
% 原占位："低秩适配微调技术"

低秩适配（Low-Rank Adaptation，简称 LoRA）及其它参数高效微调方法，已经成为在大型生成模型或识别模型上进行任务特定适配时的常用选择。其基本出发点是：当基础模型参数规模非常大时，直接全量微调代价高、容易过拟合且不利于存储与部署；通过在若干关键权重矩阵上引入低秩可训练增量，可以在冻结原有权重的前提下以极少量额外参数实现对新任务的适配\cite{hu2021loralowrankadaptationlarge}。

下面先给出一个简洁的数学表述，再讨论工程实践要点、超参数建议与局限性。

\subsection*{数学表述}
设某层的权重矩阵为 $W\in\mathbb{R}^{d_{out}\times d_{in}}$，传统微调会直接优化 $W$。LoRA 的做法是在不改变 $W$ 的前提下引入一个可训练的低秩增量 $\Delta W$：
\begin{equation}\label{eq:lora}
      W' = W + \Delta W, \quad\Delta W = B A,
\end{equation}
其中 $A\in\mathbb{R}^{r\times d_{in}}$、$B\in\mathbb{R}^{d_{out}\times r}$，$r\ll\min(d_{in},d_{out})$ 为秩超参数。常见实现会在前向计算时以尺度因子 $\alpha$ 做缩放，即 $W' = W + \frac{\alpha}{r} B A$，以便对不同秩值间的更新幅度做归一化。

可训练参数量的附加成本为 $r(d_{in}+d_{out})$（近似），远小于全量微调的 $d_{in}d_{out}$。这一形式既可看作对权重矩阵的低秩近似，也等价于对网络权重空间做一组低维方向的线性组合。


\subsubsection*{5. LoRA 的代数表示与参数量分析}
设原权重矩阵为 $W\in\mathbb{R}^{d_{out}\times d_{in}}$，LoRA 引入低秩增量 $\Delta W = B A$，其中 $A\in\mathbb{R}^{r\times d_{in}}$、$B\in\mathbb{R}^{d_{out}\times r}$。若在前向传播中采用缩放因子 $\alpha$，则实际替换为
\begin{equation}
      W' = W + \frac{\alpha}{r} B A.
\end{equation}
参数额外开销约为 $r(d_{in}+d_{out})$，远小于 $d_{out}d_{in}$。训练时仅更新 $A,B$（和可能的缩放），可显著降低显存与存储成本。训练结束后可合并：
\begin{equation}
      W'_{\text{merged}} = W + \frac{\alpha}{r} B A
\end{equation}
以便高效推理。

梯度维度说明：若损失为 $\mathcal{L}$，则对 $A,B$ 的梯度分别为
\begin{equation}
      \nabla_A \mathcal{L} = B^{\top} \nabla_{W'} \mathcal{L},\qquad \nabla_B \mathcal{L} = \nabla_{W'} \mathcal{L}\, A^{\top},
\end{equation}
从而训练仅在低维子空间中进行权重更新。

\subsection*{应用位置与工程实现}
在视觉生成与识别模型中，LoRA 常用于以下位置：
- 注意力模块中的投影矩阵（query/key/value 投影或输出投影）；
- 前馈网络（MLP）中的线性层；
- U-Net 或 Transformer 的某些瓶颈层；
- 对卷积层，可通过在 $1\times1$ 卷积或点卷积处应用低秩分解实现类似效果。

实现要点：
% 原句备份：
% - 冻结原始网络参数，仅优化 $A,B$（以及可能的偏置项），这能显著降低显存占用并避免破坏原有能力；
% - 初始值选取：常用将 $A$ 置为小的高斯随机或零初始化、将 $B$ 置为零或小随机值，以保证训练初期网络行为接近原模型；\\
% - 缩放策略：采用 $\frac{\alpha}{r}$ 缩放项，能让不同秩设置下的学习率与幅度更可比；\\
% - 推理优化：训练完成后可将 $\Delta W$ 与 $W$ 合并以得到纯粹的密集权重用于高效推理，或在运行时将低秩模块以高效内核实现以节省内存。

- 冻结与分层解冻策略（实践建议）：通常建议在微调初期冻结基础模型 $W$，仅训练 LoRA 增量 $A,B$。这样能显著降低显存与计算开销，同时保留预训练模型的通用能力，从而减少过拟合风险与训练不稳定性。当验证性能不足时，可采用分层解冻（逐步解冻靠近输出或条件模块的少量参数）以增加适配能力，但应配合更小的学习率与更短的训练轮次以避免破坏原有能力。

- 初始化与数值稳定性：将 $A$ 以小方差高斯或零初始化、$B$ 以零或极小值初始化是常见做法；同时建议使用学习率预热（warmup）、梯度裁剪与较低的基线学习率（对 LoRA 参数）以避免早期发散。

- 缩放与超参数对齐：采用 $\frac{\alpha}{r}$ 作为缩放因子能让不同秩设置下的更新幅度更可比；在实验设计中应把 $r$ 与 $\alpha$ 作为一组超参数并联合调优，而不是孤立调整某一项。

- 正则化与监控：对 $B$ 使用轻度 L2 正则化或范数约束可抑制过大投影；训练过程中优先以识别器嵌入一致性（验证集余弦相似度 / TAR@FAR）作为早停与模型选择指标，辅以感知质量指标（LPIPS/FID）防止视觉质量退化。

- 推理与部署：训练结束后可将 $\Delta W=BA$ 与原权重合并（$W'=W+\frac{\alpha}{r}BA$）以获得最高推理速度；若需支持多套 LoRA 配置，建议将底层模型保存为只读并动态加载 LoRA 增量以节省存储与便于切换。


\subsection*{优势、局限与替代方法}
优势：低秩适配在参数效率、存储开销与部署便捷性上具有明显优势。对于需要对大量目标（例如多个身份）做个性化适配的场景，可以仅保存每个目标的小量 LoRA 权重而不是完整模型副本，从而极大降低存储成本。

局限：若目标变化过大或需要学习与基础模型截然不同的表征结构，低秩增量的容量可能不足，导致适配效果受限。此外，过度依赖缩放或不当初始化可能引入训练不稳定或伪影。

替代/补充方法：除 LoRA 外，常见的参数高效方法还包括 Adapter（在层间插入小型瓶颈模块）、Prefix-tuning（对 Transformer 的注意力键值加入可学习前缀）、BitFit（仅微调偏置）等。实践中可根据任务选择或组合这些方法（例如 LoRA + Adapter），以兼顾效率与表达能力。

\subsection*{针对模板逆向重建任务的建议}
在本研究的模板逆向重建情形，应考虑如下工程建议：
- 首先在公共数据集或通用无条件生成器上完成基础训练（或采用开源预训练权重），再对每个目标模板做 LoRA 微调（微适配），以避免对大模型全量训练；
- 微调目标应以嵌入一致性损失为核心（见第3章），同时辅以感知/像素损失以维持视觉质量；LoRA 的低秩参数仅需在微调阶段更新；
- 为提高样本效率，可在微调时采用学习率预热、梯度累积与小批次多次采样策略；
- 存储与复现：仅保存 $A,B$ 参数与对应的元信息（所用层、秩 $r$、$\alpha$、训练步数与验证指标），便于复现实验并降低存储成本。

% === 结束新增内容 ===
\section[本章小结]{本章小结}

本章围绕支持第3章和后续实验的理论与工程基础展开论述，归纳如下要点：

（一）人脸识别模型。明确了基于深度特征的识别系统的典型体系结构——预处理（对齐/裁剪/归一化）、特征提取网络 $F(\cdot;\theta)$ 与基于嵌入的匹配策略；给出了嵌入表示及其归一化、余弦相似度等常用表述，并阐明了度量学习（如对比损失、三元组损失、ArcFace 等）在构造判别性嵌入方面的作用。评估维度上区分了验证（TAR@FAR/ROC）与识别/检索（top‑k）任务，强调在研究中同时报告识别一致性与感知质量指标的必要性。

（二）扩散生成模型的数学与实现要点。系统性地回顾了离散时间 DDPM 的变分表述与基于得分函数的连续时间 SDE 表述，推导了噪声预测目标与得分函数之间的等价性（式~\eqref{eqn-5}、式~\eqref{eqn-26} 等），并说明了条件化采样（分类器引导与 classifier‑free guidance）与基于能量的引导方法之间的关系。对于工程实现，讨论了在潜空间建模（LDM）、噪声调度与高效采样器（如 EDM/二阶采样器）上的折中与优化要点。

（三）换脸先验与模板逆向的关联。回顾了主流换脸方法的分类、身份与可变因素的分解策略、视频时序一致性处理以及无缝融合的工程技术，指出换脸技术在高保真重构身份信息方面的能力如何为模板逆向重建提供方法学参考，同时也提醒了相应的伦理与滥用风险。

（四）参数高效微调（以 LoRA 为代表）。给出了 LoRA 的代数表述与参数量分析，讨论了其在注意力/线性层的应用位置、初始化/缩放/正则化等实现细节，并就如何将 LoRA 用作模板重建任务的微适配提出了工程建议（包括保存元信息、分层解冻与早停策略等）。

（五）实践建议与可复现性要点。为保证实验的可比性与可复现性，建议明确数据预处理流程与符号约定、在报告中同时给出识别一致性（TAR@FAR、top‑k）与感知质量（LPIPS、FID）指标、记录微调与采样的超参数与随机种子，并在涉及隐私或潜在风险的实验中遵守数据许可与伦理规范。

本章所建立的符号体系、理论推导与工程实践建议将直接支撑第3章中关于嵌入一致性损失、基于扩散模型的模板重建方法及其攻击/防御评估；后续实验部分将在本文给定的评估协议与实现细节下进行严格比较与报告。

% End of math supplement

% Local Variables:
% TeX-master: "../main"
% TeX-engine: xetex
% End:
