% !Mode:: "TeX:UTF-8"
\chapter{理论基础}[Theoretical Foundation]
\section{引言}
\section{人脸识别模型}

深度学习技术的蓬勃发展使得人脸识别系统在近十年内取得了突破性进展，已广泛应用于身份验证、访问控制、安全监控等关键领域。现代人脸识别系统通常基于深度神经网络提取高维特征表示（即嵌入向量或模板），并通过度量学习优化使得同一身份的样本在嵌入空间中聚集、不同身份样本相互分离。本节系统性地阐述深度人脸识别系统的理论基础、技术架构与评估方法，为第3章提出的嵌入一致性约束、模板逆向重建方法及后续实验评价提供必要的理论支撑与工程依据。

\subsection{系统架构与表示学习}

典型的深度人脸识别系统可抽象为一个端到端的映射流程，由以下三个核心模块构成：

\textbf{（1）预处理模块。}该模块负责将原始输入图像转换为适合网络输入的标准化表示。具体包括：
\begin{itemize}
  \item \textit{人脸检测与对齐}：采用级联卷积网络（如 MTCNN）或基于锚框的检测器（如 RetinaFace）定位人脸区域，并通过人脸关键点（通常为5点或68点标记）进行仿射变换，将人脸对齐到标准姿态；
  \item \textit{裁剪与尺度归一化}：将对齐后的人脸裁剪为固定分辨率（常用 $112\times112$ 或 $224\times224$ 像素），以适配后续网络的输入要求；
  \item \textit{像素值归一化}：对输入像素进行归一化处理，例如缩放到 $[-1,1]$ 区间或基于训练集统计量进行零均值、单位方差标准化，以改善网络训练的收敛性与鲁棒性。
\end{itemize}

\textbf{（2）特征提取网络。}该模块是识别系统的核心，通常采用深度卷积神经网络（CNN）或 Transformer 架构作为骨干网络。给定预处理后的输入图像 $x\in\mathcal{X}$，特征提取网络 $F(\cdot;\theta)$ 输出一个固定维度的实值嵌入向量：
\begin{equation}\label{eq:embedding}
      f = F(x;\theta)\in\mathbb{R}^d,
\end{equation}
其中 $\theta$ 表示网络的可学习参数，$d$ 为嵌入维度（通常取 128、256 或 512）。该嵌入向量旨在编码输入图像的身份相关信息，同时抑制与身份无关的变化因素（如光照、表情、姿态等）。

为提高匹配的稳定性与可比性，实践中通常对嵌入向量进行 $L_2$ 归一化：
\begin{equation}\label{eq:norm_embedding}
      \tilde{f} = \frac{f}{\|f\|_2},
\end{equation}
使得所有嵌入向量位于单位超球面上。此时，余弦相似度与欧氏距离存在简单的等价关系，便于后续的相似度计算与阈值决策。

\textbf{（3）匹配与决策模块。}在推理阶段，系统通过计算查询图像嵌入 $\tilde{f}_1$ 与参考模板嵌入 $\tilde{f}_2$ 之间的相似度来判断是否属于同一身份。常用的相似度度量包括：
\begin{itemize}
  \item \textit{余弦相似度}：
  \begin{equation}\label{eq:cosine_sim}
      \mathrm{sim}_{\text{cos}}(\tilde{f}_1,\tilde{f}_2) = \tilde{f}_1 \cdot \tilde{f}_2 = \frac{f_1 \cdot f_2}{\|f_1\|_2\|f_2\|_2};
  \end{equation}
  \item \textit{欧氏距离}：
  \begin{equation}\label{eq:euclidean_dist}
      d_{\text{euc}}(\tilde{f}_1,\tilde{f}_2) = \|\tilde{f}_1 - \tilde{f}_2\|_2.
  \end{equation}
\end{itemize}
对于归一化嵌入，余弦相似度与欧氏距离具有单调关系：$d_{\text{euc}}^2 = 2(1-\mathrm{sim}_{\text{cos}})$。在验证任务中，通过设定阈值 $\tau$ 判定匹配结果；在识别任务中，则在候选集中选择相似度最高或距离最小的样本作为识别结果。

\subsection{训练范式：度量学习与判别性损失}

深度人脸识别模型的训练目标是学习一个嵌入空间，使得同一身份的样本在该空间中距离较近（类内紧致性），不同身份的样本相互远离（类间可分性）。这一目标通常通过度量学习（metric learning）实现，主流方法可归纳为以下几类：

\textbf{（1）对比损失（Contrastive Loss）。}对比损失通过构造正样本对（同一身份）与负样本对（不同身份），直接优化样本对之间的距离：
\begin{equation}\label{eq:contrastive_loss}
      L_{\text{con}} = y \cdot d^2 + (1-y) \cdot \max(0, m - d)^2,
\end{equation}
其中 $d=\|f_1-f_2\|_2$ 为嵌入距离，$y\in\{0,1\}$ 为标签（1表示同一身份，0表示不同身份），$m$ 为边界超参数。该损失鼓励正样本对距离小于边界，负样本对距离大于边界。

\textbf{（2）三元组损失（Triplet Loss）。}三元组损失通过构造锚点（anchor）、正样本（positive）与负样本（negative）的三元组，优化相对距离：
\begin{equation}\label{eq:triplet_loss}
      L_{\text{tri}} = \max\left(0, \|f_a - f_p\|_2 - \|f_a - f_n\|_2 + \alpha\right),
\end{equation}
其中 $f_a, f_p, f_n$ 分别为锚点、正样本与负样本的嵌入，$\alpha$ 为边界参数。该损失要求正样本对距离比负样本对距离至少小 $\alpha$。三元组损失的有效性依赖于困难样本挖掘策略（如 hard negative mining）。

\textbf{（3）基于角度边距的分类损失。}近年来，基于角度边距的损失函数（如 ArcFace、CosFace、SphereFace 等）成为主流方法。这类方法将人脸识别视为分类问题，在 softmax 分类器中引入角度或余弦边距，使得类内嵌入更加紧致、类间分离更加明显。

以 ArcFace 为例，其损失函数定义为：
\begin{equation}\label{eq:arcface_loss}
      L_{\text{arc}} = -\frac{1}{N}\sum_{i=1}^N \log \frac{e^{s\cos(\theta_{y_i}+m)}}{e^{s\cos(\theta_{y_i}+m)} + \sum_{j\neq y_i} e^{s\cos\theta_j}},
\end{equation}
其中 $\theta_j$ 为嵌入向量与第 $j$ 类权重向量之间的夹角，$y_i$ 为真实类别标签，$s$ 为尺度因子，$m$ 为角度边距。通过在目标类的角度上增加 $m$，ArcFace 显式地增大了决策边界，从而提升了模型的判别能力与泛化性能。

实践中，角度边距损失通常与归一化权重向量与嵌入向量结合使用，使得特征学习在超球面上进行，具有更强的几何可解释性。

\subsection{网络架构与工程实现}

\textbf{（1）骨干网络选择。}人脸识别系统的性能很大程度上取决于骨干网络的表达能力。常用的骨干架构包括：
\begin{itemize}
  \item \textit{残差网络（ResNet）}：ResNet 系列（如 ResNet-50、ResNet-100、ResNet-152）通过引入残差连接缓解了深度网络的梯度消失问题，是人脸识别中最广泛使用的骨干网络之一；
  \item \textit{轻量级网络}：在移动端或资源受限场景下，常采用 MobileNet、EfficientNet 等轻量级架构，以在保持较高精度的同时降低计算与存储开销；
  \item \textit{Transformer 架构}：近年来，基于自注意力机制的 Vision Transformer（ViT）及其变体（如 Swin Transformer）在人脸识别中也展现出了优异性能，尤其在大规模数据集上的泛化能力更强。
\end{itemize}

\textbf{（2）嵌入维度与存储策略。}嵌入维度 $d$ 的选择需要在表达能力与计算效率之间权衡。较高的维度（如 512）能提供更丰富的表示，但也增加了存储与计算成本；较低的维度（如 128）则更适合于轻量级应用。实际部署中，模板通常以归一化的浮点向量形式存储，或经过量化（如 8位整数）以进一步压缩存储空间。需要注意的是，存储格式、归一化策略以及是否附带元数据（如采集时间、设备信息等）均会影响模板的安全性与可逆性。

\textbf{（3）数据增强与训练技巧。}为提高模型的鲁棒性与泛化能力，训练过程中通常采用丰富的数据增强策略，包括随机裁剪、水平翻转、色彩抖动、随机遮挡等。此外，混合精度训练、学习率预热与余弦退火、梯度裁剪等工程技巧也是提升训练效率与模型性能的重要手段。

\subsection{评估协议与性能指标}

人脸识别系统的评估通常分为两类任务：验证（verification）与识别/检索（identification/retrieval）。

\textbf{（1）验证任务。}验证任务旨在判断两幅输入图像是否属于同一身份。评估时，通常构造正样本对与负样本对，计算嵌入相似度，并根据不同阈值计算以下指标：
\begin{itemize}
  \item \textit{真接受率（True Accept Rate, TAR）}：正样本对中相似度高于阈值的比例；
  \item \textit{假接受率（False Accept Rate, FAR）}：负样本对中相似度高于阈值的比例；
  \item \textit{TAR@FAR}：在指定假接受率（如 FAR=0.01\% 或 FAR=0.1\%）下的真接受率，是衡量系统安全性与便捷性的关键指标；
  \item \textit{ROC 曲线与 AUC}：接收者操作特征（ROC）曲线展示了不同阈值下 TAR 与 FAR 的权衡关系，曲线下面积（AUC）越大表示系统性能越优。
\end{itemize}

\textbf{（2）识别/检索任务。}识别任务在给定的候选集（gallery）中对查询样本（probe）进行检索，选出最匹配的身份。常用评估指标包括：
\begin{itemize}
  \item \textit{Top-k 准确率（Rank-k Accuracy）}：查询样本的真实身份出现在前 $k$ 个检索结果中的比例，尤其关注 Top-1（即首位命中率）；
  \item \textit{平均精度均值（Mean Average Precision, mAP）}：综合考虑精度与召回率的指标，常用于大规模检索场景。
\end{itemize}

\textbf{（3）常用基准数据集。}人脸识别领域的标准评估数据集包括 LFW（Labeled Faces in the Wild）、CFP（Celebrities in Frontal-Profile）、AgeDB、MegaFace、IJB-B/C 等。这些数据集涵盖了不同年龄、姿态、光照、遮挡等变化因素，为模型性能的全面评估提供了基准。

\subsection{模板安全性与隐私风险}

人脸识别系统中的嵌入向量（模板）既用于实时比对，也可能长期存储于数据库中，因此属于高度敏感的生物特征信息。模板一旦泄露，攻击者可能通过逆向重建、模型反演等手段恢复原始人脸图像，造成严重的隐私侵犯。影响模板安全性的因素包括：
\begin{itemize}
  \item \textit{存储格式}：浮点表示保留了更多信息，量化或哈希表示则在一定程度上降低了可逆性；
  \item \textit{归一化策略}：归一化会改变嵌入的数值分布，影响逆向攻击的难度；
  \item \textit{元数据泄露}：如果模板附带采集设备、时间戳等元数据，攻击者可能获得额外的上下文信息，进一步提高攻击成功率。
\end{itemize}

为应对上述风险，研究者提出了多种防护方案，包括可撤销生物识别（revocable biometrics）、加密模板存储（encrypted template）、模板保护方案（template protection schemes）以及基于同态加密或安全多方计算的隐私保护匹配协议。然而，这些方案在安全性、效率与兼容性之间仍存在权衡，尚未形成广泛部署的统一标准。

本章所述的嵌入表示、训练范式与评估协议将直接用于第3章所提出的嵌入一致性损失、模板逆向攻击判据（如式~\eqref{eq:attack_goal}）以及后续的实验评价。在实验设计中，本研究将同时报告识别一致性指标（如 TAR@FAR、Top-k 准确率）与感知质量指标（如 LPIPS、FID、IS 等），以全面评估模板重建攻击的有效性。

\subsection{小结}

本节系统性地阐述了深度人脸识别模型的理论基础与工程实践，涵盖了系统架构（预处理、特征提取、匹配决策）、训练范式（对比损失、三元组损失、角度边距损失）、网络架构选择（ResNet、MobileNet、Transformer）、评估协议（验证任务、识别任务、基准数据集）以及模板安全性问题。这些内容为后续章节的方法设计、实验评估与安全性分析奠定了坚实的理论基础，确保了研究工作在概念定义、度量标准与实现细节上的严谨性与一致性。


\section{扩散模型基础}

扩散概率模型（Diffusion Probabilistic Models）作为一类新兴的深度生成模型，近年来在图像生成、语音合成、分子设计等多个领域展现出了卓越的性能。与生成对抗网络（GAN）和变分自编码器（VAE）相比，扩散模型通过构建一个可逆的马尔可夫链，将数据分布逐步转化为简单的高斯分布，再通过学习逆向过程实现数据生成，具有训练稳定、生成质量高、模式覆盖全面等优势。本节系统性地阐述扩散模型的理论基础、数学推导与条件生成机制，为第3章中基于扩散模型的模板逆向重建方法提供必要的理论支撑。

\subsection{扩散模型的数学框架}

扩散模型的核心思想是将复杂的数据分布通过前向扩散过程逐步转化为简单的先验分布（通常为标准高斯分布），再通过学习参数化的反向去噪过程来生成新样本。从数学角度看，扩散模型可以从两个互补的视角来理解：基于离散时间的变分推断框架（DDPM）和基于连续时间的随机微分方程框架（Score-based SDE）。这两种表述在理论上是等价的，但在实现和应用上各有侧重。

\textbf{符号约定。}为便于后续表述，本文采用如下记号体系：
\begin{itemize}
  \item $x_0\in\mathbb{R}^D$ 表示原始数据样本；
  \item $x_t$ 表示扩散过程在时刻 $t\in[0,T]$ 的噪声化状态；
  \item $\epsilon_\theta(x_t,t)$ 或 $s_\theta(x_t,t)$ 表示参数化的噪声预测网络或得分函数估计器；
  \item $\beta_t\in(0,1)$ 为噪声调度参数，$\alpha_t=1-\beta_t$，$\overline{\alpha}_t=\prod_{i=1}^t\alpha_i$ 为累积信噪比相关量；
  \item $\sigma_t$ 表示反向过程中的随机性强度。
\end{itemize}

\subsection{前向扩散过程}

前向扩散过程将原始数据 $x_0$ 通过逐步添加高斯噪声的方式，在 $T$ 个时间步内转化为近似标准高斯分布的随机噪声 $x_T$。具体地，前向过程定义为一个马尔可夫链：
\begin{equation}\label{eq:forward_diffusion}
      q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}\,x_{t-1}, \beta_t I),
\end{equation}
其中 $\{\beta_t\}_{t=1}^T$ 为预先设定的噪声调度序列，满足 $0<\beta_1<\beta_2<\cdots<\beta_T<1$。通常采用线性调度、余弦调度或其他启发式策略，以确保扩散过程的平滑性与最终噪声水平的充分性。

整个前向过程可表示为联合分布：
\begin{equation}\label{eq:forward_joint}
      q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1}).
\end{equation}

前向扩散过程的一个重要性质是可以通过重参数化技巧直接从 $x_0$ 采样任意时刻 $t$ 的噪声状态 $x_t$，而无需逐步迭代。定义 $\alpha_t=1-\beta_t$ 和 $\overline{\alpha}_t=\prod_{i=1}^t\alpha_i$，通过递归展开式~\eqref{eq:forward_diffusion} 并利用高斯分布的卷积性质，可得：
\begin{equation}\label{eq:forward_closed_form}
      q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\overline{\alpha}_t}\,x_0, (1-\overline{\alpha}_t)I).
\end{equation}

等价地，采用重参数化表示：
\begin{equation}\label{eq:reparameterization}
      x_t = \sqrt{\overline{\alpha}_t}\,x_0 + \sqrt{1-\overline{\alpha}_t}\,\epsilon, \quad \epsilon\sim\mathcal{N}(0,I).
\end{equation}

当扩散步数 $T$ 足够大且噪声调度合理时，$\overline{\alpha}_T\approx 0$，此时 $x_T$ 近似服从标准高斯分布 $\mathcal{N}(0,I)$，从而实现了将任意数据分布映射到简单先验分布的目标。

\subsection{反向去噪过程}

反向去噪过程旨在从随机噪声 $x_T\sim\mathcal{N}(0,I)$ 出发，通过逐步去噪恢复原始数据 $x_0$。理想情况下，若已知真实的反向条件分布 $q(x_{t-1}|x_t)$，则可以精确地逆转扩散过程。然而，该分布依赖于整个数据集的全局信息，无法直接计算。

扩散模型的核心思想是用参数化的神经网络 $p_\theta$ 来近似反向过程，将其建模为一个参数化的马尔可夫链：
\begin{equation}\label{eq:reverse_process}
      p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \Sigma_\theta(x_t,t)),
\end{equation}
其中 $\mu_\theta$ 和 $\Sigma_\theta$ 为待学习的均值和协方差函数。实践中，通常将协方差固定为 $\Sigma_\theta(x_t,t)=\sigma_t^2 I$（$\sigma_t$ 为预定义或可学习的标量），仅学习均值函数。

完整的生成过程定义为：
\begin{equation}\label{eq:reverse_joint}
      p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t), \quad p(x_T)=\mathcal{N}(0,I).
\end{equation}

\subsection{训练目标：变分下界}

扩散模型的训练基于变分推断中的证据下界（Evidence Lower Bound, ELBO）。将扩散模型视为一个包含 $T$ 个隐变量的层次化隐变量模型（类似于层次化VAE），可以推导出对数似然的变分下界：
\begin{equation}\label{eq:elbo}
      \log p_\theta(x_0) \geq \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right] = -L_{\text{VLB}},
\end{equation}
其中训练目标为最小化负的变分下界 $L_{\text{VLB}}$。通过展开并利用马尔可夫性质和条件独立性，可以将 $L_{\text{VLB}}$ 分解为：
\begin{align}\label{eq:loss_decomposition}
      L_{\text{VLB}} = &\; D_{\text{KL}}(q(x_T|x_0)\|p(x_T)) \nonumber\\
      &+ \sum_{t=2}^T \mathbb{E}_{q(x_t|x_0)}\left[D_{\text{KL}}(q(x_{t-1}|x_t,x_0)\|p_\theta(x_{t-1}|x_t))\right] \nonumber\\
      &- \mathbb{E}_{q(x_1|x_0)}[\log p_\theta(x_0|x_1)],
\end{align}
其中第一项为终止时刻的KL散度（由于前向过程固定，该项为常数可忽略），第二项为中间时刻的KL散度之和，第三项为重构项。

关键的观察是，条件后验分布 $q(x_{t-1}|x_t,x_0)$ 在给定 $x_0$ 的情况下是可处理的高斯分布。利用贝叶斯规则：
\begin{equation}\label{eq:posterior}
      q(x_{t-1}|x_t,x_0) = \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)} \propto \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t,x_0), \tilde{\beta}_t I),
\end{equation}
其中通过配方可得：
\begin{align}
      \tilde{\mu}_t(x_t,x_0) &= \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}x_t + \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t}x_0, \label{eq:posterior_mean}\\
      \tilde{\beta}_t &= \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t}\beta_t. \label{eq:posterior_var}
\end{align}

为使 $p_\theta(x_{t-1}|x_t)$ 与 $q(x_{t-1}|x_t,x_0)$ 的KL散度最小化，自然的做法是让神经网络预测的均值 $\mu_\theta(x_t,t)$ 接近真实后验均值 $\tilde{\mu}_t(x_t,x_0)$。然而，$\tilde{\mu}_t$ 依赖于未知的 $x_0$，需要通过式~\eqref{eq:reparameterization} 将其重参数化为关于噪声 $\epsilon$ 的函数。

\subsection{噪声预测与得分匹配的等价性}

将式~\eqref{eq:reparameterization} 改写为：
\begin{equation}\label{eq:x0_from_xt}
      x_0 = \frac{x_t - \sqrt{1-\overline{\alpha}_t}\,\epsilon}{\sqrt{\overline{\alpha}_t}}.
\end{equation}
代入式~\eqref{eq:posterior_mean}，可以得到后验均值关于噪声 $\epsilon$ 的表达式。实践中发现，直接让网络预测噪声 $\epsilon$ 而非均值 $\mu_\theta$ 或原始数据 $x_0$，能够获得更稳定的训练与更好的生成质量。

具体地，定义噪声预测网络 $\epsilon_\theta(x_t,t)$，训练目标简化为：
\begin{equation}\label{eq:noise_loss}
      L_{\text{simple}} = \mathbb{E}_{t,x_0,\epsilon}\left[\|\epsilon - \epsilon_\theta(x_t,t)\|^2\right],
\end{equation}
其中 $t$ 从 $\{1,\ldots,T\}$ 中均匀采样，$x_0$ 从数据分布采样，$\epsilon\sim\mathcal{N}(0,I)$，$x_t$ 由式~\eqref{eq:reparameterization} 生成。该简化目标等价于在所有时间步上均匀加权的变分下界（忽略了与 $t$ 相关的权重因子）。

从得分匹配（score matching）的角度看，噪声预测目标与学习数据分布的得分函数 $\nabla_{x_t}\log p(x_t)$ 是等价的。根据Tweedie's formula，对于高斯扰动的后验均值估计，有：
\begin{equation}\label{eq:tweedie}
      \mathbb{E}[x_0|x_t] = \frac{x_t + (1-\overline{\alpha}_t)\nabla_{x_t}\log p(x_t)}{\sqrt{\overline{\alpha}_t}}.
\end{equation}
结合式~\eqref{eq:x0_from_xt} 和噪声预测 $\epsilon_\theta$，可以建立得分函数与噪声预测器的关系：
\begin{equation}\label{eq:score_noise_relation}
      \nabla_{x_t}\log p(x_t) = -\frac{1}{\sqrt{1-\overline{\alpha}_t}}\epsilon_\theta(x_t,t).
\end{equation}

这一等价性说明，训练扩散模型的噪声预测目标本质上是在学习不同噪声水平下的得分函数，而反向采样过程则相当于沿着得分函数的方向进行朗之万动力学（Langevin dynamics）采样。

\subsection{采样过程与随机微分方程视角}

训练完成后，生成新样本的过程从 $x_T\sim\mathcal{N}(0,I)$ 开始，逐步迭代反向过程直至 $x_0$。标准的DDPM采样公式为：
\begin{equation}\label{eq:ddpm_sampling}
      x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\overline{\alpha}_t}}\epsilon_\theta(x_t,t)\right) + \sigma_t z,
\end{equation}
其中 $z\sim\mathcal{N}(0,I)$，$\sigma_t$ 通常取 $\sqrt{\beta_t}$ 或 $\sqrt{\tilde{\beta}_t}$。

从连续时间的视角看，前向扩散过程可以建模为随机微分方程（SDE）：
\begin{equation}\label{eq:forward_sde}
      dx = f(x,t)\,dt + g(t)\,dW_t,
\end{equation}
其中 $f(x,t)$ 为漂移项，$g(t)$ 为扩散系数，$W_t$ 为标准维纳过程。对应的反向SDE为：
\begin{equation}\label{eq:reverse_sde}
      dx = \left[f(x,t) - g(t)^2\nabla_x\log p_t(x)\right]dt + g(t)\,d\bar{W}_t,
\end{equation}
其中 $\bar{W}_t$ 为反向维纳过程。通过学习得分函数 $\nabla_x\log p_t(x)$（即通过 $\epsilon_\theta$ 隐式表示），可以数值求解反向SDE来生成样本。

连续时间视角不仅提供了理论上的统一框架，还启发了多种高效采样器的设计，例如DDIM（去除随机性的确定性采样）、DPM-Solver（基于高阶数值解法）以及EDM（Elucidating Diffusion Models，统一调度与采样策略）等，这些方法通过减少采样步数显著提升了生成效率。

\subsection{条件生成与引导机制}

在实际应用中，往往需要根据特定条件（如类别标签、文本描述、人脸嵌入等）来控制生成内容。扩散模型的条件生成通常通过在采样过程中引入条件信息 $y$ 来实现，核心思想是学习条件分布 $p(x|y)$ 的得分函数 $\nabla_x\log p(x_t|y)$。

\textbf{（1）分类器引导（Classifier Guidance）。}若已有一个在噪声数据上训练的分类器 $p_\phi(y|x_t)$，则根据贝叶斯规则：
\begin{equation}\label{eq:classifier_guidance}
      \nabla_x\log p(x_t|y) = \nabla_x\log p(x_t) + \nabla_x\log p_\phi(y|x_t).
\end{equation}
在采样时，将无条件得分与分类器梯度相加，并通过引导强度 $\omega$ 调节条件信息的权重：
\begin{equation}\label{eq:guided_score}
      \tilde{\nabla}_x\log p(x_t|y) = \nabla_x\log p(x_t) + \omega\nabla_x\log p_\phi(y|x_t).
\end{equation}

\textbf{（2）无分类器引导（Classifier-Free Guidance）。}为避免训练额外的分类器，无分类器引导在训练时随机屏蔽条件信息（例如以一定概率将条件 $y$ 替换为空标记 $\emptyset$），使模型同时学习条件分布 $p(x|y)$ 和无条件分布 $p(x)$。在采样时，通过线性外推组合两者的得分：
\begin{equation}\label{eq:cfg}
      \tilde{\epsilon}_\theta(x_t,y,t) = \epsilon_\theta(x_t,\emptyset,t) + \omega\left[\epsilon_\theta(x_t,y,t) - \epsilon_\theta(x_t,\emptyset,t)\right],
\end{equation}
其中 $\omega>1$ 时可以增强条件的影响，但过大的 $\omega$ 可能导致生成样本偏离真实分布或产生伪影。

\textbf{（3）基于能量的引导。}对于本研究中的模板逆向重建任务，条件信息为目标人脸嵌入 $f_{\text{target}}$。可以将嵌入相似度定义为能量函数：
\begin{equation}\label{eq:energy_guidance}
      E(x;f_{\text{target}}) = -\lambda\cdot\text{sim}(F(x), f_{\text{target}}),
\end{equation}
其中 $F(\cdot)$ 为预训练的人脸识别网络，$\text{sim}(\cdot,\cdot)$ 为余弦相似度，$\lambda$ 为引导强度。在采样过程中，通过反向传播计算 $\nabla_x E(x;f_{\text{target}})$，并将其叠加到无条件得分上：
\begin{equation}\label{eq:energy_score}
      \tilde{\nabla}_x\log p(x_t|f_{\text{target}}) = \nabla_x\log p(x_t) - \nabla_x E(x_t;f_{\text{target}}).
\end{equation}

这种基于能量的引导方式允许在不重新训练生成模型的情况下，灵活地引入不同的约束条件（如身份一致性、感知质量、属性控制等），为第3章中提出的嵌入一致性约束与模板逆向重建方法提供了直接的技术路径。

\subsection{潜空间扩散与高效采样}

尽管扩散模型在像素空间取得了成功，但直接在高分辨率图像上进行扩散建模计算代价高昂。潜空间扩散模型（Latent Diffusion Models, LDM）通过在预训练的自编码器（如VAE）的潜在表示空间中进行扩散，显著降低了计算与存储开销，同时保持了生成质量。

具体地，首先训练一个编码器 $\mathcal{E}$ 和解码器 $\mathcal{D}$，使得 $z=\mathcal{E}(x)$ 为原始图像 $x$ 的低维表示，$\hat{x}=\mathcal{D}(z)\approx x$。扩散过程在潜变量 $z$ 上进行，训练目标和采样过程与像素空间扩散模型类似，最终通过解码器 $\mathcal{D}$ 将生成的潜变量映射回图像空间。

此外，为进一步提升采样效率，研究者提出了多种加速策略，包括：
\begin{itemize}
  \item \textit{确定性采样（DDIM）}：通过去除采样过程中的随机性，实现更少步数的高质量生成；
  \item \textit{高阶数值求解器（DPM-Solver、EDM）}：利用常微分方程（ODE）求解器的高阶方法减少离散化误差；
  \item \textit{知识蒸馏与一致性模型}：通过将多步扩散过程蒸馏为单步或少步模型，实现接近实时的生成速度。
\end{itemize}

本研究在实现模板重建时将综合考虑生成质量与计算效率，选用潜空间扩散模型作为基础架构，并结合高效采样器与基于能量的引导机制，以在有限的计算资源下实现高保真的人脸重建。



\subsection{小结}

本节系统性地阐述了扩散概率模型的理论基础与核心机制，涵盖了以下要点：（1）前向扩散过程通过马尔可夫链将数据分布逐步转化为高斯噪声，其闭式表达（式~\eqref{eq:forward_closed_form}）支持高效的训练数据生成；（2）反向去噪过程通过参数化神经网络学习条件分布，并基于变分下界（ELBO）进行训练；（3）噪声预测目标与得分匹配在理论上的等价性（式~\eqref{eq:score_noise_relation}），为扩散模型提供了坚实的理论基础；（4）条件生成机制（分类器引导、无分类器引导、基于能量的引导）为本研究中基于人脸嵌入的模板重建提供了直接的技术路径；（5）潜空间扩散与高效采样策略显著降低了计算开销，使得高分辨率人脸生成成为可能。

这些理论与方法将在第3章中被具体应用于设计嵌入一致性约束的条件扩散模型，以实现从人脸识别模板到原始图像的高保真重建。通过结合本节所述的得分函数引导机制与人脸识别领域的特定约束（如身份一致性、感知质量、属性保持等），本研究将探索扩散模型在生物特征隐私保护与攻击评估中的潜力与局限性。

% 换脸生成模型与身份迁移技术 - 重新编写版本
% 请替换原文件中的 \subsection[换脸先验模型]{换脸先验模型} 到 "上述内容为换脸技术的要点梳理" 之间的内容

\subsection{换脸生成模型与身份迁移技术}

换脸（face swapping）或人脸替换（face replacement）技术旨在将源人物的面部身份特征无缝迁移至目标图像或视频帧中，同时保持目标场景的表情、姿态、光照等属性不变。该技术最早源于传统图像处理中的面部区域几何对齐与色彩融合，随着深度学习的发展，已演进为能够实现高保真、实时化身份替换的端到端生成系统。换脸技术在影视制作、虚拟化身、增强现实等领域具有广泛应用前景，但同时也引发了严重的隐私侵犯、身份伪造与信息安全风险，尤其在深度伪造（deepfake）滥用的背景下，已成为生物特征安全领域的重要研究对象。

本小节系统性地阐述换脸技术的理论框架、主流方法学路线、关键技术挑战及其与本研究中模板逆向重建任务的内在联系，为第3章中基于生成模型的模板重建方法提供技术参考与对比基准。

\subsubsection{换脸任务的形式化定义与约束}

从数学角度看，换脸任务可形式化为如下条件生成问题：给定源人物图像 $x_s\in\mathcal{X}$（包含待迁移的身份信息）与目标图像 $x_t\in\mathcal{X}$（提供姿态、表情、光照等上下文），换脸系统需生成合成图像 $\hat{x}\in\mathcal{X}$，使得：

\begin{equation}\label{eq:face_swap_objective}
\begin{aligned}
      \text{(1) 身份一致性：} &\quad \text{sim}(F(\hat{x}), F(x_s)) \geq \tau_{\text{id}}, \\
      \text{(2) 属性保持：} &\quad \mathcal{A}(\hat{x}) \approx \mathcal{A}(x_t), \\
      \text{(3) 感知真实性：} &\quad D(\hat{x}) \approx 1,
\end{aligned}
\end{equation}

其中 $F(\cdot)$ 为预训练的人脸识别网络（提取身份嵌入），$\text{sim}(\cdot,\cdot)$ 为相似度度量（如余弦相似度），$\tau_{\text{id}}$ 为身份匹配阈值；$\mathcal{A}(\cdot)$ 表示属性提取算子（如姿态角度、表情参数、光照向量等）；$D(\cdot)$ 为判别器或感知质量评估模型。

上述三项约束构成了换脸任务的核心目标：既要确保生成图像在身份空间上与源图像一致（以通过人脸识别验证），又要在视觉属性与上下文语义上继承目标图像的特征（以保持自然性与场景一致性），同时整体外观需达到足够的真实性（以规避人工或算法检测）。这三项约束之间往往存在权衡：过度追求身份保真可能导致不自然的融合或属性失真；过分强调感知质量则可能损失身份细节，导致识别失败。

\subsubsection{方法学分类与技术路线}

根据建模方式与生成机制的不同，当前主流的换脸方法可归纳为以下四类：

\textbf{（1）基于自编码器的端到端映射。}该类方法将换脸视为图像到图像的翻译问题，采用编码器-解码器结构（或U-Net变体）学习从输入图像对 $(x_s, x_t)$ 到输出图像 $\hat{x}$ 的端到端映射。训练时通常采用复合损失函数：
\begin{equation}\label{eq:autoencoder_loss}
      L_{\text{swap}} = \lambda_{\text{rec}} L_{\text{rec}} + \lambda_{\text{id}} L_{\text{id}} + \lambda_{\text{adv}} L_{\text{adv}} + \lambda_{\text{perc}} L_{\text{perc}},
\end{equation}
其中 $L_{\text{rec}}$ 为像素或潜变量重构损失，$L_{\text{id}}$ 为身份嵌入一致性损失（通常定义为 $1-\text{sim}(F(\hat{x}), F(x_s))$ 或余弦距离），$L_{\text{adv}}$ 为对抗损失（引入判别器以提升真实性），$L_{\text{perc}}$ 为感知损失（基于预训练网络的中间层特征距离，如VGG或LPIPS）。

代表性工作包括FaceSwap、DeepFaceLab等，这些方法通过共享编码器或分离编码器结构实现身份与属性的解耦。训练时，通常构造自重构任务（输入与输出为同一身份）与交叉重构任务（身份来自源图像，属性来自目标图像），通过交替优化实现身份迁移能力的学习。

\textbf{（2）基于三维模型的几何驱动方法。}该类方法利用3D可变形人脸模型（3D Morphable Model, 3DMM）对源图像与目标图像分别进行三维重建，提取几何（形状参数）、纹理（外观参数）与光照（球谐系数）等物理参数，再通过参数融合与重新渲染生成换脸结果。

具体流程包括：（a）人脸检测与关键点定位；（b）基于3DMM的参数回归（如通过优化或神经网络直接预测）；（c）几何与纹理融合（将源人物的身份相关参数替换目标参数）；（d）渲染与后处理（利用可微渲染器生成图像，并通过Poisson融合、色彩迁移等技术进行无缝合成）。

该类方法的优势在于物理可解释性强、对大角度旋转与极端光照具有更好的鲁棒性，但受限于3DMM的表达能力与拟合精度，生成的图像往往缺乏高频细节，需要额外的超分辨率或纹理细化模块进行补偿。代表性工作包括Face2Face、FaceShifter中的几何引导分支等。

\textbf{（3）基于生成对抗网络的条件生成。}该类方法将换脸任务建模为条件生成对抗网络（Conditional GAN）的学习问题，通过引入判别器与多尺度监督提升生成质量与真实性。生成器通常采用多阶段结构：首先通过编码器提取源图像的身份特征与目标图像的属性特征，再通过自适应归一化层（如AdaIN、SPADE）将身份特征注入解码器的生成过程中，最终生成换脸图像。

判别器的设计对该类方法至关重要，通常采用多尺度判别器（PatchGAN或多分辨率判别）以捕捉不同尺度的真实性线索，同时引入身份判别器（identity discriminator）以显式约束生成图像的身份一致性。代表性工作包括FSGAN、FaceShifter、SimSwap等，这些方法在公开数据集上展现了接近真实图像的合成质量，但计算代价较高，且对训练数据的多样性与规模要求严格。

\textbf{（4）基于扩散模型与隐式表示的新兴方法。}近年来，基于扩散模型（Diffusion Models）与神经辐射场（Neural Radiance Fields, NeRF）的换脸方法开始涌现。扩散模型通过在噪声去噪过程中引入身份条件与属性条件，能够生成高保真的换脸图像，同时具有更强的模式覆盖能力与训练稳定性。NeRF则通过三维隐式表示学习可控的面部外观与几何，支持跨视角、跨光照的一致性换脸。

这些方法的优势在于生成质量高、理论基础坚实，但计算开销大、推理速度慢，尚未在实时应用中广泛部署。代表性工作包括DiffSwap、DiffFace等。

\subsubsection{身份与属性解耦的关键技术}

换脸任务的核心挑战在于如何有效地将身份信息与可变因素（表情、姿态、光照、遮挡等）进行解耦，以实现精确的身份迁移而不破坏目标场景的自然性。常用的技术手段包括：

\textbf{（1）特征空间分离。}通过设计双分支编码器分别提取身份特征与属性特征，并在解码阶段通过自适应归一化层（AdaIN、AdaBN）或交叉注意力机制进行融合。身份编码器通常复用预训练的人脸识别网络（如ArcFace的骨干网络），以利用其在大规模数据上学到的判别性身份表示；属性编码器则关注姿态、表情等与身份无关的变化因素。

\textbf{（2）对抗性解耦与互信息最小化。}通过引入身份判别器与属性判别器，在训练过程中显式地优化身份与属性特征的独立性。具体地，可以在损失函数中加入互信息最小化项：
\begin{equation}\label{eq:mutual_info}
      L_{\text{MI}} = I(f_{\text{id}}; f_{\text{attr}}),
\end{equation}
其中 $f_{\text{id}}$ 与 $f_{\text{attr}}$ 分别为身份特征与属性特征，$I(\cdot;\cdot)$ 为互信息。实践中通常通过变分界或对抗性估计器来近似优化该目标。

\textbf{（3）基于3DMM的显式参数化。}利用3DMM将人脸分解为形状、外观、表情与姿态参数，通过参数级别的操作实现解耦。例如，保持目标图像的形状与姿态参数不变，仅替换源图像的外观参数，再通过可微渲染器合成换脸结果。

\textbf{（4）嵌入对齐与身份损失。}在训练过程中引入预训练人脸识别模型的嵌入一致性约束，显式地将生成图像的身份嵌入拉近到源图像的嵌入。该策略与本研究中的嵌入一致性目标高度相关（见第3章式~\eqref{eq:embedding_consistency}），事实上，本文提出的模板逆向重建方法可视为换脸技术的逆向应用：换脸从图像生成图像（在嵌入约束下），而模板重建从嵌入生成图像（在相同的嵌入约束下）。

\subsubsection{视频换脸中的时序一致性}

将换脸技术应用于视频时，除了单帧的身份与属性约束外，还需确保帧间的时序连贯性，以避免闪烁、抖动或不自然的形变。常用的时序一致性策略包括：

\textbf{（1）光流约束与运动场估计。}通过计算相邻帧之间的光流场 $\mathcal{F}_{t\to t+1}$，在损失函数中加入翘曲一致性项：
\begin{equation}\label{eq:temporal_warp}
      L_{\text{temp}} = \|\hat{x}_{t+1} - \mathcal{W}(\hat{x}_t, \mathcal{F}_{t\to t+1})\|_1,
\end{equation}
其中 $\mathcal{W}(\cdot,\cdot)$ 为翘曲操作（warp）。该损失鼓励生成的相邻帧在光流场约束下保持一致。

\textbf{（2）时序卷积与循环结构。}在生成器中引入时序卷积层（如3D卷积、因果卷积）或循环神经网络（LSTM、GRU）以显式建模时序依赖关系。这类方法能够在生成过程中自动利用历史帧信息，提升时序平滑性。

\textbf{（3）关键帧锚定与插值。}对于长视频，可以先在关键帧上进行换脸处理（确保高质量），再通过基于光流或插值的方法生成中间帧。该策略能够在保证关键帧质量的同时降低计算开销。

\textbf{（4）口型同步与表情驱动。}在音频驱动的视频换脸场景中（如虚拟主播），需要额外引入音频-视觉同步模块，通过音频特征（如MFCC、Wav2Vec）驱动面部动作单元（Action Units）或直接驱动嘴部关键点，以实现精确的口型同步。

\subsubsection{无缝融合与后处理技术}

换脸生成的图像往往在面部边界、色彩分布与光照一致性上存在不匹配问题，需要通过后处理技术进行优化：

\textbf{（1）面部掩模与软边界混合。}通过人脸分割网络或关键点生成面部掩模 $M\in[0,1]^{H\times W}$，将生成的面部区域与原始背景通过软混合进行融合：
\begin{equation}\label{eq:soft_blend}
      \hat{x}_{\text{final}} = M\odot\hat{x}_{\text{face}} + (1-M)\odot x_{\text{bg}},
\end{equation}
其中 $\odot$ 为逐元素乘法。实践中通常对掩模边界进行高斯模糊以获得更自然的过渡。

\textbf{（2）色彩迁移与直方图匹配。}通过统计方法（如均值-方差匹配、直方图均衡化）或深度学习方法（如风格迁移网络）将生成面部的色彩分布调整至与目标图像一致，以消除色差伪影。

\textbf{（3）Poisson融合。}利用Poisson方程求解平滑梯度场下的像素值，使得融合边界的梯度一致性最大化，从而实现无缝拼接。该方法在传统图像处理中广泛应用，也被集成到深度换脸流程的后处理阶段。

\textbf{（4）超分辨率与细节增强。}对于低分辨率或细节缺失的生成结果，可以通过预训练的超分辨率网络（如Real-ESRGAN、GFPGAN）进行细节恢复，以提升最终图像的视觉质量。

\subsubsection{评估指标体系}

换脸技术的评估需要同时考虑身份保真度、感知质量与安全性等多个维度：

\textbf{（1）身份一致性指标。}通过预训练的人脸识别模型计算生成图像与源图像之间的嵌入相似度（余弦相似度）或验证成功率（TAR@FAR）。该指标直接反映了换脸系统在身份迁移任务上的有效性。

\textbf{（2）感知质量指标。}包括FID（Fréchet Inception Distance）、SSIM、LPIPS、IS（Inception Score）等，用于评估生成图像的真实性与视觉质量。

\textbf{（3）属性保持指标。}通过姿态估计网络、表情识别网络或属性分类器评估生成图像在姿态、表情、年龄、性别等属性上与目标图像的一致性。

\textbf{（4）时序一致性指标（视频换脸）。}包括帧间光流误差、时序LPIPS（在时间维度上计算感知距离）以及用户研究中的抖动/闪烁评分等。

\textbf{（5）可检测性与鲁棒性。}通过深度伪造检测器（如XceptionNet、EfficientNet-B4等）评估生成图像被识别为合成内容的概率，以及在JPEG压缩、噪声添加等扰动下的鲁棒性。

\subsubsection{换脸技术与模板逆向重建的关联}

换脸技术与本研究中的模板逆向重建任务在方法论上具有深刻的内在联系：

\textbf{（1）生成模型与嵌入约束的共性。}换脸系统通过生成模型（自编码器、GAN、扩散模型）在嵌入一致性约束下生成目标图像，而模板逆向重建则从嵌入向量出发，通过相似的生成架构与约束机制恢复原始图像。两者的核心技术（如条件生成、嵌入对齐损失、感知质量优化）具有高度相似性。

\textbf{（2）身份解耦与属性控制。}换脸中的身份-属性解耦技术可直接迁移到模板重建中：在给定目标嵌入的前提下，通过控制属性变量（姿态、表情、光照）生成多样化的重建图像，从而探索嵌入对应的身份流形。

\textbf{（3）隐私泄露风险。}换脸技术展示了在嵌入约束下生成高保真人脸图像的可行性，这直接证明了模板泄露的严重性：攻击者可利用换脸技术从泄露的嵌入向量重建逼真的人脸图像，用于身份伪造或隐私侵犯。

\textbf{（4）数据增强与训练策略。}换脸系统在训练过程中采用的数据增强策略（如随机姿态、表情、光照变化）、损失函数设计（如多尺度感知损失、对抗损失）以及微调策略（如LoRA、Adapter）均可应用于模板重建模型的训练与优化，以提升生成质量与泛化能力。

\subsubsection{伦理、安全与监管考量}

换脸技术的滥用（尤其是深度伪造内容的传播）已引发全球范围内的伦理与法律关注。在研究与应用换脸技术时，必须遵守以下原则：

\textbf{（1）数据使用合规性。}训练换脸模型所用的人脸数据必须获得明确授权，且需符合相关隐私保护法规（如GDPR、CCPA等）。公开发布的模型应附带使用协议，明确禁止非法用途。

\textbf{（2）合成内容标注。}生成的换脸图像或视频应在元数据中嵌入合成标记（如EXIF标签、数字水印），并在视觉上添加可见或不可见的标识（如边框、二维码），以便于识别与溯源。

\textbf{（3）可检测性设计。}在模型设计阶段应主动引入可检测特征（如频域伪影、统计不一致性），使得合成内容易于被检测器识别，从而降低滥用风险。

\textbf{（4）技术与政策协同。}换脸技术的治理需要技术手段（如检测器、水印）与政策手段（如立法、平台审核）协同推进，构建多层次的防护体系。

\subsubsection{小结}

本小节系统性地阐述了换脸生成技术的理论基础、方法学分类、关键技术挑战及其与模板逆向重建的内在联系。换脸技术通过生成模型在嵌入约束下实现高保真的身份迁移，其核心机制（条件生成、身份解耦、嵌入对齐）与本研究中的模板重建方法具有深刻的共性。同时，换脸技术的滥用风险也凸显了生物特征模板保护的紧迫性。在后续章节中，本研究将借鉴换脸技术的生成与融合策略，设计基于扩散模型的模板逆向重建方法，并通过对比实验评估不同方法在身份一致性与感知质量上的权衡，为生物特征系统的安全性评估提供实证依据。

\section{参数高效微调与低秩适配技术}

在深度神经网络特别是大规模预训练模型的微调任务中，全参数微调（full fine-tuning）往往面临计算资源消耗巨大、容易过拟合、存储与部署成本高昂等挑战。参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法应运而生，旨在通过仅更新模型的少量参数或引入少量可训练模块，实现对下游任务的高效适配。低秩适配（Low-Rank Adaptation, LoRA）作为PEFT方法中的代表性技术，已在自然语言处理、计算机视觉等领域展现出优异的性能与效率平衡，并被广泛应用于大型语言模型、扩散生成模型等的任务特定化定制中。

本节系统性地阐述低秩适配技术的理论基础、数学原理、工程实现策略及其在模板逆向重建任务中的应用方法，为第3章中基于扩散模型的个性化模板重建提供技术支撑。

\subsection{参数高效微调的理论动机}

\subsubsection{全参数微调的局限性}

给定预训练模型 $\mathcal{M}_{\theta_0}$（参数为 $\theta_0\in\mathbb{R}^d$）与下游任务数据集 $\mathcal{D}=\{(x_i,y_i)\}_{i=1}^N$，传统的全参数微调通过最小化任务损失来更新所有模型参数：
\begin{equation}\label{eq:full_finetune}
      \theta^* = \arg\min_{\theta} \sum_{(x,y)\in\mathcal{D}} L(\mathcal{M}_{\theta}(x), y).
\end{equation}

这种方法存在以下问题：
\begin{itemize}
  \item \textit{计算与显存开销}：对于参数量达数十亿甚至千亿级的大模型（如GPT系列、Stable Diffusion等），全参数微调需要维护完整的梯度、优化器状态与激活值，显存需求往往超出单卡或常规集群的承载能力；
  \item \textit{过拟合风险}：当下游任务数据量有限时，更新全部参数容易导致模型过度拟合训练数据，泛化能力下降；
  \item \textit{存储与部署成本}：针对多个下游任务或多个目标（如不同身份、不同域）进行微调时，需为每个任务保存完整的模型副本，导致存储与部署成本线性增长；
  \item \textit{灾难性遗忘}：全参数更新可能破坏预训练模型在通用任务上的能力，导致在新任务上性能提升的同时，在原任务上性能大幅下降。
\end{itemize}

\subsubsection{低秩假设与内在维度}

参数高效微调的理论基础源于神经网络参数空间的低秩结构与任务适配的内在维度（intrinsic dimension）假设。研究表明，尽管深度神经网络的参数空间维度极高，但针对特定任务的有效优化往往可以在远低于全参数空间维度的子空间中实现。

具体地，Aghajanyan等人通过实验发现，对于给定的下游任务，存在一个低维参数子空间，在该子空间中进行优化即可达到与全参数微调相当的性能。形式化地，假设任务适配所需的参数更新 $\Delta\theta$ 位于参数空间的一个低秩子空间中：
\begin{equation}\label{eq:low_rank_assumption}
      \Delta\theta \approx V\phi,
\end{equation}
其中 $V\in\mathbb{R}^{d\times r}$ 为子空间的基向量矩阵，$\phi\in\mathbb{R}^r$ 为低维表示，$r\ll d$。这一假设为低秩适配方法提供了理论支撑：通过显式地将参数更新约束在低秩结构上，可以在大幅降低可训练参数量的同时，保持模型的适配能力。

\subsection{低秩适配（LoRA）的数学原理}

\subsubsection{核心思想与形式化定义}

LoRA的核心思想是在不改变预训练权重的前提下，通过低秩矩阵分解为模型的权重矩阵引入可训练的增量。具体地，对于神经网络中的某一权重矩阵 $W_0\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$（如线性层、注意力投影层等），LoRA将其在微调后的权重表示为：
\begin{equation}\label{eq:lora_core}
      W = W_0 + \Delta W,
\end{equation}
其中 $W_0$ 保持冻结（不参与梯度更新），而增量 $\Delta W$ 通过低秩分解表示：
\begin{equation}\label{eq:lora_decomposition}
      \Delta W = B A,
\end{equation}
这里 $B\in\mathbb{R}^{d_{\text{out}}\times r}$、$A\in\mathbb{R}^{r\times d_{\text{in}}}$，秩 $r$ 满足 $r\ll\min(d_{\text{out}}, d_{\text{in}})$。

在前向传播时，输入 $x\in\mathbb{R}^{d_{\text{in}}}$ 的输出计算为：
\begin{equation}\label{eq:lora_forward}
      h = Wx = W_0 x + BAx = W_0 x + B(Ax).
\end{equation}

为了控制不同秩设置下更新幅度的一致性，实践中通常引入缩放因子 $\alpha$：
\begin{equation}\label{eq:lora_scaling}
      W = W_0 + \frac{\alpha}{r} BA,
\end{equation}
使得更新的有效幅度为 $\frac{\alpha}{r}\|BA\|$，从而在不同秩 $r$ 下保持相对稳定的学习率敏感性。

\subsubsection{参数量与计算复杂度分析}

LoRA的参数效率优势体现在可训练参数量的大幅减少。对于原始权重矩阵 $W_0\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$，全参数微调需要存储与更新 $d_{\text{out}}\times d_{\text{in}}$ 个参数；而LoRA仅需存储与更新矩阵 $A$ 与 $B$，总参数量为：
\begin{equation}\label{eq:lora_param_count}
      \text{Params}_{\text{LoRA}} = r\times d_{\text{in}} + r\times d_{\text{out}} = r(d_{\text{in}} + d_{\text{out}}).
\end{equation}

参数压缩比为：
\begin{equation}\label{eq:lora_compression_ratio}
      \text{Ratio} = \frac{d_{\text{out}}\times d_{\text{in}}}{r(d_{\text{in}}+d_{\text{out}})} \approx \frac{\min(d_{\text{out}}, d_{\text{in}})}{r},
\end{equation}
当 $r$ 取较小值（如8、16、32）时，压缩比可达数十倍至上百倍。

在计算复杂度方面，前向传播的额外计算量为两次矩阵-向量乘法 $Ax$ 与 $B(Ax)$，复杂度为 $O(r\times d_{\text{in}} + r\times d_{\text{out}})$，相比原始的 $O(d_{\text{out}}\times d_{\text{in}})$ 在 $r\ll\min(d_{\text{out}}, d_{\text{in}})$ 时可忽略不计。反向传播时，梯度计算遵循链式法则：
\begin{equation}\label{eq:lora_gradients}
\begin{aligned}
      \nabla_A \mathcal{L} &= B^{\top} \nabla_{W}\mathcal{L}, \\
      \nabla_B \mathcal{L} &= \nabla_{W}\mathcal{L} \cdot A^{\top},
\end{aligned}
\end{equation}
其中 $\nabla_{W}\mathcal{L}$ 为损失对权重 $W$ 的梯度。由于仅需计算 $A$ 与 $B$ 的梯度，显存占用与计算开销显著降低。

\subsubsection{初始化策略与训练稳定性}

LoRA的初始化对训练的稳定性与收敛速度有重要影响。标准的初始化策略为：
\begin{itemize}
  \item \textit{矩阵 $A$ 的初始化}：采用高斯随机初始化 $A\sim\mathcal{N}(0, \sigma^2)$，其中 $\sigma$ 为较小的标准差（如0.01），以保证初始更新幅度适中；
  \item \textit{矩阵 $B$ 的初始化}：置零初始化 $B=0$，从而在训练开始时 $\Delta W = BA = 0$，模型行为与预训练权重完全一致。这一策略确保了训练初期模型输出的稳定性，避免了由于随机初始化引入的剧烈扰动。
\end{itemize}

该初始化方案的理论依据在于：通过将 $B$ 置零，使得初始时刻 $W=W_0$，模型从预训练状态出发逐步适配新任务，避免了"warm-up"阶段的性能下降或不稳定。随着训练的进行，$B$ 的梯度逐渐积累，模型开始学习任务特定的低秩更新。

为进一步提升训练稳定性，实践中还常采用以下策略：
\begin{itemize}
  \item \textit{学习率预热（Warmup）}：在训练初期使用较小的学习率，逐步增加至目标值，以避免早期梯度过大导致的发散；
  \item \textit{梯度裁剪（Gradient Clipping）}：限制梯度范数的上界，防止梯度爆炸；
  \item \textit{权重衰减与正则化}：对 $A$ 或 $B$ 施加L2正则化或范数约束，抑制参数过度增长，增强泛化能力。
\end{itemize}

\subsection{LoRA在视觉生成模型中的应用}

\subsubsection{应用位置的选择策略}

在计算机视觉的生成模型（如扩散模型、GAN、VAE等）中，LoRA通常应用于以下关键模块：

\textbf{（1）注意力机制的投影矩阵。}Transformer架构或U-Net中的自注意力与交叉注意力模块包含Query、Key、Value与输出投影矩阵（$W_Q, W_K, W_V, W_O$），这些矩阵决定了模型对特征的聚合与重组能力。在这些投影层上应用LoRA能够高效地调整注意力模式，使模型关注任务特定的特征区域。

\textbf{（2）前馈网络（Feed-Forward Network, FFN）。}FFN中的线性层（通常为两层MLP）在特征变换与非线性映射中起关键作用。对FFN的权重矩阵应用LoRA可以调整特征的表示空间，适配新的数据分布或任务需求。

\textbf{（3）卷积层的点卷积（$1\times1$ Convolution）。}对于包含卷积操作的网络（如U-Net的编码器与解码器），可以在通道混合的 $1\times1$ 卷积层上应用LoRA，通过低秩分解调整通道间的特征融合权重。

\textbf{（4）条件注入层。}在条件生成任务中（如文本到图像、图像到图像），条件信息（如文本嵌入、图像特征）通过特定的注入层（如AdaLN、cross-attention）融入生成过程。在这些层上应用LoRA能够精细控制条件信息对生成结果的影响。

实践中，通常不对所有层同时应用LoRA，而是根据任务需求与计算资源约束选择性地在若干关键层上引入低秩适配。常见的策略包括：
\begin{itemize}
  \item \textit{仅对注意力层应用LoRA}：实验表明，注意力机制对任务适配的影响最为显著，仅在注意力投影矩阵上应用LoRA即可取得良好效果；
  \item \textit{分层选择}：根据网络深度，优先在靠近输出或瓶颈层的位置应用LoRA，以最大化对生成结果的影响；
  \item \textit{动态选择}：通过实验或敏感性分析确定对任务性能贡献最大的层，针对性地应用LoRA。
\end{itemize}

\subsubsection{超参数调优与实验设计}

LoRA的性能受多个超参数影响，主要包括秩 $r$、缩放因子 $\alpha$ 以及学习率 $\eta$。合理的超参数设置对于平衡模型性能与参数效率至关重要。

\textbf{（1）秩 $r$ 的选择。}秩 $r$ 决定了低秩子空间的表达能力。较大的 $r$ 提供了更强的适配能力，但也增加了参数量与计算开销；较小的 $r$ 则可能导致表达能力不足，影响任务性能。实践中，$r$ 的典型取值范围为4到128，具体选择依赖于任务复杂度与数据量：
\begin{itemize}
  \item \textit{简单任务或小数据集}：$r=4$或$r=8$ 往往已足够；
  \item \textit{中等复杂度任务}：$r=16$或$r=32$ 是常用选择；
  \item \textit{复杂任务或大数据集}：可尝试$r=64$或$r=128$，但需权衡参数效率。
\end{itemize}

\textbf{（2）缩放因子 $\alpha$ 的设置。}缩放因子 $\alpha$ 调节低秩更新的有效幅度，通常与秩 $r$ 联合考虑。常见的做法是固定 $\alpha=r$ 或 $\alpha=2r$，使得 $\frac{\alpha}{r}$ 保持为1或2，从而在不同秩设置下保持更新幅度的可比性。若任务需要较强的适配信号，可适当增大 $\alpha$；反之，若需保留更多预训练知识，可减小 $\alpha$。

\textbf{（3）学习率的调整。}由于LoRA参数量远小于全参数模型，梯度更新的有效自由度降低，因此LoRA参数的学习率通常需要高于全参数微调时的设置。实践中建议采用：
\begin{equation}\label{eq:lora_lr}
      \eta_{\text{LoRA}} = \beta \cdot \eta_{\text{full}},
\end{equation}
其中 $\beta\in[5, 50]$ 为放大因子，具体取值需通过验证集性能进行调优。

\subsubsection{训练流程与早停策略}

针对模板逆向重建等生成任务，LoRA的训练流程通常包括以下步骤：

\textbf{步骤1：预训练模型加载与冻结。}加载预训练的扩散模型（如Stable Diffusion、DDPM等），冻结所有原始权重 $W_0$，仅将LoRA的低秩矩阵 $A$ 与 $B$ 设置为可训练状态。

\textbf{步骤2：损失函数设计。}对于模板逆向重建任务，损失函数通常包含嵌入一致性损失与感知质量损失的加权组合：
\begin{equation}\label{eq:lora_loss_template}
      \mathcal{L} = \lambda_{\text{id}} L_{\text{id}}(F(x_{\text{gen}}), f_{\text{target}}) + \lambda_{\text{perc}} L_{\text{perc}}(x_{\text{gen}}, x_{\text{ref}}),
\end{equation}
其中 $L_{\text{id}}$ 为身份一致性损失（如余弦距离或对比损失），$L_{\text{perc}}$ 为感知损失（如LPIPS），$\lambda_{\text{id}}$ 与 $\lambda_{\text{perc}}$ 为权重系数。

\textbf{步骤3：训练与验证监控。}在训练过程中，除监控训练损失外，还需在验证集上定期评估以下指标：
\begin{itemize}
  \item \textit{嵌入相似度}：生成图像与目标模板的余弦相似度或欧氏距离；
  \item \textit{感知质量}：LPIPS、FID、IS等指标；
  \item \textit{识别一致性}：在人脸识别任务中的验证成功率（TAR@FAR）。
\end{itemize}

\textbf{步骤4：早停与模型选择。}采用基于验证集性能的早停策略，当嵌入一致性指标连续若干轮未提升时停止训练，并选择验证集上表现最优的模型。为避免过拟合，建议结合身份一致性与感知质量的综合指标进行模型选择：
\begin{equation}\label{eq:lora_selection_metric}
      \text{Score} = w_1\cdot\text{sim}(F(x_{\text{gen}}), f_{\text{target}}) - w_2\cdot\text{LPIPS}(x_{\text{gen}}, x_{\text{ref}}),
\end{equation}
其中 $w_1, w_2$ 为权重系数。

\subsection{LoRA的推理优化与部署策略}

\subsubsection{权重合并与高效推理}

训练完成后，为实现最高推理效率，可将LoRA的低秩增量 $\Delta W = \frac{\alpha}{r}BA$ 与原始权重 $W_0$ 合并为单一权重矩阵：
\begin{equation}\label{eq:lora_merge}
      W_{\text{merged}} = W_0 + \frac{\alpha}{r}BA.
\end{equation}

合并后的模型在推理时与标准模型无异，无需额外的矩阵乘法操作，从而避免了运行时开销。合并过程在CPU或GPU上均可快速完成，且无精度损失。

\subsubsection{多任务与多目标的模块化部署}

LoRA的一个重要优势在于支持多任务或多目标的模块化部署。对于需要针对多个身份、多个域或多个任务进行个性化适配的场景（如本研究中的多目标模板重建），可以采用"一个基础模型+多组LoRA权重"的架构：
\begin{itemize}
  \item \textit{共享基础模型}：所有任务或目标共享同一预训练的基础模型 $\mathcal{M}_{\theta_0}$，该模型保持冻结状态并作为只读模块存储；
  \item \textit{任务特定LoRA}：为每个任务或目标训练一组独立的LoRA权重 $\{A_i, B_i\}$，存储开销仅为基础模型参数量的 $\frac{r(d_{\text{in}}+d_{\text{out}})}{d_{\text{out}}\times d_{\text{in}}}$ 倍；
  \item \textit{动态加载}：在推理时，根据任务需求动态加载对应的LoRA权重，实现快速切换而无需重新加载完整模型。
\end{itemize}

该架构在存储成本、部署灵活性与推理效率上均具有显著优势，特别适用于需要支持大量个性化配置的实际应用场景。

\subsection{LoRA的局限性与替代方法}

尽管LoRA在参数效率与性能上表现优异，但其仍存在一定的局限性：

\textbf{（1）表达能力受限。}当任务与预训练数据分布差异较大，或需要学习全新的表征结构时，低秩约束可能导致适配能力不足。此时可考虑增大秩 $r$ 或采用更灵活的PEFT方法。

\textbf{（2）超参数敏感性。}秩 $r$、缩放因子 $\alpha$ 与学习率的选择对性能有显著影响，不当的超参数设置可能导致训练不稳定或性能下降。建议通过系统的超参数搜索（如网格搜索、贝叶斯优化）确定最优配置。

\textbf{（3）与其他PEFT方法的互补性。}LoRA主要作用于权重空间，而其他PEFT方法（如Adapter、Prefix-tuning）则通过引入额外的模块或修改输入表示实现适配。在某些任务中，组合使用多种PEFT方法（如LoRA+Adapter）可能取得更好的效果。

常见的替代或补充方法包括：
\begin{itemize}
  \item \textit{Adapter}：在网络层间插入小型可训练模块（通常为瓶颈结构），通过残差连接调整特征表示；
  \item \textit{Prefix-tuning}：在输入序列前添加可学习的前缀向量，通过调整注意力的键值对实现任务适配；
  \item \textit{BitFit}：仅微调模型中的偏置项，适用于资源极度受限的场景；
  \item \textit{LoRA变体}：如AdaLoRA（自适应调整不同层的秩）、QLoRA（结合量化技术进一步降低显存需求）等。
\end{itemize}

\subsection{针对模板逆向重建任务的应用建议}

结合本研究中的模板逆向重建任务需求，提出以下基于LoRA的工程实践建议：

\textbf{（1）基础模型的选择与预训练。}采用在大规模人脸数据集上预训练的扩散模型（如基于CelebA-HQ、FFHQ训练的Stable Diffusion变体）作为基础模型，以确保模型具备生成高质量人脸图像的能力。

\textbf{（2）LoRA应用位置。}优先在U-Net的交叉注意力层与自注意力层的投影矩阵上应用LoRA，以最大化对身份特征的控制能力。根据计算资源，可选择性地在部分前馈层上也应用LoRA。

\textbf{（3）超参数配置。}建议采用 $r=16$ 或 $r=32$ 作为初始秩设置，缩放因子 $\alpha=r$，学习率为全参数微调的10-20倍（如 $\eta=1\times10^{-4}$ 至 $2\times10^{-4}$）。

\textbf{（4）训练策略。}采用学习率预热（前10\%步数线性增长）与余弦退火调度，梯度裁剪阈值设为1.0，使用AdamW优化器。每个目标模板训练2000-5000步，根据验证集嵌入相似度进行早停。

\textbf{（5）数据增强。}在微调过程中，对生成的中间图像进行随机姿态、光照、表情等增强，以提升模型对不同条件的鲁棒性，同时避免过拟合到特定的采样噪声或初始化。

\textbf{（6）存储与版本管理。}为每个目标模板保存独立的LoRA权重文件（约数MB至数十MB），并记录元信息（目标嵌入向量、训练步数、验证指标、超参数配置等），便于实验复现与模型管理。

\textbf{（7）评估协议。}在评估时，除计算生成图像与目标模板的嵌入相似度外，还需评估生成图像的感知质量（LPIPS、FID）、属性保持情况（姿态、年龄、性别等）以及在标准人脸识别基准上的验证成功率，以全面衡量模板重建的有效性与安全性。

\subsection{小结}

本节系统性地阐述了参数高效微调方法与低秩适配技术的理论基础、数学原理及其在视觉生成模型中的应用策略。LoRA通过在冻结预训练权重的前提下引入低秩可训练增量，实现了参数效率与模型性能的优异平衡，特别适用于大规模生成模型的任务特定化定制。在模板逆向重建任务中,LoRA能够以极低的参数成本（通常仅为全参数微调的1\%-5\%）实现对不同目标模板的个性化适配，同时保持预训练模型的生成质量与泛化能力。

本节所建立的LoRA应用框架（包括应用位置选择、超参数调优、训练流程、推理优化与部署策略）将直接支撑第3章中基于扩散模型的模板重建方法的实现。通过合理运用LoRA技术，本研究能够在有限的计算资源下高效地针对多个目标模板进行重建实验，为生物特征模板安全性的系统评估提供技术基础。

\section{本章小结}

本章系统构建了支撑生物特征模板逆向攻击研究的理论基础,为后续章节的方法设计与实验评估提供了必要的技术支撑。

\textbf{人脸识别系统基础。}本章建立了基于深度嵌入的人脸识别系统形式化框架,阐述了特征提取、嵌入归一化、距离度量与决策策略等核心组件,分析了度量学习中对比损失、三元组损失与角度边距损失(ArcFace、CosFace)在构造判别性嵌入空间中的作用,并建立了验证任务(TAR@FAR)与识别任务(Top-k准确率)的评估体系。这些内容为定义模板逆向攻击的目标函数与评估指标奠定了基础。

\textbf{扩散模型理论。}本章系统推导了扩散概率模型的数学原理,包括前向扩散过程的马尔可夫链表述、逆向去噪过程的变分下界优化、噪声预测与得分函数的等价性,以及分类器引导与无分类器引导的条件生成机制。通过分析潜在扩散模型(LDM)、噪声调度策略与高效采样器(DDIM、DPM-Solver),为基于扩散模型的模板重建方法提供了理论工具与实现指南。

\textbf{换脸技术方法论。}本章全面回顾了换脸技术的发展历程与技术路线,包括基于3DMM的几何方法、基于自编码器的端到端学习、基于GAN的对抗生成与基于扩散模型的新兴方法。重点分析了身份-属性解耦、时序一致性维护与无缝融合等关键技术,揭示了换脸方法在身份控制与感知质量优化方面的策略,为模板重建的损失函数设计与生成框架构建提供了方法论参考。

\textbf{参数高效微调技术。}本章阐述了低秩适配(LoRA)的理论基础与实现策略,推导了低秩增量 $\Delta W = BA$ 的数学原理,分析了应用位置选择、超参数调优、初始化策略与训练流程等工程问题。LoRA技术使得针对多个目标模板的个性化适配成为可能,为大规模模板重建实验的高效开展提供了技术保障。

本章所建立的理论框架在目标定义、技术路径、方法论参考与工程实现四个层面形成了完整的逻辑闭环,为后续章节的攻击方法设计与实验评估提供了坚实支撑。

% End of math supplement

% Local Variables:
% TeX-master: "../main"
% TeX-engine: xetex
% End:
