% !Mode:: "TeX:UTF-8"
\chapter{理论基础}[Theoretical Foundation]\label{chap:theory}
\section{引言}\label{sec:theory_intro}

生物特征识别系统已广泛应用于身份验证、访问控制、金融支付等关键领域，通常将用户的生物特征转换为固定维度的嵌入向量形成模板，并通过相似度计算实现身份匹配。与传统认证机制相比，生物特征认证具有便捷性与难以伪造性等优势，但随着深度学习技术的深入应用，模板泄露导致的隐私风险显著增加：模板具有不可撤销性，且随着生成式模型技术的发展，从泄露模板逆向重建原始图像已成为现实威胁，攻击者可利用重建图像进行身份伪造或隐私侵犯。

本章为后续提出的模板逆向攻击方法与模型反演攻击方法建立理论基础，系统阐述三个核心概念：（1）人脸识别系统——详述系统架构、训练范式、评估标准及模板安全性问题，为定义攻击目标函数与评估指标提供概念框架；（2）扩散概率模型——推导前向扩散、反向去噪、变分下界优化与条件生成机制，并介绍高效采样策略，为构建基于扩散模型的模板逆向重建攻击方法提供数学工具与算法基础；（3）参数高效微调技术——阐述低秩适配的理论与工程实践，为构建模型反演攻击方法的生成模型微调方法提供高效的技术路径。这三个方面共同构成了本研究从目标定义、生成建模到高效优化的完整理论框架。

\section{人脸识别模型}\label{sec:face_recognition}

深度学习技术的蓬勃发展使得人脸识别系统在近十年内取得了突破性进展\cite{he2016deep,kingma2014adam}，已广泛应用于身份验证、访问控制、安全监控等关键领域。现代人脸识别系统可采用两种主要架构实现——基于模板匹配的检索型架构和基于分类的端到端架构。无论采用哪种架构，系统的处理流程均可抽象为一个映射，由以下三个核心模块构成：

预处理模块。该模块负责将原始输入图像转换为适合网络输入的标准化表示。具体包括：（1）人脸检测与对齐：采用多任务级联卷积网络或基于锚框的检测器定位人脸区域，并通过人脸关键点进行仿射变换，将人脸对齐到标准姿态；（2）裁剪与尺度归一化：将对齐后的人脸裁剪为固定分辨率，常用 $112\times112$ 或 $224\times224$ 像素，以适配后续网络的输入要求；（3）像素值归一化：对输入像素进行归一化处理，例如缩放到 $[-1,1]$ 区间或基于训练集统计量进行零均值、单位方差标准化，以改善网络训练的收敛性与鲁棒性。

特征提取网络。该模块是识别系统的核心，通常采用深度卷积神经网络或 Transformer 架构作为骨干网络。给定预处理后的输入图像 $x\in\mathcal{X}$，特征提取网络 $F(\cdot;\theta)$ 输出一个固定维度的实值嵌入向量：
\begin{equation}\label{eq:embedding}
      f = F(x;\theta)\in\mathbb{R}^d
\end{equation}
其中 $\theta$ 表示网络的可学习参数，$d$ 为嵌入维度，实际应用中常取128、256或512等值。该嵌入向量旨在编码输入图像的身份相关信息，同时需要抑制诸如光照、表情、姿态等与身份无关的变化因素。为提高匹配的稳定性与可比性，实践中通常对嵌入向量进行 $L_2$ 归一化：
\begin{equation}\label{eq:norm_embedding}
      \tilde{f} = \frac{f}{\|f\|_2}
\end{equation}
使得所有嵌入向量位于单位超球面上。此时，余弦相似度与欧氏距离存在简单的等价关系，便于后续的相似度计算与阈值决策。

匹配与决策模块。在推理阶段，系统通过计算查询图像嵌入 $\tilde{f}_1$ 与参考模板嵌入 $\tilde{f}_2$ 之间的相似度来判断是否属于同一身份。常用的相似度度量为余弦相似度与欧式距离：
\begin{align}
\mathrm{sim}_{\text{cos}}(\tilde{f}_1,\tilde{f}_2) &= \tilde{f}_1 \cdot \tilde{f}_2 = \frac{f_1 \cdot f_2}{\|f_1\|_2\|f_2\|_2}, \label{eq:cosine_sim}\\
d_{\text{euc}}(\tilde{f}_1,\tilde{f}_2) &= \|\tilde{f}_1 - \tilde{f}_2\|_2. \label{eq:euclidean_dist}
\end{align}
对于归一化嵌入，两者具有单调关系：$d_{\text{euc}}^2 = 2(1-\mathrm{sim}_{\text{cos}})$。在验证任务中，通过设定阈值 $\tau$ 判定匹配结果；在识别任务中，则选择相似度最高或距离最小的样本作为识别结果。

深度人脸识别模型的训练目标是学习一个嵌入空间，使得同一身份的样本在该空间中距离较近，不同身份的样本相互远离。这一目标通常通过度量学习实现\cite{siamese2015}，主流方法可归纳为以下几类：

（1）对比损失\cite{hadsell2006dimensionality}。对比损失通过构造正样本对与负样本对，直接优化样本对之间的距离：
\begin{equation}\label{eq:contrastive_loss}
      L_{\text{con}} = y \cdot d^2 + (1-y) \cdot \max(0, m - d)^2
\end{equation}
其中 $d=\|f_1-f_2\|_2$ 为嵌入距离，$y\in\{0,1\}$ 为标签，1表示同一身份，0表示不同身份，$m$ 为边界超参数。该损失鼓励正样本对距离小于边界，负样本对距离大于边界。

（2）三元组损失（Triplet Loss）\cite{schroff2015facenet}。三元组损失通过构造锚点、正样本与负样本的三元组，优化相对距离：
\begin{equation}\label{eq:triplet_loss}
      L_{\text{tri}} = \max\left(0, \|f_a - f_p\|_2 - \|f_a - f_n\|_2 + \alpha\right)
\end{equation}
其中 $f_a, f_p, f_n$ 分别为锚点、正样本与负样本的嵌入，$\alpha$ 为边界参数。该损失要求正样本对距离比负样本对距离至少小 $\alpha$。三元组损失的有效性依赖于困难样本挖掘策略。

（3）基于角度边距的分类损失。近年来，如 ArcFace\cite{deng2019arcface}、CosFace\cite{wang2018cosface}、SphereFace\cite{liu2017sphereface}等基于角度边距的损失函数成为主流方法。这类方法将人脸识别视为分类问题，在 softmax 分类器中引入角度或余弦边距，使得类内嵌入更加紧致、类间分离更加明显。以 ArcFace 为例，其损失函数定义为：
\begin{equation}\label{eq:arcface_loss}
      L_{\text{arc}} = -\frac{1}{N}\sum_{i=1}^N \log \frac{e^{s\cos(\theta_{y_i}+m)}}{e^{s\cos(\theta_{y_i}+m)} + \sum_{j\neq y_i} e^{s\cos\theta_j}}
\end{equation}
其中 $\theta_j$ 为嵌入向量与第 $j$ 类权重向量之间的夹角，$y_i$ 为真实类别标签，$s$ 为尺度因子，$m$ 为角度边距。通过在目标类的角度上增加 $m$，ArcFace 显式地增大了决策边界，从而提升了模型的判别能力与泛化性能。实践中，角度边距损失通常与归一化权重向量与嵌入向量结合使用，使得特征学习在超球面上进行，具有更强的几何可解释性。

除了损失函数的设计，人脸识别系统的性能很大程度上还取决于骨干网络的表达能力。常用的骨干架构包括以下几类：（1）残差网络\cite{he2016deep}：通过引入残差连接缓解了深度网络的梯度消失问题，是人脸识别中最广泛使用的骨干网络之一，其深层结构能够学习复杂的特征表示；（2）在移动端或资源受限场景下，常采用 MobileNet\cite{howard2017mobilenets}、EfficientNet\cite{tan2019efficientnet} 等轻量级架构，以在保持较高精度的同时降低计算与存储开销；（3）近年来，基于自注意力机制的 Vision Transformer\cite{dosovitskiy2020image}及其变体在人脸识别中也展现出了优异性能，例如 Swin Transformer 等改进模型在大规模数据集上的泛化能力更强。

在确定网络架构后，嵌入维度 $d$ 的选择需要在表达能力与计算效率之间权衡。选用较高的维度（例如512）能提供更丰富的表示，但也增加了存储与计算成本；选用较低的维度（例如128）则更适合于轻量级应用。实际部署中，模板通常以归一化的浮点向量形式存储，也可经过量化处理以进一步压缩存储空间。需要注意的是，存储格式、归一化策略以及是否附带元数据均会影响模板的安全性与可逆性。

为提高模型的鲁棒性与泛化能力，训练过程中通常采用丰富的数据增强策略，包括随机裁剪、水平翻转、色彩抖动、随机遮挡等，以模拟真实场景中的各种变化因素。此外，混合精度训练、学习率预热与余弦退火、梯度裁剪等工程技巧也是提升训练效率与模型性能的重要手段，这些训练策略的综合运用使得现代人脸识别模型能够在大规模数据集上达到接近人类水平的识别精度。

人脸识别系统的评估通常分为两类任务：验证与识别/检索。

验证任务旨在判断两幅输入图像是否属于同一身份。评估时，通常构造正样本对与负样本对，计算嵌入相似度，并根据不同阈值计算以下指标：

(1)真接受率(True Accept Rate, TAR):正样本对中相似度高于阈值的比例;(2)假接受率(False Accept Rate, FAR):负样本对中相似度高于阈值的比例;(3)接收者操作特征(Receiver Operating Characteristic, ROC)曲线与曲线下面积(Area Under Curve, AUC):接收者操作特征曲线展示了不同阈值下 TAR 与 FAR 的权衡关系,曲线下面积越大表示系统性能越优。

对于模板逆向攻击评估,本研究采用攻击成功率(Success Attack Rate, SAR)作为核心指标,即在指定误识率(False Match Rate, FMR)阈值下重建图像成功通过身份验证的比例。实际应用中常考察 FMR=$10^{-2}$ 或 FMR=$10^{-3}$ 等不同安全级别下的攻击表现。

识别任务在给定的候选集中对查询样本进行检索,选出最匹配的身份。常用评估指标包括:

(1)Top-k 准确率(Rank-k Accuracy):查询样本的真实身份出现在前 $k$ 个检索结果中的比例,尤其关注首位命中率Top-1与Top-5准确率;(2)平均精度均值(Mean Average Precision, mAP):综合考虑精度与召回率的指标,常用于大规模检索场景。

\subsection{小结}

本节系统阐述了深度人脸识别系统的理论基础与技术架构，建立了从原始图像到固定维度嵌入向量的完整映射框架。通过分析主流的度量学习方法，揭示了如何在嵌入空间中构造判别性表示，使同一身份样本聚集、不同身份样本分离。这些理论与方法为后续章节定义模板逆向攻击的目标函数提供了形式化框架，同时，人脸识别系统的评估指标体系也将直接用于衡量重建图像的身份保真度与攻击成功率。

\section{扩散概率模型}\label{sec:diffusion_models}

扩散概率模型作为一类新兴的深度生成模型,近年来在图像生成、语音合成、分子设计等多个领域展现出了卓越的性能\cite{hoDenoisingDiffusionProbabilistic2020,songScoreBasedGenerativeModeling2021}。与生成对抗网络\cite{goodfellowGenerativeAdversarialNetworks2014}和变分自编码器\cite{kingmaAutoEncodingVariational2014}相比,扩散模型通过构建可逆的马尔可夫链,将数据分布逐步转化为简单的高斯分布,再通过学习逆向过程实现数据生成,具有训练稳定、生成质量高、模式覆盖全面等优势。本节基于离散时间的去噪扩散概率模型（Denoising Diffusion Probabilistic Models，DDPM）框架,系统推导扩散模型的理论基础、训练优化方法、高效采样策略与条件生成机制,为后续基于扩散模型的模板重建方法提供数学工具与算法基础。

\subsection{前向扩散过程}

前向扩散过程将原始数据 $x_0$ 通过逐步添加高斯噪声的方式，在 $T$ 个时间步内转化为近似标准高斯分布的随机噪声 $x_T$（如图~\ref{fig:forward_diffusion}所示）。具体地，前向过程定义为一个马尔可夫链：
\begin{equation}\label{eq:forward_diffusion}
      q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}\,x_{t-1}, \beta_t I)
\end{equation}
其中 $x_t$ 表示扩散过程在时刻 $t$ 的噪声化状态,$\{\beta_t\}_{t=1}^T$ 为噪声调度参数序列。整个前向过程可表示为联合分布：
\begin{equation}\label{eq:forward_joint}
      q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1})
\end{equation}

前向扩散过程可以通过重参数化技巧直接从 $x_0$ 采样任意时刻 $t$ 的噪声状态 $x_t$，而无需逐步迭代。定义 $\alpha_t=1-\beta_t$ 和 $\overline{\alpha}_t=\prod_{i=1}^t\alpha_i$ ，通过递归展开式~\eqref{eq:forward_diffusion} 并利用高斯分布的卷积性质，可得：
\begin{equation}\label{eq:forward_closed_form}
      q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\overline{\alpha}_t}\,x_0, (1-\overline{\alpha}_t)I)
\end{equation}

等价地，采用重参数化表示：
\begin{equation}\label{eq:reparameterization}
      x_t = \sqrt{\overline{\alpha}_t}\,x_0 + \sqrt{1-\overline{\alpha}_t}\,\epsilon, \quad \epsilon\sim\mathcal{N}(0,I)
\end{equation}
其中 $\epsilon$ 为服从标准高斯分布的随机噪声。

当扩散步数 $T$ 足够大且噪声调度合理时，$\overline{\alpha}_T\approx 0$，此时 $x_T$ 近似服从标准高斯分布 $\mathcal{N}(0,I)$，从而实现了将任意数据分布映射到简单先验分布的目标。

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{images/forward_composed.png}
  \caption{前向扩散过程}
  \label{fig:forward_diffusion}
\end{figure}

\subsection{反向去噪过程与生成模型}

反向去噪过程旨在从随机噪声 $x_T\sim\mathcal{N}(0,I)$ 出发，通过逐步去噪恢复原始数据 $x_0$（如图~\ref{fig:reverse_diffusion}所示）。理想情况下，若已知真实的反向条件分布 $q(x_{t-1}|x_t)$，则可以精确地逆转扩散过程。然而，该分布依赖于整个数据集的全局信息，无法直接计算。

扩散模型的核心思想是用参数化的神经网络 $p_\theta$ 来近似反向过程，将其建模为一个参数化的马尔可夫链：
\begin{equation}\label{eq:reverse_process}
      p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \Sigma_\theta(x_t,t))
\end{equation}
其中 $\mu_\theta$ 和 $\Sigma_\theta$ 为待学习的均值和协方差函数。实践中，通常将协方差固定为 $\Sigma_\theta(x_t,t)=\sigma_t^2 I$，仅学习均值函数。

完整的生成过程定义为：
\begin{equation}\label{eq:reverse_joint}
      p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t), \quad p(x_T)=\mathcal{N}(0,I)
\end{equation}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{images/reverse_decomposed.png}
  \caption{反向去噪过程}
  \label{fig:reverse_diffusion}
\end{figure}

\subsection{模型训练}

\subsubsection{变分下界推导}

扩散模型的训练基于变分推断中的证据下界。将扩散模型视为一个包含 $T$ 个隐变量的层次化隐变量模型，类似于层次化VAE，可以推导出对数似然的变分下界：
\begin{equation}\label{eq:elbo}
      \log p_\theta(x_0) \geq \mathbb{E}_{q(x_{1:T}|x_0)}\left[\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right] = -L_{\text{VLB}}
\end{equation}

其中训练目标为最小化负的变分下界 $L_{\text{VLB}}$。通过展开并利用马尔可夫性质,可将 $L_{\text{VLB}}$ 分解为三项：
\begin{align}\label{eq:loss_decomposition}
      L_{\text{VLB}} = &\; D_{\text{KL}}(q(x_T|x_0)\|p(x_T)) \nonumber\\
      &+ \sum_{t=2}^T \mathbb{E}_{q(x_t|x_0)}\left[D_{\text{KL}}(q(x_{t-1}|x_t,x_0)\|p_\theta(x_{t-1}|x_t))\right] \nonumber\\
      &- \mathbb{E}_{q(x_1|x_0)}[\log p_\theta(x_0|x_1)],
\end{align}
其中第一项为终止项可在训练中忽略,第二项为去噪项是训练的核心,第三项为重构项用于确保生成质量。

\subsubsection{后验分布与重参数化}

关键的观察是，条件后验分布 $q(x_{t-1}|x_t,x_0)$ 在给定 $x_0$ 的情况下是可处理的高斯分布。利用贝叶斯规则并结合前向扩散过程的马尔可夫性质：
\begin{equation}\label{eq:posterior}
      q(x_{t-1}|x_t,x_0) = \frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)} \propto \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t,x_0), \tilde{\beta}_t I)
\end{equation}
由于前向过程每一步均为高斯分布，根据高斯分布的性质，上述后验分布也服从高斯分布。具体地，将式~\eqref{eq:forward_diffusion} 和式~\eqref{eq:forward_closed_form} 代入贝叶斯公式并利用高斯分布的指数形式展开，可以得到后验均值与方差的表达：
\begin{align}
      \tilde{\mu}_t(x_t,x_0) &= \frac{\sqrt{\alpha_t}(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}x_t + \frac{\sqrt{\overline{\alpha}_{t-1}}\beta_t}{1-\overline{\alpha}_t}x_0, \label{eq:posterior_mean}\\
      \tilde{\beta}_t &= \frac{1-\overline{\alpha}_{t-1}}{1-\overline{\alpha}_t}\beta_t. \label{eq:posterior_var}
\end{align}

\subsubsection{噪声预测目标}

为使 $p_\theta(x_{t-1}|x_t)$ 与 $q(x_{t-1}|x_t,x_0)$ 的KL散度最小化，自然的做法是让神经网络预测的均值 $\mu_\theta(x_t,t)$ 接近真实后验均值 $\tilde{\mu}_t(x_t,x_0)$。然而，$\tilde{\mu}_t$ 依赖于未知的 $x_0$，需要通过式~\eqref{eq:reparameterization} 将其重参数化为关于噪声 $\epsilon$ 的函数。

将式~\eqref{eq:reparameterization} 改写为：
\begin{equation}\label{eq:x0_from_xt}
      x_0 = \frac{x_t - \sqrt{1-\overline{\alpha}_t}\,\epsilon}{\sqrt{\overline{\alpha}_t}}
\end{equation}
代入式~\eqref{eq:posterior_mean}，可以得到后验均值关于噪声 $\epsilon$ 的表达式。实践中发现，直接让网络预测噪声 $\epsilon$ 而非均值 $\mu_\theta$ 或原始数据 $x_0$，能够获得更稳定的训练与更好的生成质量。具体地，定义噪声预测网络 $\epsilon_\theta(x_t,t)$，训练目标简化为：
\begin{equation}\label{eq:noise_loss}
      L_{\text{simple}} = \mathbb{E}_{t,x_0,\epsilon}\left[\|\epsilon - \epsilon_\theta(x_t,t)\|^2\right]
\end{equation}
其中 $t$ 从 $\{1,\ldots,T\}$ 中均匀采样,$x_0$ 从数据分布采样,$\epsilon\sim\mathcal{N}(0,I)$,$x_t$ 由式~\eqref{eq:reparameterization} 生成。从得分匹配的视角看,该噪声预测目标等价于学习数据分布的得分函数 $\nabla_{x_t}\log p(x_t)$,两者关系为：
\begin{equation}\label{eq:score_noise_relation}
      \nabla_{x_t}\log p(x_t) = -\frac{1}{\sqrt{1-\overline{\alpha}_t}}\epsilon_\theta(x_t,t)
\end{equation}

\subsection{采样策略}

\subsubsection{标准DDPM采样}

训练完成后，生成新样本的过程从 $x_T\sim\mathcal{N}(0,I)$ 开始，逐步迭代反向过程直至 $x_0$。标准的DDPM采样公式为：
\begin{equation}\label{eq:ddpm_sampling}
      x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\overline{\alpha}_t}}\epsilon_\theta(x_t,t)\right) + \sigma_t z
\end{equation}
其中 $z\sim\mathcal{N}(0,I)$ 为标准高斯噪声，$\sigma_t$ 通常取 $\sqrt{\beta_t}$ 或 $\sqrt{\tilde{\beta}_t}$。

\subsubsection{确定性采样与加速策略}

为提高采样效率,去噪扩散隐式模型（Denoising Diffusion Implicit Models，DDIM）通过设置$\sigma_t=0$构造确定性采样轨迹：
\begin{equation}\label{eq:ddim_sampling}
      x_{t-1} = \sqrt{\overline{\alpha}_{t-1}}\left(\frac{x_t - \sqrt{1-\overline{\alpha}_t}\epsilon_\theta(x_t,t)}{\sqrt{\overline{\alpha}_t}}\right) + \sqrt{1-\overline{\alpha}_{t-1}}\epsilon_\theta(x_t,t)
\end{equation}
DDIM允许使用任意子序列时间步采样,可将步数从1000步降低至50步甚至10步而无需重新训练。EDM\cite{karrasEluvidatingDiffusionModels2022}进一步通过统一噪声调度与网络预处理设计,提升了训练稳定性与采样效率。

值得注意的是，从连续时间视角看，前向扩散可建模为随机微分方程（Stochastic Differential Equation，SDE），对应的反向SDE通过学习得分函数$\nabla_x\log p_t(x)$来生成样本。这一视角与离散时间的DDPM框架在理论上等价，但提供了统一连续与离散采样的数学工具。这些高效采样策略为本研究的模板重建方法提供了重要技术支撑。

\subsection{条件生成机制}

在实际应用中，往往需要根据特定条件来控制生成内容，这些条件可以是类别标签、文本描述、人脸嵌入等多种形式。扩散模型的条件生成通常通过在采样过程中引入条件信息 $y$ 来实现，核心思想是学习条件分布 $p(x|y)$ 的得分函数 $\nabla_x\log p(x_t|y)$。本小节介绍三种主流的引导机制。

\subsubsection{分类器引导}

若已有一个参数为$\phi$的分类器$p_\phi(y|x_t)$在噪声数据上训练完成，则根据贝叶斯规则：
\begin{equation}\label{eq:classifier_guidance}
      \nabla_x\log p(x_t|y) = \nabla_x\log p(x_t) + \nabla_x\log p_\phi(y|x_t)
\end{equation}
在采样时，将无条件得分与分类器梯度相加，并通过引导强度 $\omega$ 调节条件信息的权重：
\begin{equation}\label{eq:guided_score}
      \tilde{\nabla}_x\log p(x_t|y) = \nabla_x\log p(x_t) + \omega\nabla_x\log p_\phi(y|x_t)
\end{equation}

\subsubsection{无分类器引导}

为避免训练额外的分类器，无分类器引导在训练时采用随机屏蔽条件信息的策略，即以一定概率将条件 $y$ 替换为空标记 $\emptyset$，使模型同时学习条件分布 $p(x|y)$ 和无条件分布 $p(x)$。在采样时，通过线性外推组合两者的得分：
\begin{equation}\label{eq:cfg}
      \tilde{\epsilon}_\theta(x_t,y,t) = \epsilon_\theta(x_t,\emptyset,t) + \omega\left[\epsilon_\theta(x_t,y,t) - \epsilon_\theta(x_t,\emptyset,t)\right]
\end{equation}
其中 $\omega>1$ 时可以增强条件的影响，但过大的 $\omega$ 可能导致生成样本偏离真实分布或产生伪影。

\subsubsection{基于能量的引导}

对于本研究中的模板逆向重建任务，条件信息为目标人脸嵌入 $f_{\text{target}}$。可以将嵌入相似度定义为能量函数：
\begin{equation}\label{eq:energy_guidance}
      E(x;f_{\text{target}}) = -\lambda\cdot\text{sim}(F(x), f_{\text{target}})
\end{equation}
其中 $F(\cdot)$ 为预训练的人脸识别网络，$\text{sim}(\cdot,\cdot)$ 为余弦相似度，$\lambda$ 为引导强度。在采样过程中，通过反向传播计算 $\nabla_x E(x;f_{\text{target}})$，并将其叠加到无条件得分上：
\begin{equation}\label{eq:energy_score}
      \tilde{\nabla}_x\log p(x_t|f_{\text{target}}) = \nabla_x\log p(x_t) - \nabla_x E(x_t;f_{\text{target}})
\end{equation}

这种基于能量的引导方式允许在不重新训练生成模型的情况下，灵活地引入多种约束条件，包括身份一致性、感知质量、属性控制等不同方面的需求，为第3章中提出的嵌入一致性约束与模板逆向重建方法提供了直接的技术路径。

\subsection{小结}

本节系统推导了扩散概率模型的数学原理，建立了从前向扩散到反向去噪的完整理论框架。通过变分下界推导，揭示了噪声预测目标与得分函数估计的内在联系，为训练稳定、生成质量高的扩散模型提供了理论指导。条件生成机制为在给定人脸嵌入约束下生成目标图像提供了直接的技术路径。高效采样策略则在保证生成质量的同时显著降低采样步数，使得模板逆向重建在计算效率与攻击实用性之间取得平衡。本节所建立的扩散模型理论将在第3章中被直接应用于构建高效模板逆向重建方法。

\section{低秩适配技术}\label{sec:peft_lora}

在深度神经网络特别是大规模预训练模型的微调任务中，全参数微调往往面临计算资源消耗巨大、容易过拟合、存储与部署成本高昂等挑战。参数高效微调方法（Parameter-Efficient Fine-Tuning，PEFT）应运而生，旨在通过仅更新模型的少量参数或引入少量可训练模块，实现对下游任务的高效适配。低秩适配（Low-Rank Adaptation，LoRA）\cite{huLowRankAdaptation2021}作为PEFT方法中的代表性技术，已在自然语言处理、计算机视觉等领域展现出优异的性能与效率平衡，并被广泛应用于大型语言模型、扩散生成模型等的任务特定化定制中。

本节系统性地阐述低秩适配技术的理论基础、数学原理、工程实现策略及其在模板逆向重建任务中的应用方法，为第3章中基于扩散模型的个性化模板重建提供技术支撑。

\subsection{参数高效微调的理论动机}

给定参数为$\theta_0\in\mathbb{R}^d$的预训练模型$\mathcal{M}_{\theta_0}$与下游任务数据集$\mathcal{D}=\{(x_i,y_i)\}_{i=1}^N$，传统的全参数微调通过最小化任务损失来更新所有模型参数：
\begin{equation}\label{eq:full_finetune}
      \theta^* = \arg\min_{\theta} \sum_{(x,y)\in\mathcal{D}} L(\mathcal{M}_{\theta}(x), y)
\end{equation}

这种方法存在以下问题：首先是计算与显存开销问题，对于参数量达数十亿甚至千亿级的大型模型如GPT系列、Stable Diffusion等，全参数微调需要维护完整的梯度、优化器状态与激活值，显存需求往往超出单卡或常规集群的承载能力；其次是过拟合风险，当下游任务数据量有限时，更新全部参数容易导致模型过度拟合训练数据，泛化能力下降；再次是存储与部署成本，当需要针对多个下游任务或多个目标如不同身份、不同领域进行微调时，需为每个任务保存完整的模型副本，导致存储与部署成本线性增长；最后是灾难性遗忘，全参数更新可能破坏预训练模型在通用任务上的能力，导致在新任务上性能提升的同时，在原任务上性能大幅下降。

参数高效微调的理论基础源于神经网络参数空间的低秩结构与任务适配的内在维度假设\cite{aghajanyanIntrinsicDimensionality2020}。研究表明，尽管深度神经网络的参数空间维度极高，但针对特定任务的有效优化往往可以在远低于全参数空间维度的子空间中实现。

具体地，Aghajanyan等人通过实验发现，对于给定的下游任务，存在一个低维参数子空间，在该子空间中进行优化即可达到与全参数微调相当的性能。形式化地，假设任务适配所需的参数更新 $\Delta\theta$ 位于参数空间的一个低秩子空间中，即$\Delta\theta \approx V\phi$。其中 $V\in\mathbb{R}^{d\times r}$ 为子空间的基向量矩阵，$\phi\in\mathbb{R}^r$ 为低维表示，$r\ll d$。这一假设为低秩适配方法提供了理论支撑：通过显式地将参数更新约束在低秩结构上，可以在大幅降低可训练参数量的同时，保持模型的适配能力。

\subsection{低秩适配技术原理}

LoRA的核心思想是在不改变预训练权重的前提下，通过低秩矩阵分解为模型的权重矩阵引入可训练的增量。具体地，对于神经网络中位于线性层或注意力投射层等位置的权重矩阵$W_0\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$，LoRA将微调后的权重表示为：
\begin{equation}\label{eq:lora_core}
\begin{aligned}
      W &= W_0 + \Delta W, \\
      \Delta W &= BA,
\end{aligned}
\end{equation}
其中$W$为微调后的权重矩阵，$W_0$为原始预训练权重矩阵并保持冻结，$\Delta W$为可训练的权重增量，$B\in\mathbb{R}^{d_{\text{out}}\times r}$与$A\in\mathbb{R}^{r\times d_{\text{in}}}$为低秩分解矩阵，$d_{\text{out}}$与$d_{\text{in}}$分别为输出与输入维度，$r$为低秩适配的秩参数且满足$r\ll\min(d_{\text{out}}, d_{\text{in}})$。在前向传播时，对于输入向量$x\in\mathbb{R}^{d_{\text{in}}}$，输出$h\in\mathbb{R}^{d_{\text{out}}}$的计算公式为：
\begin{equation}\label{eq:lora_forward}
      h = Wx = W_0 x + B(Ax)
\end{equation}
为保证不同秩设置下学习率敏感性的稳定性，实践中引入缩放超参数$\alpha$对低秩增量进行归一化，使得最终权重更新表达式为：
\begin{equation}\label{eq:lora_scaling}
      W = W_0 + \frac{\alpha}{r} BA
\end{equation}

LoRA的参数效率体现在可训练参数量的大幅减少。对于原始权重矩阵 $W_0\in\mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$，全参数微调需要存储 $d_{\text{out}}\times d_{\text{in}}$ 个参数；而LoRA仅需 $r(d_{\text{in}}+d_{\text{out}})$ 个参数。参数压缩比为：
\begin{equation}\label{eq:lora_compression_ratio}
      \text{Ratio} = \frac{d_{\text{out}}\times d_{\text{in}}}{r(d_{\text{in}}+d_{\text{out}})} \approx \frac{\min(d_{\text{out}}, d_{\text{in}})}{r}
\end{equation}
当 $r\in\{8,16,32\}$ 等较小值时，压缩比可达数十倍至上百倍。在计算复杂度方面，前向传播的额外计算量为 $O(r(d_{\text{in}}+d_{\text{out}}))$，相比原始 $O(d_{\text{out}}\times d_{\text{in}})$ 在 $r\ll\min(d_{\text{out}}, d_{\text{in}})$ 时可忽略不计。

反向传播时，梯度计算遵循链式法则：
\begin{equation}\label{eq:lora_gradients}
\begin{aligned}
      \nabla_A \mathcal{L} &= B^{\top} \nabla_{W}\mathcal{L}, \\
      \nabla_B \mathcal{L} &= \nabla_{W}\mathcal{L} \cdot A^{\top},
\end{aligned}
\end{equation}
其中$\mathcal{L}$为任务损失函数，$\nabla_{W}\mathcal{L}$为损失对权重$W$的梯度，$\nabla_A \mathcal{L}$与$\nabla_B \mathcal{L}$分别为损失对低秩矩阵$A$和$B$的梯度，上标$\top$表示矩阵转置。由于仅需计算$A$与$B$的梯度而无需计算$W_0$的梯度，显存占用与计算开销显著降低。


\subsection{LoRA在视觉生成模型中的应用}

在计算机视觉的生成模型中，包括扩散模型、生成对抗网络、变分自编码器等架构，LoRA通常应用于以下关键模块：

（1）注意力机制的投影矩阵。Transformer架构或U-Net中的自注意力与交叉注意力模块包含Query、Key、Value与输出投影矩阵（$W_Q, W_K, W_V, W_O$），这些矩阵决定了模型对特征的聚合与重组能力。在这些投影层上应用LoRA能够高效地调整注意力模式，使模型关注任务特定的特征区域。

（2）前馈网络（Feed-Forward Network, FFN）。FFN中的线性层（通常为两层MLP）在特征变换与非线性映射中起关键作用。对FFN的权重矩阵应用LoRA可以调整特征的表示空间，适配新的数据分布或任务需求。

（3）卷积层的点卷积（$1\times1$ Convolution）。对于包含卷积操作的网络，可以在通道混合的 $1\times1$ 卷积层上应用LoRA，通过低秩分解调整通道间的特征融合权重。

（4）条件注入层。在条件生成任务中，例如文本到图像、图像到图像等场景，条件信息需要通过特定的注入层融入生成过程，常用的注入机制包括AdaLN、交叉注意力机制等。在这些层上应用LoRA能够精细控制条件信息对生成结果的影响。

实践中，通常不对所有层同时应用LoRA，而是根据任务需求与计算资源约束选择性地在若干关键层上引入低秩适配。

\subsection{LoRA的推理优化与部署策略}

训练完成后，为实现最高推理效率，可将LoRA的低秩增量 $\Delta W = \frac{\alpha}{r}BA$ 与原始权重 $W_0$ 合并为单一权重矩阵：
\begin{equation}\label{eq:lora_merge}
      W_{\text{merged}} = W_0 + \frac{\alpha}{r}BA
\end{equation}

合并后的模型在推理时与标准模型无异，无需额外的矩阵乘法操作，从而避免了运行时开销。合并过程在CPU或GPU上均可快速完成，且无精度损失。

LoRA的一个重要优势在于支持多任务或多目标的模块化部署。对于需要针对多个身份、多个域或多个任务进行个性化适配的场景，例如本研究中的多目标模板重建任务，可以采用“一个基础模型+多组LoRA权重”的架构：
（1）共享基础模型：所有任务或目标共享同一预训练的基础模型 $\mathcal{M}_{\theta_0}$，该模型保持冻结状态并作为只读模块存储；
（2）任务特定LoRA：为每个任务或目标训练一组独立的LoRA权重 $\{A_i, B_i\}$，存储开销仅为基础模型参数量的 $\frac{r(d_{\text{in}}+d_{\text{out}})}{d_{\text{out}}\times d_{\text{in}}}$ 倍；
（3）动态加载：在推理时，根据任务需求动态加载对应的LoRA权重，实现快速切换而无需重新加载完整模型。

该架构在存储成本、部署灵活性与推理效率上均具有显著优势，特别适用于需要支持大量个性化配置的实际应用场景。

\subsection{小结}

本节系统阐述了参数高效微调方法与低秩适配技术的理论基础、数学原理及其在视觉生成模型中的应用策略。LoRA通过在冻结预训练权重的前提下引入低秩可训练增量，实现了参数效率与模型性能的优异平衡，特别适用于大规模生成模型的任务特定化定制。

在模板逆向重建任务中，LoRA的优势主要体现在三个方面：第一，参数效率显著，通常仅需全参数微调成本的1\%至5\%即可实现对不同目标模板的个性化适配；第二，训练稳定性强，冻结预训练权重可有效防止灾难性遗忘，保持模型原有的生成质量与泛化能力；第三，部署灵活性高，通过模块化权重管理可快速切换不同目标模板的适配参数。

本节建立的LoRA应用框架涵盖应用位置选择、秩参数设定、超参数调优、训练流程设计、推理优化与部署策略等关键环节，将直接支撑后续章节中基于扩散模型的模板重建方法实现。

\section{本章小结}

本章系统构建了支撑生物特征模板逆向攻击研究的理论基础，为后续章节的方法设计与实验评估提供了必要的技术支撑。

第~\ref{sec:face_recognition}~节建立了基于深度嵌入的人脸识别系统形式化框架，阐述了特征提取、嵌入归一化、距离度量与决策策略等核心组件。通过分析度量学习中的对比损失、三元组损失与角度边距损失在构造判别性嵌入空间中的作用，为理解"身份信息如何被编码为固定维度向量"这一核心问题提供了理论基础。同时，建立的验证任务与识别任务评估体系，将在后续章节中直接用于衡量模板重建图像的身份保真度与攻击成功率。

第~\ref{sec:diffusion_models}~节系统推导了扩散概率模型的数学原理，包括前向扩散过程的马尔可夫链表述、反向去噪过程的变分下界优化、噪声预测与得分函数的等价性。通过详细推导后验分布、重参数化技巧与简化训练目标，建立了从理论到实现的完整路径。条件生成机制为在给定嵌入约束下生成图像提供了多种技术选择。DDIM与EDM等高效采样策略的介绍，展示了如何在保证生成质量的同时显著降低计算开销。这些理论与方法为第3章中基于扩散模型的模板重建方法提供了数学工具与算法基础。

第~\ref{sec:peft_lora}~节阐述了低秩适配的理论基础与实现策略，推导了低秩增量 $\Delta W = BA$ 的数学原理，分析了其在参数效率、计算复杂度与存储开销方面的优势。通过讨论应用位置选择、权重合并与模块化部署等工程问题，为在有限计算资源下针对多个目标模板进行个性化生成模型微调提供了高效技术路径。LoRA技术使得大规模模板重建实验的开展成为可能，同时保持了预训练模型的生成质量与泛化能力。

本章所建立的理论框架在人脸识别的嵌入表示、扩散模型的条件生成、LoRA的参数高效微调三个层面形成了完整的逻辑链条，为后续章节的攻击方法设计、实验评估奠定了坚实的理论与技术基础。

% End of math supplement

% Local Variables:
% TeX-master: "../main"
% TeX-engine: xetex
% End:
